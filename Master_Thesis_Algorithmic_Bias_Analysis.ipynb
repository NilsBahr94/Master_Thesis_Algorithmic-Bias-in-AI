{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master_Thesis-Algorithmic_Bias_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1xgO861smhhAVw-ALtedP9mnmWIFwVSvY",
      "authorship_tag": "ABX9TyO7v/BTSxf26sblxjTbq2ZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilsBahr94/Master_Thesis_Algorithmic-Bias-in-AI/blob/master/Master_Thesis_Algorithmic_Bias_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LslOkLJNLSY-"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5GE201KpLiKY"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0J4SEBlzMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install relevant libraries\n",
        "!pip install plotnine  \n",
        "!pip install pandas\n",
        "!pip install plotly\n",
        "!pip install datatable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFZNjOwEsaEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1fafbbcb-6d9a-466c-e94e-681c7ff3c396"
      },
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotnine import *\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import collections\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn import model_selection\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Learning Curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_validate\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kWQTpKzC0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9e87c66c-6432-4cdf-c55f-c5b0337b9925"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ89yoNTe1AE",
        "colab_type": "text"
      },
      "source": [
        "## Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KErf0aaNiQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ORIGINAL FUNCTION\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  import numpy as np\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize lists for metrics \n",
        "  # COMPLETE\n",
        "  rows_compl_list = [] \n",
        "  f1_compl_list = []\n",
        "  f1_avg_train_score_list = []\n",
        "  tpr_compl_list = []\n",
        "  # MINORITY\n",
        "  rows_min_list = [] \n",
        "  f1_min_list = []\n",
        "  tpr_min_list = []\n",
        "  fpr_min_list = []\n",
        "  prob_y_1_min_list = []\n",
        "  # MAJORITY\n",
        "  rows_maj_list = []\n",
        "  f1_maj_list = []\n",
        "  tpr_maj_list = []\n",
        "  fpr_maj_list = []\n",
        "  prob_y_1_maj_list = []\n",
        "\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      # NUMBER OF ROWS\n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)   \n",
        "      # F1 test scores\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "      # F1 training scores\n",
        "      f1 = make_scorer(f1_score)\n",
        "      cross_val_results = cross_validate(estimator = model, \n",
        "                                         X = df_train_input, \n",
        "                                         y = df_train_label, \n",
        "                                         cv = cv, \n",
        "                                         scoring = f1, \n",
        "                                         return_train_score=True)\n",
        "      f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n",
        "      f1_avg_train_score_list.append(f1_avg_train_score)\n",
        "      # Recall\n",
        "      tpr_compl = recall_score(df_train_label, y_train_pred)\n",
        "      tpr_compl_list.append(tpr_compl)\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = fp_min / (fp_min+tn_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "      # Cond. Prob. P(y_min=1|minority)\n",
        "      filter_race_black_y_1 = dataset_var[discr_feature].isin([min_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_black = dataset_var[discr_feature].isin([min_value])\n",
        "      prob_y_1_min = len(dataset_var[filter_race_black_y_1].index) / len(dataset_var[filter_race_black].index)\n",
        "      prob_y_1_min_list.append(prob_y_1_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) \n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = fp_maj / (fp_maj+tn_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "      # Cond. Prob. P(y_maj=1|majority)\n",
        "      filter_race_white_y_1 = dataset_var[discr_feature].isin([maj_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      prob_race_white_y_1 = len(dataset_var[filter_race_white_y_1].index) / len(dataset_var[filter_race_white].index)\n",
        "      prob_y_1_maj_list.append(prob_race_white_y_1)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                             \"rows_minority\": rows_min_list,\n",
        "                             \"rows_majority\": rows_maj_list, \n",
        "                             'f1_complete': f1_compl_list,\n",
        "                             \"f1_complete_train\": f1_avg_train_score_list,\n",
        "                             'f1_minority': f1_min_list,\n",
        "                             'f1_majority': f1_maj_list,\n",
        "                             \"tpr_complete\": tpr_compl_list,\n",
        "                             'tpr_minority': tpr_min_list,\n",
        "                             \"tpr_majority\": tpr_maj_list,\n",
        "                             \"fpr_minority\": fpr_min_list,\n",
        "                             \"fpr_majority\": fpr_maj_list,\n",
        "                             \"prob_yhat_1_minority\": prob_y_1_min_list,\n",
        "                             \"prob_yhat_1_majority\": prob_y_1_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"])\n",
        "  # FAIRNESS METRICS\n",
        "\n",
        "  ## Chosen\n",
        "  # Statistical Parity Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"stat_parity_diff\"] =  results_df[\"prob_yhat_1_minority\"] - results_df[\"prob_yhat_1_majority\"]\n",
        "  # Equal Opportunity Distance -> The closer to 0, the fairer.\n",
        "  results_df[\"equal_opport_dist\"] = results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"]  \n",
        "\n",
        "  ## Discarded\n",
        "  # Disparate Impact -> The closer to 1, the fairer.\n",
        "  results_df[\"disparate_impact\"] =  results_df[\"prob_yhat_1_minority\"] / results_df[\"prob_yhat_1_majority\"]\n",
        "  # Average Absolute Odds Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"aver_abs_odds_diff\"] = 0.5*(abs(results_df[\"fpr_minority\"] - results_df[\"fpr_majority\"])+abs(results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"])) \n",
        "\n",
        "  return(results_df)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label, cv):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.2],\n",
        "                \"n_estimators\": [50, 100, 150],\n",
        "                \"max_depth\": [3, 6, 9],\n",
        "                \"min_child_weight\": [1, 3, 5], \n",
        "                \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "  xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  print(grid_rf_class.best_params_)\n",
        "  print(grid_rf_class.best_estimator_) \n",
        "\n",
        "  return(grid_rf_class)\n",
        "\n",
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(\"7. Class Balance: The Class Balance looks as follows:\")\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  # data.set_index(disc_feature).isna().sum(level=0)\n",
        "  print(data.apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().mean()))\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  #if second_disc_feature:\n",
        "  #  print(\"10. Cross-sectional sample size disparity\")\n",
        "  #  data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "# Define Data Preprocessing Function\n",
        "\n",
        "def fair_preprocess(data, label, neg_class, pos_class):\n",
        "\n",
        "  ''' Applies binary encoding on the values of the label feature.\n",
        "\n",
        "  Returns two datasets in the following order: data_input_features and data_label. '''\n",
        "\n",
        "  # Binary encoding\n",
        "  data[label] = data[label].replace({neg_class: 0, pos_class: 1})\n",
        "\n",
        "  # Create separate dataset version for input features and label\n",
        "  data_input_features = data.drop(columns=[label])\n",
        "  data_label = data[label]\n",
        "\n",
        "  return data_input_features, data_label\n",
        "\n",
        "\n",
        "# Function to create datasets with different minority group sizes\n",
        "\n",
        "def create_datasets(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "    datasets = []\n",
        "    for training_size in training_sizes:\n",
        "      if abs(training_size) <= len(min_data.index):\n",
        "        dataset_min_sample = min_data.sample(n=training_size)\n",
        "        dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "        datasets.append(dataset)\n",
        "      else:\n",
        "        break\n",
        "    print(len(datasets))\n",
        "    print([df.shape[0] for df in datasets])\n",
        "    return datasets\n",
        "\n",
        "\n",
        "\n",
        "# Learning Curve https://www.kaggle.com/grfiv4/learning-curves-1\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, \n",
        "                        ylim=None, cv=None, \n",
        "                        scoring=None, obj_line=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "              A string (see model evaluation documentation)\n",
        "              or a scorer callable object / function with signature scorer(estimator, X, y)\n",
        "              For Python 3.5 the documentation is here:\n",
        "              http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "              For example, Log Loss is specified as 'neg_log_loss'\n",
        "              \n",
        "    obj_line : numeric or None (default: None)\n",
        "               draw a horizontal line \n",
        "               \n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "        \n",
        "        \n",
        "    Citation\n",
        "    --------\n",
        "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "        \n",
        "    Usage\n",
        "    -----\n",
        "        plot_learning_curve(estimator = best_estimator, \n",
        "                            title     = best_estimator_title, \n",
        "                            X         = X_train, \n",
        "                            y         = y_train, \n",
        "                            ylim      = (-1.1, 0.1), # neg_log_loss is negative\n",
        "                            cv        = StatifiedCV, # CV generator\n",
        "                            scoring   = scoring,     # eg., 'neg_log_loss'\n",
        "                            obj_line  = obj_line,    # horizontal line\n",
        "                            n_jobs    = n_jobs)      # how many CPUs\n",
        "\n",
        "         plt.show()\n",
        "    \"\"\"\n",
        "    \n",
        "    from sklearn.model_selection import learning_curve\n",
        "    import numpy as np\n",
        "    from matplotlib import pyplot as plt\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std  = np.std(train_scores, axis=1)\n",
        "    test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "    test_scores_std   = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.style.use('seaborn')\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    if obj_line:\n",
        "        plt.axhline(y=obj_line, color='blue')\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "# Minority Line Chart - Function Definition\n",
        "\n",
        "def min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "  \n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "  # Fairness Metrics\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"aver_abs_odds_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Avg. Abs. Odds Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"stat_parity_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Statistical Parity Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"equal_opport_dist\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Equal Opportunity Distance'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"disparate_impact\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Disparate Impact'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Minority',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "# Majority <-> Minority Line Chart - Function Definition\n",
        "\n",
        "def maj_min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "def reduce_mem_usage(props):\n",
        "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
        "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
        "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
        "    for col in props.columns:\n",
        "        if props[col].dtype != object:  # Exclude strings\n",
        "            \n",
        "            # Print current column type\n",
        "            print(\"******************************\")\n",
        "            print(\"Column: \",col)\n",
        "            print(\"dtype before: \",props[col].dtype)\n",
        "            \n",
        "            # make variables for Int, max and min\n",
        "            IsInt = False\n",
        "            mx = props[col].max()\n",
        "            mn = props[col].min()\n",
        "            \n",
        "            # Integer does not support NA, therefore, NA needs to be filled\n",
        "            if not np.isfinite(props[col]).all(): \n",
        "                NAlist.append(col)\n",
        "                props[col].fillna(mn-1,inplace=True)  \n",
        "                   \n",
        "            # test if column can be converted to an integer\n",
        "            asint = props[col].fillna(0).astype(np.int64)\n",
        "            result = (props[col] - asint)\n",
        "            result = result.sum()\n",
        "            if result > -0.01 and result < 0.01:\n",
        "                IsInt = True\n",
        "\n",
        "            \n",
        "            # Make Integer/unsigned Integer datatypes\n",
        "            if IsInt:\n",
        "                if mn >= 0:\n",
        "                    if mx < 255:\n",
        "                        props[col] = props[col].astype(np.uint8)\n",
        "                    elif mx < 65535:\n",
        "                        props[col] = props[col].astype(np.uint16)\n",
        "                    elif mx < 4294967295:\n",
        "                        props[col] = props[col].astype(np.uint32)\n",
        "                    else:\n",
        "                        props[col] = props[col].astype(np.uint64)\n",
        "                else:\n",
        "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
        "                        props[col] = props[col].astype(np.int8)\n",
        "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
        "                        props[col] = props[col].astype(np.int16)\n",
        "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
        "                        props[col] = props[col].astype(np.int32)\n",
        "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
        "                        props[col] = props[col].astype(np.int64)    \n",
        "            \n",
        "            # Make float datatypes 32 bit\n",
        "            else:\n",
        "                props[col] = props[col].astype(np.float32)\n",
        "            \n",
        "            # Print new column type\n",
        "            print(\"dtype after: \",props[col].dtype)\n",
        "            print(\"******************************\")\n",
        "    \n",
        "    # Print final result\n",
        "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
        "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
        "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
        "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
        "    return props, NAlist\n",
        "\n",
        "# Plot with selection of relevant fairness metrics and seperate priv. & unpriv. group performance metrics \n",
        "\n",
        "def min_metrics_line_chart_selection(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "  \n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Unprivileged Group'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Privileged Group'))\n",
        "  \n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"prob_yhat_1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='P(Ŷ=1|G=unprivileged)'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"prob_yhat_1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='P(Ŷ=1|G=privileged)'))\n",
        "\n",
        "  # Fairness Metrics\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"stat_parity_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Statistical Parity Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"equal_opport_dist\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Equal Opportunity Distance'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Number of observations in the unprivileged group (privileged group has a fixed number of observations)',\n",
        "                    yaxis_title='Metric score', \n",
        "                    font=dict(size=13))\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z0MC-9BGLofv"
      },
      "source": [
        "# Case Studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66N6XRfQf1Pb",
        "colab_type": "text"
      },
      "source": [
        "## 0) Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xIamwLuVcSP",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZJiBYwBVqOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "best_classifier_adult = hyperparameter_tuning(df_3_adult_train_input, df_3_adult_train_label, cv = 5)\n",
        "\n",
        "# Resulting Model\n",
        "\n",
        "# {'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'reg_lambda': 1}\n",
        "# XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "#               learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "#               nthread=None, objective='binary:logistic', random_state=0,\n",
        "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "#               silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZJ5ojlvVm59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b7d1943e-8480-4f3f-f53b-5501733fcb66"
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label, cv = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-947c25bc7585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_compas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_compas_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_compas_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_compas_train_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BvyZkFRVn8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3ceaffda-7696-40ee-fd81-4771dddac835"
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label, cv = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TerminatedWorkerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-24e7ab38772a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_homicide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_homicide_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_homicide_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ca97542c01a2>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m(df_train_input, df_train_label, cv)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_input_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeGjO1NFVyDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_credit = hyperparameter_tuning(df_credit_train_input, df_credit_train_label, cv = 5)\n",
        "\n",
        "{'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 50, 'reg_lambda': 1.2}\n",
        "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DTSOwjLd9IU",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Curves: Compare diff. algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKTdADBDbp14",
        "colab_type": "text"
      },
      "source": [
        "Selection of classification method:\n",
        "\n",
        "- **Linear** machine learning algorithms often have a high bias but a low variance.\n",
        "    1. Logistic Regression\n",
        "    2. Linear Discriminant Analysis\n",
        "    3. Partial Least Squares Discriminant Analysis\n",
        "- **Nonlinear** machine learning algorithms often have a low bias but a high variance.\n",
        "    1. Nonlinear Discriminant Analysis\n",
        "    2. Neural Networks\n",
        "    3. Flexible Discriminant Analysis\n",
        "    4. Support Vector Machines \n",
        "    5. K-Nearest Neighbors\n",
        "    6. Naive Bayes\n",
        "- Others\n",
        "    1. Basic Classification Trees\n",
        "    2. Random Forest\n",
        "    3. Boosted Trees - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHM2sG6QNt0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define different classification algorithms\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "log_reg = LogisticRegression()\n",
        "svm = SVC(C = 1.0, kernel = \"rbf\")\n",
        "\n",
        "# Store different models in a list\n",
        "models = [random_forest, log_reg, svm]\n",
        "\n",
        "# Define other input arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Plot learning curve for different classifcation algorithms\n",
        "for model in models:\n",
        "  plt.subplot\n",
        "  plot_learning_curve(estimator = model, \n",
        "                      title = f\"{model} Learning Curve\", \n",
        "                      X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                      cv = 10, \n",
        "                      scoring = f1, \n",
        "                      ylim = (0, 1), \n",
        "                      train_sizes = sizes)\n",
        "\n",
        "# Plot different subplots \n",
        "\n",
        "# for model, i in [(RandomForestClassifier(), 1), (KNeighborsClassifier(),2)]:\n",
        "#     plt.subplot(1,2,i)\n",
        "#     learning_curves(estimator = model, \n",
        "#                     data = df_3_adult_dummies, \n",
        "#                     features = df_3_adult_train_input.columns, \n",
        "#                     target= \"Over-50K\", \n",
        "#                     train_sizes = train_sizes, \n",
        "#                     cv= 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhsVjAHMqceC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def data_preprocess_fair(full_dataset, \n",
        "#                          outcome_feature, \n",
        "#                          privileged_outcome_label, \n",
        "#                          discriminatory_feature, \n",
        "#                          majority_label,\n",
        "#                          minority_label)\n",
        "\n",
        "\n",
        "## 1) Convert the labels of the target variable into a suitable  format (binary integer)\n",
        "# Encoding Binary \n",
        "df_3_adult[\"Over-50KVariables\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" \n",
        "                                                               else 0)\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "## 2. Prepare data for training (input features and label).\n",
        "# Features of complete dataset\n",
        "# print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "# print(df_3_adult_train_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]\n",
        "# print(df_3_adult_train_label)\n",
        "\n",
        "## 3) Prepare data as such that machine learning classification algorithm can handle that properly (e.g. standardization, normalization, feature scaling, dummy encoding)\n",
        "# Dummy \n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "## 4) Filter dataset based on certain feature values which identify subpopulations and create different dataset versions based on that\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "# print(df_3_adult.shape)\n",
        "# print(df_3_adult_black.shape)\n",
        "# print(df_3_adult_white.shape)\n",
        "# print(df_3_adult_female.shape)\n",
        "# print(df_3_adult_male.shape)\n",
        "\n",
        "\n",
        "  # \"\"\"\n",
        "  #   Convert the target label in an binary number format, create two different datasets sets \n",
        "\n",
        "  #   Parameters\n",
        "  #   ----------\n",
        "  #   estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "  #       An object of that type which is cloned for each validation.\n",
        "\n",
        "  #   title : string\n",
        "  #       Title for the chart.\n",
        "\n",
        "  #   X : array-like, shape (n_samples, n_features)\n",
        "  #       Training vector, where n_samples is the number of samples and\n",
        "  #       n_features is the number of features.\n",
        "\n",
        "  #   y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "  #       Target relative to X for classification or regression;\n",
        "  #       None for unsupervised learning.\n",
        "\n",
        "  #   ylim : tuple, shape (ymin, ymax), optional\n",
        "  #       Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "  # \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxrRXcX700-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "model = xgboost.XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdqX0VzxMCuy"
      },
      "source": [
        "## 1) US Adult Income Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpPHz6cXJhv",
        "colab_type": "text"
      },
      "source": [
        "UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people’s annual incomes, or various other studies [6]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1N6Co27mxY",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNF_2o9_ALP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "               \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_3_adult = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q76uhuRlwiAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxt_BWxnmvKs",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqOxLfCdywjg",
        "colab_type": "text"
      },
      "source": [
        "Shrink the size of the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8QYrtg2oH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBkOfk12Vc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_3_adult_copy = df_3_adult.select_dtypes(include=['object']).copy()\n",
        "df_3_adult_copy.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5sRmQEk25ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_3_adult, NAlist = reduce_mem_usage(df_3_adult)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)\n",
        "\n",
        "for col in [\"Workclass\", \"Education\",\"Marital-Status\",\"Occupation\",\"Relationship\",\"Race\",\"Sex\",\"Country\",\"Over-50K\"]:\n",
        "    df_3_adult[col] = df_3_adult[col].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl76RtG5KyC4",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeiMbxejo7Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze class balance for dataset with only unprivileged and privileged dataset\n",
        "is_unpriv = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_priv = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "df_unpriv_adult = df_3_adult[is_unpriv]\n",
        "df_priv_adult = df_3_adult[is_priv]\n",
        "\n",
        "df_unpriv_priv_complete_adult = pd.concat([df_unpriv_adult, df_priv_adult])\n",
        "\n",
        "print(df_unpriv_priv_complete_adult[\"Over-50K\"].value_counts(dropna=False))\n",
        "print(df_unpriv_priv_complete_adult[\"Over-50K\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiopzwnVruCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.set_index('Race').isna().sum(level=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffuJd_WpxZcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.info)\n",
        "print(df_3_adult.describe())\n",
        "print(df_3_adult.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CierIp3nTNln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_3_adult, disc_feature= \"Race\", disc_min_value=\"Black\", label = \"Over-50K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvxB8Eg_SFP",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzjd5IvaEJk-",
        "colab_type": "text"
      },
      "source": [
        "###### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HHPEI2vf-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_u077HEWs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7MvxyWrGlnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a250bfcd-53c1-4200-c11c-f555870652f6"
      },
      "source": [
        "# Evaluate using Cross Validation\n",
        "\n",
        "model_adult = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
        "              min_child_weight=1, missing=None, n_estimators=80, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "df_adult_train_input_dummy_test = pd.get_dummies(df_3_adult_train_input)\n",
        "X = df_adult_train_input_dummy_test\n",
        "Y = df_3_adult_train_label\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Results\n",
        "results = model_selection.cross_val_score(model_adult, X, Y, cv=kfold, scoring = recall)\n",
        "print(results)\n",
        "print(results.std())\n",
        "print(results.mean())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.65203562 0.64721141 0.67075533 0.65019011 0.664375  ]\n",
            "0.009059781518093274\n",
            "0.6569134954486548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mJ9TYZWTP3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY190l6ZkNN3",
        "colab_type": "text"
      },
      "source": [
        "###### 1) Define hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AAaPCPTImax",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzBhMm1Nx23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "6a77fda0-a209-44ab-edb9-181f38644efb"
      },
      "source": [
        "# HYPERPARAMETER TUNING - XGBoost\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "              \"n_estimators\": [90, 100],\n",
        "              \"max_depth\": [7], \n",
        "              \"min_child_weight\": [1], \n",
        "              \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "xgb_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= recall, \n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "\n",
        "# Fit model\n",
        "xgb_class.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "\n",
        "print(xgb_class.best_params_)\n",
        "print(xgb_class.best_estimator_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.3, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 90, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
            "              min_child_weight=1, missing=None, n_estimators=90, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeCn37qQpS-",
        "colab_type": "text"
      },
      "source": [
        "###### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTkqDkW4QrWV",
        "colab_type": "text"
      },
      "source": [
        "First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGE2KGthLrrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2617740d-591a-4de4-cd50-1e0cc1a023bb"
      },
      "source": [
        "# Determine number of training examples for the examples that are at risk of being discriminated\n",
        "print(df_3_adult.groupby(['Race', \"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             Over-50K\n",
            "Race               Over-50K          \n",
            "Amer-Indian-Eskimo 0              275\n",
            "                   1               36\n",
            "Asian-Pac-Islander 0              763\n",
            "                   1              276\n",
            "Black              0             2737\n",
            "                   1              387\n",
            "Other              0              246\n",
            "                   1               25\n",
            "White              0            20699\n",
            "                   1             7117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-GcZxMQEaxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f3330c93-87cf-4cf1-e962-859489f755d8"
      },
      "source": [
        "# Define arguments\n",
        "\n",
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "df_3_adult_black = df_3_adult[is_black]  # Minority\n",
        "df_3_adult_white = df_3_adult[is_white]  # Majority\n",
        "\n",
        "# Execute function\n",
        "list_dfs_adult = create_datasets(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "[27841, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyn0FoHBJmAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "e46f1288-c10b-47fb-d576-32bae1ef2915"
      },
      "source": [
        "df_test = list_dfs_adult[15]\n",
        "print(df_test.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Over-50K\n",
            "Over-50K          \n",
            "0            21308\n",
            "1             7208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkzaawcuCKZ",
        "colab_type": "text"
      },
      "source": [
        "###### 3) Create dataframe with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBSkMj5qSUX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "f395a1ac-269e-4fed-ae4e-504e206e4c6f"
      },
      "source": [
        "# Define arguments\n",
        "list_dfs = list_dfs_adult\n",
        "label = \"Over-50K\"\n",
        "adult_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
        "              min_child_weight=1, missing=None, n_estimators=90, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "cv = 5\n",
        "discr_feature = \"Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Execute function\n",
        "results_df_adult = metrics_to_df(list_dfs = list_dfs_adult, label = label, model = adult_model, cv = cv, \n",
        "                                 discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_adult"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27841</td>\n",
              "      <td>25</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702682</td>\n",
              "      <td>0.834674</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.702789</td>\n",
              "      <td>0.649529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.649712</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.217213</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>-0.217213</td>\n",
              "      <td>-0.649712</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27866</td>\n",
              "      <td>50</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.707039</td>\n",
              "      <td>0.833822</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.707217</td>\n",
              "      <td>0.656742</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.656737</td>\n",
              "      <td>0.085106</td>\n",
              "      <td>0.068941</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.219334</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>-0.099334</td>\n",
              "      <td>0.009929</td>\n",
              "      <td>0.547110</td>\n",
              "      <td>0.013048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27891</td>\n",
              "      <td>75</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.701762</td>\n",
              "      <td>0.832780</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.701658</td>\n",
              "      <td>0.645202</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.645216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066670</td>\n",
              "      <td>0.093333</td>\n",
              "      <td>0.214697</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>-0.121363</td>\n",
              "      <td>-0.008852</td>\n",
              "      <td>0.434722</td>\n",
              "      <td>0.037761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27916</td>\n",
              "      <td>100</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.700808</td>\n",
              "      <td>0.833157</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.700908</td>\n",
              "      <td>0.650996</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.650977</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.071018</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.219406</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>-0.119406</td>\n",
              "      <td>0.015690</td>\n",
              "      <td>0.455776</td>\n",
              "      <td>0.021376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27941</td>\n",
              "      <td>125</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.703855</td>\n",
              "      <td>0.831729</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.703939</td>\n",
              "      <td>0.652741</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.652803</td>\n",
              "      <td>0.036697</td>\n",
              "      <td>0.069424</td>\n",
              "      <td>0.112000</td>\n",
              "      <td>0.218687</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>-0.106687</td>\n",
              "      <td>-0.027803</td>\n",
              "      <td>0.512147</td>\n",
              "      <td>0.030265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27966</td>\n",
              "      <td>150</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.703681</td>\n",
              "      <td>0.828985</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.648717</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.648728</td>\n",
              "      <td>0.022059</td>\n",
              "      <td>0.067056</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.215883</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>-0.135883</td>\n",
              "      <td>-0.005871</td>\n",
              "      <td>0.370571</td>\n",
              "      <td>0.025434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27991</td>\n",
              "      <td>175</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.708091</td>\n",
              "      <td>0.831863</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.708229</td>\n",
              "      <td>0.658024</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.658423</td>\n",
              "      <td>0.006369</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.219873</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>-0.162731</td>\n",
              "      <td>-0.158423</td>\n",
              "      <td>0.259890</td>\n",
              "      <td>0.110570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>28016</td>\n",
              "      <td>200</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.703428</td>\n",
              "      <td>0.831733</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.703822</td>\n",
              "      <td>0.649258</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.649431</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.067395</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.216314</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>-0.091314</td>\n",
              "      <td>-0.049431</td>\n",
              "      <td>0.577863</td>\n",
              "      <td>0.029841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>28066</td>\n",
              "      <td>250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.703229</td>\n",
              "      <td>0.831178</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.703591</td>\n",
              "      <td>0.651888</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.652382</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.069472</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.218615</td>\n",
              "      <td>0.008988</td>\n",
              "      <td>-0.118615</td>\n",
              "      <td>-0.106927</td>\n",
              "      <td>0.457425</td>\n",
              "      <td>0.072070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>28116</td>\n",
              "      <td>300</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702515</td>\n",
              "      <td>0.829064</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>0.702481</td>\n",
              "      <td>0.650581</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.650415</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.069230</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.217932</td>\n",
              "      <td>0.010785</td>\n",
              "      <td>-0.117932</td>\n",
              "      <td>0.037085</td>\n",
              "      <td>0.458858</td>\n",
              "      <td>0.038233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>28166</td>\n",
              "      <td>350</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.703564</td>\n",
              "      <td>0.830288</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.703495</td>\n",
              "      <td>0.650789</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.650415</td>\n",
              "      <td>0.038961</td>\n",
              "      <td>0.068312</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.217249</td>\n",
              "      <td>0.012583</td>\n",
              "      <td>-0.097249</td>\n",
              "      <td>0.063871</td>\n",
              "      <td>0.552361</td>\n",
              "      <td>0.046611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>28216</td>\n",
              "      <td>400</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702817</td>\n",
              "      <td>0.828131</td>\n",
              "      <td>0.686869</td>\n",
              "      <td>0.702936</td>\n",
              "      <td>0.650132</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.650836</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>0.217932</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>-0.120432</td>\n",
              "      <td>-0.084169</td>\n",
              "      <td>0.447387</td>\n",
              "      <td>0.069274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>28266</td>\n",
              "      <td>450</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702589</td>\n",
              "      <td>0.826127</td>\n",
              "      <td>0.606557</td>\n",
              "      <td>0.703479</td>\n",
              "      <td>0.649777</td>\n",
              "      <td>0.552239</td>\n",
              "      <td>0.650696</td>\n",
              "      <td>0.046997</td>\n",
              "      <td>0.068506</td>\n",
              "      <td>0.122222</td>\n",
              "      <td>0.217465</td>\n",
              "      <td>0.016178</td>\n",
              "      <td>-0.095243</td>\n",
              "      <td>-0.098457</td>\n",
              "      <td>0.562032</td>\n",
              "      <td>0.059983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>28316</td>\n",
              "      <td>500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.700437</td>\n",
              "      <td>0.830300</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.700744</td>\n",
              "      <td>0.648068</td>\n",
              "      <td>0.634615</td>\n",
              "      <td>0.648166</td>\n",
              "      <td>0.033482</td>\n",
              "      <td>0.069375</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>0.217465</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>-0.121465</td>\n",
              "      <td>-0.013551</td>\n",
              "      <td>0.441451</td>\n",
              "      <td>0.024722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>28416</td>\n",
              "      <td>600</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.700975</td>\n",
              "      <td>0.831406</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.701895</td>\n",
              "      <td>0.649882</td>\n",
              "      <td>0.597222</td>\n",
              "      <td>0.650415</td>\n",
              "      <td>0.047348</td>\n",
              "      <td>0.069762</td>\n",
              "      <td>0.113333</td>\n",
              "      <td>0.218328</td>\n",
              "      <td>0.021570</td>\n",
              "      <td>-0.104994</td>\n",
              "      <td>-0.053192</td>\n",
              "      <td>0.519098</td>\n",
              "      <td>0.037803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>28516</td>\n",
              "      <td>700</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.704690</td>\n",
              "      <td>0.826133</td>\n",
              "      <td>0.694611</td>\n",
              "      <td>0.704818</td>\n",
              "      <td>0.651360</td>\n",
              "      <td>0.637363</td>\n",
              "      <td>0.651539</td>\n",
              "      <td>0.029557</td>\n",
              "      <td>0.067829</td>\n",
              "      <td>0.108571</td>\n",
              "      <td>0.217177</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>-0.108606</td>\n",
              "      <td>-0.014176</td>\n",
              "      <td>0.499921</td>\n",
              "      <td>0.026224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28616</td>\n",
              "      <td>800</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702747</td>\n",
              "      <td>0.831521</td>\n",
              "      <td>0.674699</td>\n",
              "      <td>0.703102</td>\n",
              "      <td>0.648063</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.648026</td>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.067153</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.215775</td>\n",
              "      <td>0.028760</td>\n",
              "      <td>-0.115775</td>\n",
              "      <td>0.003137</td>\n",
              "      <td>0.463446</td>\n",
              "      <td>0.018338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>28716</td>\n",
              "      <td>900</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.701833</td>\n",
              "      <td>0.825701</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.702621</td>\n",
              "      <td>0.648776</td>\n",
              "      <td>0.580357</td>\n",
              "      <td>0.649852</td>\n",
              "      <td>0.029188</td>\n",
              "      <td>0.068747</td>\n",
              "      <td>0.097778</td>\n",
              "      <td>0.217429</td>\n",
              "      <td>0.032355</td>\n",
              "      <td>-0.119651</td>\n",
              "      <td>-0.069495</td>\n",
              "      <td>0.449700</td>\n",
              "      <td>0.054527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28816</td>\n",
              "      <td>1000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.705505</td>\n",
              "      <td>0.831555</td>\n",
              "      <td>0.650943</td>\n",
              "      <td>0.706381</td>\n",
              "      <td>0.654367</td>\n",
              "      <td>0.579832</td>\n",
              "      <td>0.655613</td>\n",
              "      <td>0.027242</td>\n",
              "      <td>0.068989</td>\n",
              "      <td>0.093000</td>\n",
              "      <td>0.219083</td>\n",
              "      <td>0.035951</td>\n",
              "      <td>-0.126083</td>\n",
              "      <td>-0.075781</td>\n",
              "      <td>0.424498</td>\n",
              "      <td>0.058764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>29066</td>\n",
              "      <td>1250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.696701</td>\n",
              "      <td>0.827192</td>\n",
              "      <td>0.625430</td>\n",
              "      <td>0.698280</td>\n",
              "      <td>0.643289</td>\n",
              "      <td>0.587097</td>\n",
              "      <td>0.644513</td>\n",
              "      <td>0.041096</td>\n",
              "      <td>0.069279</td>\n",
              "      <td>0.108800</td>\n",
              "      <td>0.216458</td>\n",
              "      <td>0.044938</td>\n",
              "      <td>-0.107658</td>\n",
              "      <td>-0.057416</td>\n",
              "      <td>0.502638</td>\n",
              "      <td>0.042800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>29316</td>\n",
              "      <td>1500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.698333</td>\n",
              "      <td>0.829374</td>\n",
              "      <td>0.623457</td>\n",
              "      <td>0.700175</td>\n",
              "      <td>0.645833</td>\n",
              "      <td>0.564246</td>\n",
              "      <td>0.647885</td>\n",
              "      <td>0.033308</td>\n",
              "      <td>0.069714</td>\n",
              "      <td>0.096667</td>\n",
              "      <td>0.217645</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>-0.120978</td>\n",
              "      <td>-0.083640</td>\n",
              "      <td>0.444149</td>\n",
              "      <td>0.060022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>29566</td>\n",
              "      <td>1750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702563</td>\n",
              "      <td>0.829702</td>\n",
              "      <td>0.660147</td>\n",
              "      <td>0.703884</td>\n",
              "      <td>0.648663</td>\n",
              "      <td>0.627907</td>\n",
              "      <td>0.649290</td>\n",
              "      <td>0.038436</td>\n",
              "      <td>0.067250</td>\n",
              "      <td>0.110857</td>\n",
              "      <td>0.216171</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>-0.105313</td>\n",
              "      <td>-0.021383</td>\n",
              "      <td>0.512823</td>\n",
              "      <td>0.025098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>29816</td>\n",
              "      <td>2000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.700856</td>\n",
              "      <td>0.830111</td>\n",
              "      <td>0.659436</td>\n",
              "      <td>0.702302</td>\n",
              "      <td>0.649668</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.651679</td>\n",
              "      <td>0.030390</td>\n",
              "      <td>0.070197</td>\n",
              "      <td>0.102500</td>\n",
              "      <td>0.218975</td>\n",
              "      <td>0.071901</td>\n",
              "      <td>-0.116475</td>\n",
              "      <td>-0.057929</td>\n",
              "      <td>0.468091</td>\n",
              "      <td>0.048868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>30066</td>\n",
              "      <td>2250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.701972</td>\n",
              "      <td>0.827104</td>\n",
              "      <td>0.685039</td>\n",
              "      <td>0.702625</td>\n",
              "      <td>0.649851</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>0.650696</td>\n",
              "      <td>0.028890</td>\n",
              "      <td>0.069279</td>\n",
              "      <td>0.102667</td>\n",
              "      <td>0.218040</td>\n",
              "      <td>0.080889</td>\n",
              "      <td>-0.115373</td>\n",
              "      <td>-0.022537</td>\n",
              "      <td>0.470862</td>\n",
              "      <td>0.031463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>30316</td>\n",
              "      <td>2500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.705539</td>\n",
              "      <td>0.825593</td>\n",
              "      <td>0.660617</td>\n",
              "      <td>0.707419</td>\n",
              "      <td>0.652203</td>\n",
              "      <td>0.598684</td>\n",
              "      <td>0.654489</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.067346</td>\n",
              "      <td>0.098800</td>\n",
              "      <td>0.217573</td>\n",
              "      <td>0.089876</td>\n",
              "      <td>-0.118773</td>\n",
              "      <td>-0.055805</td>\n",
              "      <td>0.454101</td>\n",
              "      <td>0.046776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>30566</td>\n",
              "      <td>2750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702369</td>\n",
              "      <td>0.823270</td>\n",
              "      <td>0.653722</td>\n",
              "      <td>0.704656</td>\n",
              "      <td>0.648377</td>\n",
              "      <td>0.599407</td>\n",
              "      <td>0.650696</td>\n",
              "      <td>0.032739</td>\n",
              "      <td>0.067443</td>\n",
              "      <td>0.102182</td>\n",
              "      <td>0.216674</td>\n",
              "      <td>0.098864</td>\n",
              "      <td>-0.114492</td>\n",
              "      <td>-0.051289</td>\n",
              "      <td>0.471593</td>\n",
              "      <td>0.042996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>30816</td>\n",
              "      <td>3000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.702679</td>\n",
              "      <td>0.829816</td>\n",
              "      <td>0.664697</td>\n",
              "      <td>0.704626</td>\n",
              "      <td>0.651355</td>\n",
              "      <td>0.604839</td>\n",
              "      <td>0.653787</td>\n",
              "      <td>0.030441</td>\n",
              "      <td>0.069424</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.218939</td>\n",
              "      <td>0.107852</td>\n",
              "      <td>-0.117272</td>\n",
              "      <td>-0.048948</td>\n",
              "      <td>0.464361</td>\n",
              "      <td>0.043965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0           27841             25          27816     0.702682   \n",
              "1           27866             50          27816     0.707039   \n",
              "2           27891             75          27816     0.701762   \n",
              "3           27916            100          27816     0.700808   \n",
              "4           27941            125          27816     0.703855   \n",
              "5           27966            150          27816     0.703681   \n",
              "6           27991            175          27816     0.708091   \n",
              "7           28016            200          27816     0.703428   \n",
              "8           28066            250          27816     0.703229   \n",
              "9           28116            300          27816     0.702515   \n",
              "10          28166            350          27816     0.703564   \n",
              "11          28216            400          27816     0.702817   \n",
              "12          28266            450          27816     0.702589   \n",
              "13          28316            500          27816     0.700437   \n",
              "14          28416            600          27816     0.700975   \n",
              "15          28516            700          27816     0.704690   \n",
              "16          28616            800          27816     0.702747   \n",
              "17          28716            900          27816     0.701833   \n",
              "18          28816           1000          27816     0.705505   \n",
              "19          29066           1250          27816     0.696701   \n",
              "20          29316           1500          27816     0.698333   \n",
              "21          29566           1750          27816     0.702563   \n",
              "22          29816           2000          27816     0.700856   \n",
              "23          30066           2250          27816     0.701972   \n",
              "24          30316           2500          27816     0.705539   \n",
              "25          30566           2750          27816     0.702369   \n",
              "26          30816           3000          27816     0.702679   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.834674     0.000000     0.702789      0.649529      0.000000   \n",
              "1            0.833822     0.444444     0.707217      0.656742      0.666667   \n",
              "2            0.832780     0.777778     0.701658      0.645202      0.636364   \n",
              "3            0.833157     0.631579     0.700908      0.650996      0.666667   \n",
              "4            0.831729     0.666667     0.703939      0.652741      0.625000   \n",
              "5            0.828985     0.692308     0.703704      0.648717      0.642857   \n",
              "6            0.831863     0.642857     0.708229      0.658024      0.500000   \n",
              "7            0.831733     0.600000     0.703822      0.649258      0.600000   \n",
              "8            0.831178     0.620690     0.703591      0.651888      0.545455   \n",
              "9            0.829064     0.709677     0.702481      0.650581      0.687500   \n",
              "10           0.830288     0.714286     0.703495      0.650789      0.714286   \n",
              "11           0.828131     0.686869     0.702936      0.650132      0.566667   \n",
              "12           0.826127     0.606557     0.703479      0.649777      0.552239   \n",
              "13           0.830300     0.660000     0.700744      0.648068      0.634615   \n",
              "14           0.831406     0.614286     0.701895      0.649882      0.597222   \n",
              "15           0.826133     0.694611     0.704818      0.651360      0.637363   \n",
              "16           0.831521     0.674699     0.703102      0.648063      0.651163   \n",
              "17           0.825701     0.650000     0.702621      0.648776      0.580357   \n",
              "18           0.831555     0.650943     0.706381      0.654367      0.579832   \n",
              "19           0.827192     0.625430     0.698280      0.643289      0.587097   \n",
              "20           0.829374     0.623457     0.700175      0.645833      0.564246   \n",
              "21           0.829702     0.660147     0.703884      0.648663      0.627907   \n",
              "22           0.830111     0.659436     0.702302      0.649668      0.593750   \n",
              "23           0.827104     0.685039     0.702625      0.649851      0.628159   \n",
              "24           0.825593     0.660617     0.707419      0.652203      0.598684   \n",
              "25           0.823270     0.653722     0.704656      0.648377      0.599407   \n",
              "26           0.829816     0.664697     0.704626      0.651355      0.604839   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.649712      0.000000      0.068506              0.000000   \n",
              "1       0.656737      0.085106      0.068941              0.120000   \n",
              "2       0.645216      0.000000      0.066670              0.093333   \n",
              "3       0.650977      0.043956      0.071018              0.100000   \n",
              "4       0.652803      0.036697      0.069424              0.112000   \n",
              "5       0.648728      0.022059      0.067056              0.080000   \n",
              "6       0.658423      0.006369      0.069085              0.057143   \n",
              "7       0.649431      0.057143      0.067395              0.125000   \n",
              "8       0.652382      0.032258      0.069472              0.100000   \n",
              "9       0.650415      0.029851      0.069230              0.100000   \n",
              "10      0.650415      0.038961      0.068312              0.120000   \n",
              "11      0.650836      0.014706      0.069085              0.097500   \n",
              "12      0.650696      0.046997      0.068506              0.122222   \n",
              "13      0.648166      0.033482      0.069375              0.096000   \n",
              "14      0.650415      0.047348      0.069762              0.113333   \n",
              "15      0.651539      0.029557      0.067829              0.108571   \n",
              "16      0.648026      0.033613      0.067153              0.100000   \n",
              "17      0.649852      0.029188      0.068747              0.097778   \n",
              "18      0.655613      0.027242      0.068989              0.093000   \n",
              "19      0.644513      0.041096      0.069279              0.108800   \n",
              "20      0.647885      0.033308      0.069714              0.096667   \n",
              "21      0.649290      0.038436      0.067250              0.110857   \n",
              "22      0.651679      0.030390      0.070197              0.102500   \n",
              "23      0.650696      0.028890      0.069279              0.102667   \n",
              "24      0.654489      0.029599      0.067346              0.098800   \n",
              "25      0.650696      0.032739      0.067443              0.102182   \n",
              "26      0.653787      0.030441      0.069424              0.101667   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  stat_parity_diff  \\\n",
              "0               0.217213              0.000899         -0.217213   \n",
              "1               0.219334              0.001798         -0.099334   \n",
              "2               0.214697              0.002696         -0.121363   \n",
              "3               0.219406              0.003595         -0.119406   \n",
              "4               0.218687              0.004494         -0.106687   \n",
              "5               0.215883              0.005393         -0.135883   \n",
              "6               0.219873              0.006291         -0.162731   \n",
              "7               0.216314              0.007190         -0.091314   \n",
              "8               0.218615              0.008988         -0.118615   \n",
              "9               0.217932              0.010785         -0.117932   \n",
              "10              0.217249              0.012583         -0.097249   \n",
              "11              0.217932              0.014380         -0.120432   \n",
              "12              0.217465              0.016178         -0.095243   \n",
              "13              0.217465              0.017975         -0.121465   \n",
              "14              0.218328              0.021570         -0.104994   \n",
              "15              0.217177              0.025165         -0.108606   \n",
              "16              0.215775              0.028760         -0.115775   \n",
              "17              0.217429              0.032355         -0.119651   \n",
              "18              0.219083              0.035951         -0.126083   \n",
              "19              0.216458              0.044938         -0.107658   \n",
              "20              0.217645              0.053926         -0.120978   \n",
              "21              0.216171              0.062913         -0.105313   \n",
              "22              0.218975              0.071901         -0.116475   \n",
              "23              0.218040              0.080889         -0.115373   \n",
              "24              0.217573              0.089876         -0.118773   \n",
              "25              0.216674              0.098864         -0.114492   \n",
              "26              0.218939              0.107852         -0.117272   \n",
              "\n",
              "    equal_opport_dist  disparate_impact  aver_abs_odds_diff  \n",
              "0           -0.649712          0.000000            0.359109  \n",
              "1            0.009929          0.547110            0.013048  \n",
              "2           -0.008852          0.434722            0.037761  \n",
              "3            0.015690          0.455776            0.021376  \n",
              "4           -0.027803          0.512147            0.030265  \n",
              "5           -0.005871          0.370571            0.025434  \n",
              "6           -0.158423          0.259890            0.110570  \n",
              "7           -0.049431          0.577863            0.029841  \n",
              "8           -0.106927          0.457425            0.072070  \n",
              "9            0.037085          0.458858            0.038233  \n",
              "10           0.063871          0.552361            0.046611  \n",
              "11          -0.084169          0.447387            0.069274  \n",
              "12          -0.098457          0.562032            0.059983  \n",
              "13          -0.013551          0.441451            0.024722  \n",
              "14          -0.053192          0.519098            0.037803  \n",
              "15          -0.014176          0.499921            0.026224  \n",
              "16           0.003137          0.463446            0.018338  \n",
              "17          -0.069495          0.449700            0.054527  \n",
              "18          -0.075781          0.424498            0.058764  \n",
              "19          -0.057416          0.502638            0.042800  \n",
              "20          -0.083640          0.444149            0.060022  \n",
              "21          -0.021383          0.512823            0.025098  \n",
              "22          -0.057929          0.468091            0.048868  \n",
              "23          -0.022537          0.470862            0.031463  \n",
              "24          -0.055805          0.454101            0.046776  \n",
              "25          -0.051289          0.471593            0.042996  \n",
              "26          -0.048948          0.464361            0.043965  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2lYP02s9aZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fa891a80-5bdc-48a0-80a1-4c78eda5c165"
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_adult.to_csv(\"df_adult_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_adult_metrics.csv\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4e0726b2-fcb7-47a0-b9d2-396b2a8967f8\", \"df_adult_metrics.csv\", 8868)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbXQvAkgi8vN",
        "colab_type": "text"
      },
      "source": [
        "###### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEvM_3P1K7PD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7cb3f09f-fb9b-4650-9083-0ccdbaf0bcc6"
      },
      "source": [
        "min_metrics_line_chart_selection(metric_df = results_df_adult, title=\"Fairness and Performance Metrics for the US Adult Income Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1679a77a-1f4c-4ca2-96b6-fbc31f44dca6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1679a77a-1f4c-4ca2-96b6-fbc31f44dca6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1679a77a-1f4c-4ca2-96b6-fbc31f44dca6',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Unprivileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.6666666666666666, 0.6363636363636364, 0.6666666666666666, 0.625, 0.6428571428571429, 0.5, 0.6, 0.5454545454545454, 0.6875, 0.7142857142857143, 0.5666666666666667, 0.5522388059701493, 0.6346153846153846, 0.5972222222222222, 0.6373626373626373, 0.6511627906976745, 0.5803571428571429, 0.5798319327731093, 0.5870967741935483, 0.5642458100558659, 0.627906976744186, 0.59375, 0.628158844765343, 0.5986842105263158, 0.599406528189911, 0.6048387096774194]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Privileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.649711957285373, 0.656737389349445, 0.645215680764367, 0.650976535056906, 0.6528031473935647, 0.6487283967964029, 0.6584234930448223, 0.6494309400028102, 0.6523816214697203, 0.6504145004917803, 0.6504145004917803, 0.6508360264156245, 0.6506955177743431, 0.6481663622312772, 0.6504145004917803, 0.6515385696220317, 0.6480258535899958, 0.6498524659266545, 0.6556133202191935, 0.6445131375579598, 0.6478853449487143, 0.6492904313615288, 0.6516790782633132, 0.6506955177743431, 0.654489251088942, 0.6506955177743431, 0.6537867078825348]}, {\"mode\": \"lines+markers\", \"name\": \"P(\\u0176=1|G=unprivileged)\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.12, 0.09333333333333334, 0.1, 0.112, 0.08, 0.05714285714285714, 0.125, 0.1, 0.1, 0.12, 0.0975, 0.12222222222222222, 0.096, 0.11333333333333333, 0.10857142857142857, 0.1, 0.09777777777777778, 0.093, 0.1088, 0.09666666666666666, 0.11085714285714286, 0.1025, 0.10266666666666667, 0.0988, 0.10218181818181818, 0.10166666666666667]}, {\"mode\": \"lines+markers\", \"name\": \"P(\\u0176=1|G=privileged)\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.21721311475409835, 0.21933419614610297, 0.21469657750934715, 0.2194060972102387, 0.21868708656888122, 0.215882945067587, 0.2198734541271211, 0.2163143514524015, 0.21861518550474546, 0.21793212539545584, 0.21724906528616625, 0.21793212539545584, 0.2174647684785735, 0.2174647684785735, 0.21832758124820248, 0.21717716422203048, 0.21577509347138338, 0.21742881794650562, 0.21908254242162783, 0.216458153580673, 0.21764452113891286, 0.21617054932413, 0.21897469082542423, 0.21803997699165947, 0.2175726200747771, 0.21667385677308024, 0.21893874029335633]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.21721311475409835, -0.09933419614610298, -0.12136324417601381, -0.1194060972102387, -0.10668708656888122, -0.13588294506758702, -0.16273059698426395, -0.0913143514524015, -0.11861518550474545, -0.11793212539545583, -0.09724906528616625, -0.12043212539545584, -0.09524254625635127, -0.12146476847857349, -0.10499424791486915, -0.10860573565060191, -0.11577509347138337, -0.11965104016872784, -0.12608254242162784, -0.107658153580673, -0.1209778544722462, -0.10531340646698713, -0.11647469082542423, -0.1153733103249928, -0.1187726200747771, -0.11449203859126206, -0.11727207362668966]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.649711957285373, 0.009929277317221596, -0.008852044400730619, 0.015690131609760627, -0.02780314739356471, -0.005871253939260024, -0.15842349304482228, -0.04943094000281023, -0.10692707601517493, 0.03708549950821971, 0.06387121379393401, -0.0841693597489579, -0.0984567118041938, -0.013550977615892634, -0.05319227826955808, -0.014175932259394397, 0.0031369371076787056, -0.06949532306951156, -0.0757813874460842, -0.05741636336441147, -0.08363953489284837, -0.0213834546173427, -0.05792907826331317, -0.022536673009000086, -0.05580504056262614, -0.051288989584432065, -0.04894799820511542]}],\n",
              "                        {\"font\": {\"size\": 13}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Fairness and Performance Metrics for the US Adult Income Dataset - Unprivileged & Privileged Group\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Number of observations in the unprivileged group (privileged group has a fixed number of observations)\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1679a77a-1f4c-4ca2-96b6-fbc31f44dca6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L8M_z7m_5EL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "309daa4e-ac55-4114-a2a5-5b77b076a623"
      },
      "source": [
        "## Performance Learning Curve\n",
        "\n",
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "adult_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
        "              min_child_weight=1, missing=None, n_estimators=90, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = adult_model, \n",
        "                    title = \"US Adult Income Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u0QSdisyQ9O",
        "colab_type": "text"
      },
      "source": [
        "Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n2WFGwrK2Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b85h5SQhwdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f87c7733-3eee-4f0f-8b0f-5c6a416e1a3d"
      },
      "source": [
        "min_metrics_line_chart(metric_df = results_df_adult, title=\"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b699132b-7783-4b9d-ae5e-1a2ca500da84\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b699132b-7783-4b9d-ae5e-1a2ca500da84\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b699132b-7783-4b9d-ae5e-1a2ca500da84',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.3267813217360542, 0.32792954662262236, 0.3270887077232432, 0.3274289145848995, 0.3267593789280469, 0.24501800275520716, 0.1302053475218777, 0.15948712859036918, 0.1283363906204608, 0.13411602341849663, 0.14317539966921974, 0.13101015160498267, 0.1422699534487097, 0.14915314163327353, 0.13081200956861225, 0.130389579339852, 0.12385789795807225, 0.11310798143624734, 0.09021641483617, 0.0815897043759216, 0.0883760417233408, 0.08019345543673981, 0.08911963508073041, 0.07877864442440707, 0.07203656123854496, 0.0710333727188929, 0.06393517735438485, 0.07961696979813886, 0.06786803924057411, 0.05930816543296586, 0.1641122762417779]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.19359361518550475, -0.1942047742306586, -0.19384526890997986, -0.19366551624964048, -0.19362956571757262, -0.15442047742306586, -0.11406097210238712, -0.12384526890997985, -0.11467213114754098, -0.12087144089732527, -0.13044886807181888, -0.12380931837791198, -0.1302407247627265, -0.1342047742306586, -0.12617260364024818, -0.1289531205061835, -0.13216230466877577, -0.12813287316652286, -0.12388121944204775, -0.12077242286042976, -0.1217407247627265, -0.1214073434953504, -0.12399568593615186, -0.11852832901926948, -0.1174662064998562, -0.11605908213155841, -0.11448245614035087, -0.11892694851883807, -0.1154047742306586, -0.11287047350118964, -0.15082571182053495]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.5995503723478994, -0.6017985106084024, -0.5999718982717437, -0.6013769846845581, -0.599409863706618, -0.45894136775125954, -0.23760011240691303, -0.28915800196712094, -0.22140816422114729, -0.23731909512435012, -0.2528483545216291, -0.23170915752059495, -0.2489978427790956, -0.25935153548528594, -0.22665800196712094, -0.22429951140108317, -0.2066518728832858, -0.18585186065866127, -0.13958609112355147, -0.1224751377765288, -0.1358343871481429, -0.11833734671234003, -0.1342091705306543, -0.11811949441025593, -0.10565658209032491, -0.1077916943054788, -0.09415356317140777, -0.12411425711440383, -0.10236054517352822, -0.08447645565705253, -0.27828843674972964]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.2057396449704142, 0.4122415709522045, 0.361112759643917, 0.4109473684210527, 0.3776082932247316, 0.3251691065118627, 0.3611797440178075, 0.3294880621876735, 0.308952239911144, 0.35211055143859277, 0.33513253012048194, 0.3200986375685839, 0.33997333333333335, 0.36104580011125537, 0.3762707548671158, 0.37324819544697385, 0.3729917068944176, 0.3641106194690265, 0.3906885973017927, 0.3949185185185185, 0.4010576199310893, 0.4067727272727273, 0.3856876508820799, 0.4057572750833025, 0.41588742494714587, 0.22451608133086876]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \" Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b699132b-7783-4b9d-ae5e-1a2ca500da84');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrPVof-byOTQ",
        "colab_type": "text"
      },
      "source": [
        "Majority <-> Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridl2ll4YxAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8d8b8949-7c9d-4617-b82a-a9057b2f828a"
      },
      "source": [
        "maj_min_metrics_line_chart(metric_df=results_df_adult, title=\"Majority and Minority Metrics for the Adult Dataset with XGBoost\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"28fd47ae-efdd-4002-9250-83c9500b1bd5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"28fd47ae-efdd-4002-9250-83c9500b1bd5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '28fd47ae-efdd-4002-9250-83c9500b1bd5',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.6826107822748361, 0.6842399552679926, 0.6827084499160604, 0.6845809341010877, 0.6823962249060226, 0.6839121756487027, 0.68381941669996, 0.6846270685106723, 0.6841685285668688, 0.6832814122533748, 0.6843284776692812, 0.685161496642149, 0.6843450479233226, 0.6831216550842718, 0.6832615286420933, 0.684462915601023, 0.6855637176620888, 0.6837101541903012, 0.6828137490007994, 0.6833559945613052, 0.6849840255591054, 0.684315764216588, 0.6854317837493024, 0.6848659003831418, 0.6827514580170968, 0.6837770848324938, 0.6820985182218663, 0.6814909614461686, 0.6848789839444045, 0.6828370156900416, 0.6836433304063224]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.5995503723478994, 0.6017985106084024, 0.5999718982717437, 0.6013769846845581, 0.599409863706618, 0.6017985106084024, 0.6012364760432767, 0.6016580019671209, 0.6023605451735282, 0.6009554587607138, 0.6006744414781509, 0.6020795278909653, 0.6019390192496838, 0.6008149501194323, 0.6016580019671209, 0.6016580019671209, 0.6032035970212168, 0.6012364760432767, 0.6001124069130251, 0.6002529155543066, 0.6025010538148096, 0.6010959674019952, 0.603906140227624, 0.6027820710973725, 0.600393424195588, 0.6008149501194323, 0.5982857945763664, 0.5985668118589293, 0.6023605451735282, 0.5992693550653365, 0.6016580019671209]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0540122711242089, 0.05406058263684236, 0.05420551717474274, 0.05348084448524083, 0.05410889414947582, 0.054350451712643126, 0.05406058263684236, 0.053625779023141217, 0.0544953862505435, 0.054350451712643126, 0.05323928692207353, 0.053432532972607374, 0.05406058263684236, 0.054398763225276585, 0.05483356683897773, 0.05377071356104159, 0.05381902507367506, 0.054157205662109284, 0.05420551717474274, 0.05381902507367506, 0.05386733658630852, 0.05352915599787429, 0.054398763225276585, 0.054157205662109284, 0.054447074737910044, 0.05381902507367506, 0.053625779023141217, 0.054350451712643126, 0.05386733658630852, 0.053625779023141217, 0.0544953862505435]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('28fd47ae-efdd-4002-9250-83c9500b1bd5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssH5Y-F0Lsqt"
      },
      "source": [
        "## 2) COMPAS Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-gROLSu6ag",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ7OVi5Y-MvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_compas = \"/content/drive/My Drive/Master Thesis/Data/compas-scores-two-years_dataset.csv\"\n",
        "compas_column_names = ['id', 'name', 'first', 'last', 'compas screening date', 'sex', 'dob',\n",
        "                       'age', 'age cat', 'race', 'juv fel count', 'decile score',\n",
        "                       'juv misd count', 'juv other count', 'priors count',\n",
        "                       'days b screening arrest', 'c jail in', 'c jail out', 'c case number',\n",
        "                       'c offense date', 'c arrest date', 'c days from compas',\n",
        "                       'c charge degree', 'c charge desc', 'is recid', 'r case number',\n",
        "                       'r charge degree', 'r days from arrest', 'r offense date',\n",
        "                       'r charge desc', 'r jail in', 'r jail out', 'violent recid',\n",
        "                       'is violent recid', 'vr case number', 'vr charge degree',\n",
        "                       'vr offense date', 'vr charge desc', 'type of assessment',\n",
        "                       'decile score.1', 'score text', 'screening date',\n",
        "                       'v type of assessment', 'v decile score', 'v score text',\n",
        "                       'v screening date', 'in custody', 'out custody', 'priors count.1',\n",
        "                       'start', 'end', 'event', 'two year recid']\n",
        "df_compas = pd.read_csv(path_compas, low_memory=False, names = compas_column_names, header = 0, sep = \";\")"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6DV7Qcm1yV",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h69rOkMDjAan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4f0a6e07-b04c-480c-8d85-cfc6c50c2e82"
      },
      "source": [
        "# Rename columns\n",
        "df_compas = df_compas.rename(index=str, columns={\"decile score.1\":\"decile_score a\",\n",
        "                                                 \"priors count.1\":\"priors_count a\"})\n",
        "\n",
        "# Replace empty strings with NAs\n",
        "df_compas = df_compas.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_compas = df_compas.drop(['id', 'name', 'age cat', \"is recid\", \"event\", \"start\", \"end\"], axis=1)\n",
        "\n",
        "# Swap 1 and 0 for label feature so that 1 is associated with with no allocative harms (= no prediction individual recidivsm) \n",
        "df_compas[\"two year recid\"] = df_compas[\"two year recid\"].replace({1: \"recid\", 0: \"no_recid\"})\n",
        "df_compas[\"two year recid\"] = df_compas[\"two year recid\"].replace({\"recid\": 0, \"no_recid\": 1})\n",
        "\n",
        "\n",
        "# Drop NaN from label column \n",
        "df_compas = df_compas[df_compas['two year recid'].notna()]\n",
        "print(df_compas.shape)\n",
        "\n",
        "# Convert columns to datetime format\n",
        "# Columns with yyyy-mm-dd format:\n",
        "# date_columns = [\"dob\", \"c_offense_date\", \"c_arrest_date\", \"r_offense_date\", \"r_jail_out\", \"vr_offense_date\", \"screening_date\", \"v_screening_date\", \"v_screening_date\", \"in_custody\", \"out_custody\"]\n",
        "# for date_time in date_columns:\n",
        "#   df_compas[date_time]= pd.to_datetime(df_compas[date_time])\n",
        "\n",
        "# To Do: Columns with yyyy-mm-dd and hh:mm:ss -> c_jail_in, c_jail_out"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6873, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu9ppIqXJ0f",
        "colab_type": "text"
      },
      "source": [
        "Shrink the size of the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gRsvBLJW8IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_compas.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmDRGtwnX8Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "79d455f9-a715-423a-8cf9-98cde6cd989c"
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_compas_copy = df_compas.select_dtypes(include=['object']).copy()\n",
        "df_compas_copy.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>compas screening date</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>race</th>\n",
              "      <th>c jail in</th>\n",
              "      <th>c jail out</th>\n",
              "      <th>c case number</th>\n",
              "      <th>c offense date</th>\n",
              "      <th>c arrest date</th>\n",
              "      <th>c charge degree</th>\n",
              "      <th>c charge desc</th>\n",
              "      <th>r case number</th>\n",
              "      <th>r charge degree</th>\n",
              "      <th>r offense date</th>\n",
              "      <th>r charge desc</th>\n",
              "      <th>r jail in</th>\n",
              "      <th>r jail out</th>\n",
              "      <th>vr case number</th>\n",
              "      <th>vr charge degree</th>\n",
              "      <th>vr offense date</th>\n",
              "      <th>vr charge desc</th>\n",
              "      <th>type of assessment</th>\n",
              "      <th>score text</th>\n",
              "      <th>screening date</th>\n",
              "      <th>v type of assessment</th>\n",
              "      <th>v score text</th>\n",
              "      <th>v screening date</th>\n",
              "      <th>in custody</th>\n",
              "      <th>out custody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6574</td>\n",
              "      <td>6574</td>\n",
              "      <td>6852</td>\n",
              "      <td>5768</td>\n",
              "      <td>1084</td>\n",
              "      <td>6873</td>\n",
              "      <td>6847</td>\n",
              "      <td>3295</td>\n",
              "      <td>3295</td>\n",
              "      <td>3295</td>\n",
              "      <td>3240</td>\n",
              "      <td>2194</td>\n",
              "      <td>2194</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>776</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6873</td>\n",
              "      <td>6643</td>\n",
              "      <td>6643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2698</td>\n",
              "      <td>3812</td>\n",
              "      <td>687</td>\n",
              "      <td>2</td>\n",
              "      <td>5251</td>\n",
              "      <td>6</td>\n",
              "      <td>6521</td>\n",
              "      <td>5928</td>\n",
              "      <td>6852</td>\n",
              "      <td>918</td>\n",
              "      <td>564</td>\n",
              "      <td>2</td>\n",
              "      <td>429</td>\n",
              "      <td>3295</td>\n",
              "      <td>10</td>\n",
              "      <td>1057</td>\n",
              "      <td>329</td>\n",
              "      <td>945</td>\n",
              "      <td>923</td>\n",
              "      <td>776</td>\n",
              "      <td>9</td>\n",
              "      <td>549</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>687</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>687</td>\n",
              "      <td>1149</td>\n",
              "      <td>1164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>michael</td>\n",
              "      <td>williams</td>\n",
              "      <td>20.03.2013</td>\n",
              "      <td>Male</td>\n",
              "      <td>02.05.1990</td>\n",
              "      <td>African-American</td>\n",
              "      <td>11.09.2013 03:42</td>\n",
              "      <td>06.02.2014 09:10</td>\n",
              "      <td>13013079CF10A</td>\n",
              "      <td>14.01.2013</td>\n",
              "      <td>06.02.2013</td>\n",
              "      <td>F</td>\n",
              "      <td>Battery</td>\n",
              "      <td>14006200CF10A</td>\n",
              "      <td>M1</td>\n",
              "      <td>08.12.2014</td>\n",
              "      <td>Driving License Suspended</td>\n",
              "      <td>03.03.2015</td>\n",
              "      <td>18.02.2014</td>\n",
              "      <td>14001219MM30A</td>\n",
              "      <td>M1</td>\n",
              "      <td>15.08.2015</td>\n",
              "      <td>Battery</td>\n",
              "      <td>Risk of Recidivism</td>\n",
              "      <td>Low</td>\n",
              "      <td>20.03.2013</td>\n",
              "      <td>Risk of Violence</td>\n",
              "      <td>Low</td>\n",
              "      <td>20.03.2013</td>\n",
              "      <td>07.04.2013</td>\n",
              "      <td>01.01.2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>136</td>\n",
              "      <td>75</td>\n",
              "      <td>30</td>\n",
              "      <td>5553</td>\n",
              "      <td>5</td>\n",
              "      <td>3526</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>9</td>\n",
              "      <td>4456</td>\n",
              "      <td>1093</td>\n",
              "      <td>1</td>\n",
              "      <td>1139</td>\n",
              "      <td>12</td>\n",
              "      <td>246</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>327</td>\n",
              "      <td>6</td>\n",
              "      <td>312</td>\n",
              "      <td>6873</td>\n",
              "      <td>3722</td>\n",
              "      <td>30</td>\n",
              "      <td>6873</td>\n",
              "      <td>4532</td>\n",
              "      <td>30</td>\n",
              "      <td>19</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          first      last compas screening date   sex         dob  \\\n",
              "count      6873      6873                  6873  6873        6873   \n",
              "unique     2698      3812                   687     2        5251   \n",
              "top     michael  williams            20.03.2013  Male  02.05.1990   \n",
              "freq        136        75                    30  5553           5   \n",
              "\n",
              "                    race         c jail in        c jail out  c case number  \\\n",
              "count               6873              6574              6574           6852   \n",
              "unique                 6              6521              5928           6852   \n",
              "top     African-American  11.09.2013 03:42  06.02.2014 09:10  13013079CF10A   \n",
              "freq                3526                 2                 6              1   \n",
              "\n",
              "       c offense date c arrest date c charge degree c charge desc  \\\n",
              "count            5768          1084            6873          6847   \n",
              "unique            918           564               2           429   \n",
              "top        14.01.2013    06.02.2013               F       Battery   \n",
              "freq               26             9            4456          1093   \n",
              "\n",
              "        r case number r charge degree r offense date  \\\n",
              "count            3295            3295           3295   \n",
              "unique           3295              10           1057   \n",
              "top     14006200CF10A              M1     08.12.2014   \n",
              "freq                1            1139             12   \n",
              "\n",
              "                    r charge desc   r jail in  r jail out vr case number  \\\n",
              "count                        3240        2194        2194            776   \n",
              "unique                        329         945         923            776   \n",
              "top     Driving License Suspended  03.03.2015  18.02.2014  14001219MM30A   \n",
              "freq                          246           8           9              1   \n",
              "\n",
              "       vr charge degree vr offense date vr charge desc  type of assessment  \\\n",
              "count               776             776            776                6873   \n",
              "unique                9             549             79                   1   \n",
              "top                  M1      15.08.2015        Battery  Risk of Recidivism   \n",
              "freq                327               6            312                6873   \n",
              "\n",
              "       score text screening date v type of assessment v score text  \\\n",
              "count        6873           6873                 6873         6873   \n",
              "unique          3            687                    1            3   \n",
              "top           Low     20.03.2013     Risk of Violence          Low   \n",
              "freq         3722             30                 6873         4532   \n",
              "\n",
              "       v screening date  in custody out custody  \n",
              "count              6873        6643        6643  \n",
              "unique              687        1149        1164  \n",
              "top          20.03.2013  07.04.2013  01.01.2020  \n",
              "freq                 30          19          60  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T24jQWn9W6Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_compas, NAlist = reduce_mem_usage(df_compas)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLIyfdRtW7QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in [\"race\", \"sex\",\"first\",\"compas screening date\",\"c offense date\",\"c charge degree\",\"c charge desc\",\"r charge degree\",\"r offense date\",\"r charge desc\",\"r jail in\",\"r jail out\",\"vr case number\",\"vr charge degree\",\"vr offense date\",\"vr charge desc\",\"type of assessment\",\"score text\",\"screening date\",\"v type of assessment\",\"v score text\",\"v screening date\",\"in custody\",\"out custody\"]:\n",
        "    df_compas[col] = df_compas[col].astype('category')"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_xvYT2xu8L8",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkVD1gpDo3d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "d640e83b-ef7a-4d9a-f0a3-a0cb78b03ade"
      },
      "source": [
        "# Analyze class balance for dataset with only unprivileged and privileged dataset\n",
        "is_unpriv = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_priv = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_unpriv_compas = df_compas[is_unpriv]\n",
        "df_priv_comas = df_compas[is_priv]\n",
        "\n",
        "df_unpriv_priv_complete_compas = pd.concat([df_unpriv_compas, df_priv_comas])\n",
        "\n",
        "print(df_unpriv_priv_complete_compas[\"two year recid\"].value_counts(dropna=False))\n",
        "print(df_unpriv_priv_complete_compas[\"two year recid\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    3139\n",
            "0    2718\n",
            "Name: two year recid, dtype: int64\n",
            "1    0.53594\n",
            "0    0.46406\n",
            "Name: two year recid, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVIMufYBsBxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b84d3997-cc31-4084-b046-36d02fd1c102"
      },
      "source": [
        "df_compas.set_index('race').isna().sum(level=0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>compas screening date</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>age</th>\n",
              "      <th>juv fel count</th>\n",
              "      <th>decile score</th>\n",
              "      <th>juv misd count</th>\n",
              "      <th>juv other count</th>\n",
              "      <th>priors count</th>\n",
              "      <th>days b screening arrest</th>\n",
              "      <th>c jail in</th>\n",
              "      <th>c jail out</th>\n",
              "      <th>c case number</th>\n",
              "      <th>c offense date</th>\n",
              "      <th>c arrest date</th>\n",
              "      <th>c days from compas</th>\n",
              "      <th>c charge degree</th>\n",
              "      <th>c charge desc</th>\n",
              "      <th>r case number</th>\n",
              "      <th>r charge degree</th>\n",
              "      <th>r days from arrest</th>\n",
              "      <th>r offense date</th>\n",
              "      <th>r charge desc</th>\n",
              "      <th>r jail in</th>\n",
              "      <th>r jail out</th>\n",
              "      <th>violent recid</th>\n",
              "      <th>is violent recid</th>\n",
              "      <th>vr case number</th>\n",
              "      <th>vr charge degree</th>\n",
              "      <th>vr offense date</th>\n",
              "      <th>vr charge desc</th>\n",
              "      <th>type of assessment</th>\n",
              "      <th>decile_score a</th>\n",
              "      <th>score text</th>\n",
              "      <th>screening date</th>\n",
              "      <th>v type of assessment</th>\n",
              "      <th>v decile score</th>\n",
              "      <th>v score text</th>\n",
              "      <th>v screening date</th>\n",
              "      <th>in custody</th>\n",
              "      <th>out custody</th>\n",
              "      <th>priors_count a</th>\n",
              "      <th>two year recid</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>race</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Other</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>362.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>African-American</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>623.0</td>\n",
              "      <td>2911.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1595.0</td>\n",
              "      <td>1595.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1595.0</td>\n",
              "      <td>1628.0</td>\n",
              "      <td>2250.0</td>\n",
              "      <td>2250.0</td>\n",
              "      <td>3526.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3055.0</td>\n",
              "      <td>3055.0</td>\n",
              "      <td>3055.0</td>\n",
              "      <td>3055.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Caucasian</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1356.0</td>\n",
              "      <td>1356.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1356.0</td>\n",
              "      <td>1372.0</td>\n",
              "      <td>1657.0</td>\n",
              "      <td>1657.0</td>\n",
              "      <td>2331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hispanic</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>514.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>378.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>607.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Native American</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  first  last  compas screening date  sex  dob  age  \\\n",
              "race                                                                  \n",
              "Other               0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "African-American    0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "Caucasian           0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "Hispanic            0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "Native American     0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "Asian               0.0   0.0                    0.0  0.0  0.0  0.0   \n",
              "\n",
              "                  juv fel count  decile score  juv misd count  \\\n",
              "race                                                            \n",
              "Other                       0.0           0.0             0.0   \n",
              "African-American            0.0           0.0             0.0   \n",
              "Caucasian                   0.0           0.0             0.0   \n",
              "Hispanic                    0.0           0.0             0.0   \n",
              "Native American             0.0           0.0             0.0   \n",
              "Asian                       0.0           0.0             0.0   \n",
              "\n",
              "                  juv other count  priors count  days b screening arrest  \\\n",
              "race                                                                       \n",
              "Other                         0.0           0.0                      0.0   \n",
              "African-American              0.0           0.0                      0.0   \n",
              "Caucasian                     0.0           0.0                      0.0   \n",
              "Hispanic                      0.0           0.0                      0.0   \n",
              "Native American               0.0           0.0                      0.0   \n",
              "Asian                         0.0           0.0                      0.0   \n",
              "\n",
              "                  c jail in  c jail out  c case number  c offense date  \\\n",
              "race                                                                     \n",
              "Other                  17.0        17.0            1.0            49.0   \n",
              "African-American      154.0       154.0            8.0           623.0   \n",
              "Caucasian              74.0        74.0            5.0           327.0   \n",
              "Hispanic               52.0        52.0            6.0            99.0   \n",
              "Native American         2.0         2.0            1.0             4.0   \n",
              "Asian                   0.0         0.0            0.0             3.0   \n",
              "\n",
              "                  c arrest date  c days from compas  c charge degree  \\\n",
              "race                                                                   \n",
              "Other                     314.0                 0.0              0.0   \n",
              "African-American         2911.0                 0.0              0.0   \n",
              "Caucasian                2009.0                 0.0              0.0   \n",
              "Hispanic                  514.0                 0.0              0.0   \n",
              "Native American            15.0                 0.0              0.0   \n",
              "Asian                      26.0                 0.0              0.0   \n",
              "\n",
              "                  c charge desc  r case number  r charge degree  \\\n",
              "race                                                              \n",
              "Other                       1.0          225.0            225.0   \n",
              "African-American           11.0         1595.0           1595.0   \n",
              "Caucasian                   7.0         1356.0           1356.0   \n",
              "Hispanic                    6.0          375.0            375.0   \n",
              "Native American             1.0            7.0              7.0   \n",
              "Asian                       0.0           20.0             20.0   \n",
              "\n",
              "                  r days from arrest  r offense date  r charge desc  \\\n",
              "race                                                                  \n",
              "Other                            0.0           225.0          228.0   \n",
              "African-American                 0.0          1595.0         1628.0   \n",
              "Caucasian                        0.0          1356.0         1372.0   \n",
              "Hispanic                         0.0           375.0          378.0   \n",
              "Native American                  0.0             7.0            7.0   \n",
              "Asian                            0.0            20.0           20.0   \n",
              "\n",
              "                  r jail in  r jail out  violent recid  is violent recid  \\\n",
              "race                                                                       \n",
              "Other                 266.0       266.0          362.0               0.0   \n",
              "African-American     2250.0      2250.0         3526.0               0.0   \n",
              "Caucasian            1657.0      1657.0         2331.0               0.0   \n",
              "Hispanic              470.0       470.0          607.0               0.0   \n",
              "Native American        12.0        12.0           18.0               0.0   \n",
              "Asian                  24.0        24.0           29.0               0.0   \n",
              "\n",
              "                  vr case number  vr charge degree  vr offense date  \\\n",
              "race                                                                  \n",
              "Other                      325.0             325.0            325.0   \n",
              "African-American          3055.0            3055.0           3055.0   \n",
              "Caucasian                 2122.0            2122.0           2122.0   \n",
              "Hispanic                   554.0             554.0            554.0   \n",
              "Native American             14.0              14.0             14.0   \n",
              "Asian                       27.0              27.0             27.0   \n",
              "\n",
              "                  vr charge desc  type of assessment  decile_score a  \\\n",
              "race                                                                   \n",
              "Other                      325.0                 0.0             0.0   \n",
              "African-American          3055.0                 0.0             0.0   \n",
              "Caucasian                 2122.0                 0.0             0.0   \n",
              "Hispanic                   554.0                 0.0             0.0   \n",
              "Native American             14.0                 0.0             0.0   \n",
              "Asian                       27.0                 0.0             0.0   \n",
              "\n",
              "                  score text  screening date  v type of assessment  \\\n",
              "race                                                                 \n",
              "Other                    0.0             0.0                   0.0   \n",
              "African-American         0.0             0.0                   0.0   \n",
              "Caucasian                0.0             0.0                   0.0   \n",
              "Hispanic                 0.0             0.0                   0.0   \n",
              "Native American          0.0             0.0                   0.0   \n",
              "Asian                    0.0             0.0                   0.0   \n",
              "\n",
              "                  v decile score  v score text  v screening date  in custody  \\\n",
              "race                                                                           \n",
              "Other                        0.0           0.0               0.0        17.0   \n",
              "African-American             0.0           0.0               0.0       110.0   \n",
              "Caucasian                    0.0           0.0               0.0        65.0   \n",
              "Hispanic                     0.0           0.0               0.0        37.0   \n",
              "Native American              0.0           0.0               0.0         1.0   \n",
              "Asian                        0.0           0.0               0.0         0.0   \n",
              "\n",
              "                  out custody  priors_count a  two year recid  \n",
              "race                                                           \n",
              "Other                    17.0             0.0             0.0  \n",
              "African-American        110.0             0.0             0.0  \n",
              "Caucasian                65.0             0.0             0.0  \n",
              "Hispanic                 37.0             0.0             0.0  \n",
              "Native American           1.0             0.0             0.0  \n",
              "Asian                     0.0             0.0             0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbt9k0dsxkfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check column names\n",
        "print(df_compas.columns)\n",
        "# Check datatypes of columns\n",
        "print(df_compas.dtypes)\n",
        "df_compas.head(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duNufuy52C28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_compas.columns)\n",
        "print(df_compas[\"race\"].unique())\n",
        "print(df_compas.groupby([\"race\"]).agg({\"race\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_-Y8dJtWG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_compas.set_index('race').isna().sum(level=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twN3wQQh2LHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2b53fd5-fb31-4922-ccc4-ea6c3453d029"
      },
      "source": [
        "eda_descr_stats(data = df_compas, disc_feature = \"race\", disc_min_value=\"African-American\", \n",
        "                label = \"two year recid\", second_disc_feature=\"sex\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Sensitive Attribute: One or more of the following features are sensitive ones: Index(['first', 'last', 'compas screening date', 'sex', 'dob', 'age', 'race',\n",
            "       'juv fel count', 'decile score', 'juv misd count', 'juv other count',\n",
            "       'priors count', 'days b screening arrest', 'c jail in', 'c jail out',\n",
            "       'c case number', 'c offense date', 'c arrest date',\n",
            "       'c days from compas', 'c charge degree', 'c charge desc',\n",
            "       'r case number', 'r charge degree', 'r days from arrest',\n",
            "       'r offense date', 'r charge desc', 'r jail in', 'r jail out',\n",
            "       'violent recid', 'is violent recid', 'vr case number',\n",
            "       'vr charge degree', 'vr offense date', 'vr charge desc',\n",
            "       'type of assessment', 'decile_score a', 'score text', 'screening date',\n",
            "       'v type of assessment', 'v decile score', 'v score text',\n",
            "       'v screening date', 'in custody', 'out custody', 'priors_count a',\n",
            "       'two year recid'],\n",
            "      dtype='object').\n",
            "1. Sensitive Attribute: These are the individual values for the sensitive attribute: [Other, African-American, Caucasian, Hispanic, Native American, Asian]\n",
            "Categories (6, object): [Other, African-American, Caucasian, Hispanic, Native American, Asian].\n",
            "2. Binary Target Variable: The Binary Target Feature has the following values and counts:\n",
            "                two year recid\n",
            "two year recid                \n",
            "0                         3792\n",
            "1                         3081\n",
            "3. The Total Number of Predictor Features is: 46.\n",
            "4. The Total Number of Training Examples is: 6873.\n",
            "5. The Total Number of Training Examples in the Minority Group is: 3526.\n",
            "6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  African-American    3526\n",
            "Caucasian           2331\n",
            "Hispanic             607\n",
            "Other                362\n",
            "Asian                 29\n",
            "Native American       18\n",
            "Name: race, dtype: int64.\n",
            "6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: African-American    0.513022\n",
            "Caucasian           0.339153\n",
            "Hispanic            0.088317\n",
            "Other               0.052670\n",
            "Asian               0.004219\n",
            "Native American     0.002619\n",
            "Name: race, dtype: float64.\n",
            "7. Class Balance: The Class Balance looks as follows:\n",
            "0    3792\n",
            "1    3081\n",
            "Name: two year recid, dtype: int64\n",
            "0    0.551724\n",
            "1    0.448276\n",
            "Name: two year recid, dtype: float64\n",
            "8. Coarseness of Features: Details on missing values of features in the dataset:\n",
            "first                         0\n",
            "last                          0\n",
            "compas screening date         0\n",
            "sex                           0\n",
            "dob                           0\n",
            "age                           0\n",
            "race                          0\n",
            "juv fel count                 0\n",
            "decile score                  0\n",
            "juv misd count                0\n",
            "juv other count               0\n",
            "priors count                  0\n",
            "days b screening arrest       0\n",
            "c jail in                   299\n",
            "c jail out                  299\n",
            "c case number                21\n",
            "c offense date             1105\n",
            "c arrest date              5789\n",
            "c days from compas            0\n",
            "c charge degree               0\n",
            "c charge desc                26\n",
            "r case number              3578\n",
            "r charge degree            3578\n",
            "r days from arrest            0\n",
            "r offense date             3578\n",
            "r charge desc              3633\n",
            "r jail in                  4679\n",
            "r jail out                 4679\n",
            "violent recid              6873\n",
            "is violent recid              0\n",
            "vr case number             6097\n",
            "vr charge degree           6097\n",
            "vr offense date            6097\n",
            "vr charge desc             6097\n",
            "type of assessment            0\n",
            "decile_score a                0\n",
            "score text                    0\n",
            "screening date                0\n",
            "v type of assessment          0\n",
            "v decile score                0\n",
            "v score text                  0\n",
            "v screening date              0\n",
            "in custody                  230\n",
            "out custody                 230\n",
            "priors_count a                0\n",
            "two year recid                0\n",
            "dtype: int64\n",
            "                  first  last  compas screening date  sex  dob  age  race  \\\n",
            "race                                                                        \n",
            "African-American      0     0                      0    0    0    0     0   \n",
            "Asian                 0     0                      0    0    0    0     0   \n",
            "Caucasian             0     0                      0    0    0    0     0   \n",
            "Hispanic              0     0                      0    0    0    0     0   \n",
            "Native American       0     0                      0    0    0    0     0   \n",
            "Other                 0     0                      0    0    0    0     0   \n",
            "\n",
            "                  juv fel count  decile score  juv misd count  \\\n",
            "race                                                            \n",
            "African-American              0             0               0   \n",
            "Asian                         0             0               0   \n",
            "Caucasian                     0             0               0   \n",
            "Hispanic                      0             0               0   \n",
            "Native American               0             0               0   \n",
            "Other                         0             0               0   \n",
            "\n",
            "                  juv other count  priors count  days b screening arrest  \\\n",
            "race                                                                       \n",
            "African-American                0             0                        0   \n",
            "Asian                           0             0                        0   \n",
            "Caucasian                       0             0                        0   \n",
            "Hispanic                        0             0                        0   \n",
            "Native American                 0             0                        0   \n",
            "Other                           0             0                        0   \n",
            "\n",
            "                  c jail in  c jail out  c case number  c offense date  \\\n",
            "race                                                                     \n",
            "African-American        154         154              8             623   \n",
            "Asian                     0           0              0               3   \n",
            "Caucasian                74          74              5             327   \n",
            "Hispanic                 52          52              6              99   \n",
            "Native American           2           2              1               4   \n",
            "Other                    17          17              1              49   \n",
            "\n",
            "                  c arrest date  c days from compas  c charge degree  \\\n",
            "race                                                                   \n",
            "African-American           2911                   0                0   \n",
            "Asian                        26                   0                0   \n",
            "Caucasian                  2009                   0                0   \n",
            "Hispanic                    514                   0                0   \n",
            "Native American              15                   0                0   \n",
            "Other                       314                   0                0   \n",
            "\n",
            "                  c charge desc  r case number  r charge degree  \\\n",
            "race                                                              \n",
            "African-American             11           1595             1595   \n",
            "Asian                         0             20               20   \n",
            "Caucasian                     7           1356             1356   \n",
            "Hispanic                      6            375              375   \n",
            "Native American               1              7                7   \n",
            "Other                         1            225              225   \n",
            "\n",
            "                  r days from arrest  r offense date  r charge desc  \\\n",
            "race                                                                  \n",
            "African-American                   0            1595           1628   \n",
            "Asian                              0              20             20   \n",
            "Caucasian                          0            1356           1372   \n",
            "Hispanic                           0             375            378   \n",
            "Native American                    0               7              7   \n",
            "Other                              0             225            228   \n",
            "\n",
            "                  r jail in  r jail out  violent recid  is violent recid  \\\n",
            "race                                                                       \n",
            "African-American       2250        2250           3526                 0   \n",
            "Asian                    24          24             29                 0   \n",
            "Caucasian              1657        1657           2331                 0   \n",
            "Hispanic                470         470            607                 0   \n",
            "Native American          12          12             18                 0   \n",
            "Other                   266         266            362                 0   \n",
            "\n",
            "                  vr case number  vr charge degree  vr offense date  \\\n",
            "race                                                                  \n",
            "African-American            3055              3055             3055   \n",
            "Asian                         27                27               27   \n",
            "Caucasian                   2122              2122             2122   \n",
            "Hispanic                     554               554              554   \n",
            "Native American               14                14               14   \n",
            "Other                        325               325              325   \n",
            "\n",
            "                  vr charge desc  type of assessment  decile_score a  \\\n",
            "race                                                                   \n",
            "African-American            3055                   0               0   \n",
            "Asian                         27                   0               0   \n",
            "Caucasian                   2122                   0               0   \n",
            "Hispanic                     554                   0               0   \n",
            "Native American               14                   0               0   \n",
            "Other                        325                   0               0   \n",
            "\n",
            "                  score text  screening date  v type of assessment  \\\n",
            "race                                                                 \n",
            "African-American           0               0                     0   \n",
            "Asian                      0               0                     0   \n",
            "Caucasian                  0               0                     0   \n",
            "Hispanic                   0               0                     0   \n",
            "Native American            0               0                     0   \n",
            "Other                      0               0                     0   \n",
            "\n",
            "                  v decile score  v score text  v screening date  in custody  \\\n",
            "race                                                                           \n",
            "African-American               0             0                 0         110   \n",
            "Asian                          0             0                 0           0   \n",
            "Caucasian                      0             0                 0          65   \n",
            "Hispanic                       0             0                 0          37   \n",
            "Native American                0             0                 0           1   \n",
            "Other                          0             0                 0          17   \n",
            "\n",
            "                  out custody  priors_count a  two year recid  \n",
            "race                                                           \n",
            "African-American          110               0               0  \n",
            "Asian                       0               0               0  \n",
            "Caucasian                  65               0               0  \n",
            "Hispanic                   37               0               0  \n",
            "Native American             1               0               0  \n",
            "Other                      17               0               0  \n",
            "                  first  last  compas screening date  sex  dob  age  race  \\\n",
            "race                                                                        \n",
            "African-American    0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "Asian               0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "Caucasian           0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "Hispanic            0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "Native American     0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "Other               0.0   0.0                    0.0  0.0  0.0  0.0   0.0   \n",
            "\n",
            "                  juv fel count  decile score  juv misd count  \\\n",
            "race                                                            \n",
            "African-American            0.0           0.0             0.0   \n",
            "Asian                       0.0           0.0             0.0   \n",
            "Caucasian                   0.0           0.0             0.0   \n",
            "Hispanic                    0.0           0.0             0.0   \n",
            "Native American             0.0           0.0             0.0   \n",
            "Other                       0.0           0.0             0.0   \n",
            "\n",
            "                  juv other count  priors count  days b screening arrest  \\\n",
            "race                                                                       \n",
            "African-American              0.0           0.0                      0.0   \n",
            "Asian                         0.0           0.0                      0.0   \n",
            "Caucasian                     0.0           0.0                      0.0   \n",
            "Hispanic                      0.0           0.0                      0.0   \n",
            "Native American               0.0           0.0                      0.0   \n",
            "Other                         0.0           0.0                      0.0   \n",
            "\n",
            "                  c jail in  c jail out  c case number  c offense date  \\\n",
            "race                                                                     \n",
            "African-American   0.043676    0.043676       0.002269        0.176687   \n",
            "Asian              0.000000    0.000000       0.000000        0.103448   \n",
            "Caucasian          0.031746    0.031746       0.002145        0.140283   \n",
            "Hispanic           0.085667    0.085667       0.009885        0.163097   \n",
            "Native American    0.111111    0.111111       0.055556        0.222222   \n",
            "Other              0.046961    0.046961       0.002762        0.135359   \n",
            "\n",
            "                  c arrest date  c days from compas  c charge degree  \\\n",
            "race                                                                   \n",
            "African-American       0.825581                 0.0              0.0   \n",
            "Asian                  0.896552                 0.0              0.0   \n",
            "Caucasian              0.861862                 0.0              0.0   \n",
            "Hispanic               0.846787                 0.0              0.0   \n",
            "Native American        0.833333                 0.0              0.0   \n",
            "Other                  0.867403                 0.0              0.0   \n",
            "\n",
            "                  c charge desc  r case number  r charge degree  \\\n",
            "race                                                              \n",
            "African-American       0.003120       0.452354         0.452354   \n",
            "Asian                  0.000000       0.689655         0.689655   \n",
            "Caucasian              0.003003       0.581725         0.581725   \n",
            "Hispanic               0.009885       0.617792         0.617792   \n",
            "Native American        0.055556       0.388889         0.388889   \n",
            "Other                  0.002762       0.621547         0.621547   \n",
            "\n",
            "                  r days from arrest  r offense date  r charge desc  \\\n",
            "race                                                                  \n",
            "African-American                 0.0        0.452354       0.461713   \n",
            "Asian                            0.0        0.689655       0.689655   \n",
            "Caucasian                        0.0        0.581725       0.588589   \n",
            "Hispanic                         0.0        0.617792       0.622735   \n",
            "Native American                  0.0        0.388889       0.388889   \n",
            "Other                            0.0        0.621547       0.629834   \n",
            "\n",
            "                  r jail in  r jail out  violent recid  is violent recid  \\\n",
            "race                                                                       \n",
            "African-American   0.638117    0.638117            1.0               0.0   \n",
            "Asian              0.827586    0.827586            1.0               0.0   \n",
            "Caucasian          0.710854    0.710854            1.0               0.0   \n",
            "Hispanic           0.774300    0.774300            1.0               0.0   \n",
            "Native American    0.666667    0.666667            1.0               0.0   \n",
            "Other              0.734807    0.734807            1.0               0.0   \n",
            "\n",
            "                  vr case number  vr charge degree  vr offense date  \\\n",
            "race                                                                  \n",
            "African-American        0.866421          0.866421         0.866421   \n",
            "Asian                   0.931034          0.931034         0.931034   \n",
            "Caucasian               0.910339          0.910339         0.910339   \n",
            "Hispanic                0.912685          0.912685         0.912685   \n",
            "Native American         0.777778          0.777778         0.777778   \n",
            "Other                   0.897790          0.897790         0.897790   \n",
            "\n",
            "                  vr charge desc  type of assessment  decile_score a  \\\n",
            "race                                                                   \n",
            "African-American        0.866421                 0.0             0.0   \n",
            "Asian                   0.931034                 0.0             0.0   \n",
            "Caucasian               0.910339                 0.0             0.0   \n",
            "Hispanic                0.912685                 0.0             0.0   \n",
            "Native American         0.777778                 0.0             0.0   \n",
            "Other                   0.897790                 0.0             0.0   \n",
            "\n",
            "                  score text  screening date  v type of assessment  \\\n",
            "race                                                                 \n",
            "African-American         0.0             0.0                   0.0   \n",
            "Asian                    0.0             0.0                   0.0   \n",
            "Caucasian                0.0             0.0                   0.0   \n",
            "Hispanic                 0.0             0.0                   0.0   \n",
            "Native American          0.0             0.0                   0.0   \n",
            "Other                    0.0             0.0                   0.0   \n",
            "\n",
            "                  v decile score  v score text  v screening date  in custody  \\\n",
            "race                                                                           \n",
            "African-American             0.0           0.0               0.0    0.031197   \n",
            "Asian                        0.0           0.0               0.0    0.000000   \n",
            "Caucasian                    0.0           0.0               0.0    0.027885   \n",
            "Hispanic                     0.0           0.0               0.0    0.060956   \n",
            "Native American              0.0           0.0               0.0    0.055556   \n",
            "Other                        0.0           0.0               0.0    0.046961   \n",
            "\n",
            "                  out custody  priors_count a  two year recid  \n",
            "race                                                           \n",
            "African-American     0.031197             0.0             0.0  \n",
            "Asian                0.000000             0.0             0.0  \n",
            "Caucasian            0.027885             0.0             0.0  \n",
            "Hispanic             0.060956             0.0             0.0  \n",
            "Native American      0.055556             0.0             0.0  \n",
            "Other                0.046961             0.0             0.0  \n",
            "9. Severity of Outliers for Numeric Features\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YznLf--cDdd",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgBktRpZcHdP",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hqczOKFBeeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_compas_train_input, df_compas_train_label = fair_preprocess(data = df_compas, \n",
        "                                                               label = \"two year recid\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "topfOxXKGbYK",
        "colab_type": "text"
      },
      "source": [
        "Attempt to resolve data leakage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ZZJ8D1HEyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_compas_train_input)\n",
        "print(df_compas_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKGgY5KHReRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_compas_train_input_dopped = df_compas_train_input.drop(columns=[\"\", \"\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSSHUz-MJ6Md",
        "colab_type": "text"
      },
      "source": [
        "Attempt to resolve data leakage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0pMUCk-1FjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "664643ce-7e8c-4431-fb76-92183bf9f336"
      },
      "source": [
        "# Attempt to resolve data leakage\n",
        "# Evaluate using Cross Validation\n",
        "\n",
        "model_compas = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.4, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "df_compas_train_input_dummy_test = pd.get_dummies(df_compas_train_input)\n",
        "X = df_compas_train_input_dummy_test\n",
        "Y = df_compas_train_label\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Results\n",
        "results = model_selection.cross_val_score(model_compas, X, Y, cv=kfold, scoring = f1)\n",
        "print(results)\n",
        "print(results.std())\n",
        "print(results.mean())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9659256329281494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uAkUV-YJ_DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_compas, \n",
        "                    title = \"COMPAS Dataset - Performance Learning Curve\", \n",
        "                    X = df_compas_train_input_dummy_test, y = df_compas_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oObfHusUi7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "compas_model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(compas_model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SbCWg77cIEG",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKin9sSZaZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "86c1bb9f-63a2-47b4-9080-34031743743d"
      },
      "source": [
        "df_compas_train_input.dtypes"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "first                      category\n",
              "last                         object\n",
              "compas screening date      category\n",
              "sex                        category\n",
              "dob                          object\n",
              "age                           uint8\n",
              "race                       category\n",
              "juv fel count                 uint8\n",
              "decile score                  uint8\n",
              "juv misd count                uint8\n",
              "juv other count               uint8\n",
              "priors count                  uint8\n",
              "days b screening arrest       int16\n",
              "c jail in                    object\n",
              "c jail out                   object\n",
              "c case number                object\n",
              "c offense date             category\n",
              "c arrest date                object\n",
              "c days from compas           uint16\n",
              "c charge degree            category\n",
              "c charge desc              category\n",
              "r case number                object\n",
              "r charge degree            category\n",
              "r days from arrest            int16\n",
              "r offense date             category\n",
              "r charge desc              category\n",
              "r jail in                  category\n",
              "r jail out                 category\n",
              "violent recid               float64\n",
              "is violent recid              uint8\n",
              "vr case number             category\n",
              "vr charge degree           category\n",
              "vr offense date            category\n",
              "vr charge desc             category\n",
              "type of assessment         category\n",
              "decile_score a                uint8\n",
              "score text                 category\n",
              "screening date             category\n",
              "v type of assessment       category\n",
              "v decile score                uint8\n",
              "v score text               category\n",
              "v screening date           category\n",
              "in custody                 category\n",
              "out custody                category\n",
              "priors_count a                uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AQ5ZvcFm5ZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "e0674b98-1f24-495a-9984-6bdc92fa8ac0"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3, 0.4, 0.5],\n",
        "                \"n_estimators\": [10],\n",
        "                \"max_depth\": [7], \n",
        "                \"min_child_weight\": [1],       \n",
        "                \"reg_lambda\": [1.4]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= recall, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_compas_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.4, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 10, 'reg_lambda': 1.4}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.4, max_delta_step=0, max_depth=7,\n",
            "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1.4, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFHoRCNcIes",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqNYcdQxExno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "06c9585a-d1a4-48ff-91ec-da5c3097317c"
      },
      "source": [
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_priv = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_unpriv = df_compas[is_unpriv]\n",
        "df_compas_priv = df_compas[is_priv]\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_unpriv, maj_data = df_compas_priv, training_sizes=training_sizes)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n",
            "[2336, 2341, 2351, 2361, 2371, 2381, 2406, 2431, 2456, 2481, 2506, 2531, 2581, 2631, 2681, 2731, 2781, 2831, 2931, 3031, 3131, 3231, 3331, 3581, 3831, 4081, 4331, 4581, 4831, 5081, 5331, 5581, 5831]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC9IPkgMelcZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6aaca55c-4e32-44e7-a593-02a0b62ee193"
      },
      "source": [
        "print(len(df_compas_unpriv))\n",
        "print(len(df_compas_priv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3526\n",
            "2331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYaTGG5cI-P",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTBnMllH4RI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define arguments\n",
        "label = \"two year recid\"\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                             learning_rate=0.3, max_delta_step=0, max_depth=7,\n",
        "                             min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "                             nthread=4, objective='binary:logistic', random_state=0,\n",
        "                             reg_alpha=0, reg_lambda=1.4, scale_pos_weight=1, seed=None,\n",
        "                             silent=None, subsample=1, verbosity=1)\n",
        "cv = 3 \n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "# Run function\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = compas_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_compas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fGAue8FS2WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_compas.to_csv(\"df_compas_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_compas_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1230qeIkcp-a",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usVDIXIhSuOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart_selection(metric_df = results_df_compas, title=\"Fairness and Performance Metrics for the COMPAS Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2DKNaYH3X9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "aab5ef9f-9d7c-4a32-d5a6-214f3364f964"
      },
      "source": [
        "print(df_compas[\"two year recid\"].unique())\n",
        "print(df_compas.shape)\n",
        "df_compas[\"two year recid\"].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1. nan]\n",
            "(6874, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeup5l7RXeoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c05019ad-72ea-4ba7-f184-798d5e6b51fa"
      },
      "source": [
        "# Performance Learning Curve\n",
        "\n",
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Dummy Coding\n",
        "df_compas_train_input = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = compas_model, \n",
        "                    title = \"COMPAS Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_train_input_dummy_test, y = df_compas_train_label, \n",
        "                    cv = 3, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sdh1b0FYzDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwnExnGiYzmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYLBIMnHMHQy"
      },
      "source": [
        "## 3) Homicide Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbdFqBmv7QI",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otsY0WTLCVdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_homicide = \"/content/drive/My Drive/Master Thesis/Data/homicide_dataset.csv\"\n",
        "df_homicide_full = pd.read_csv(path_homicide, low_memory=False)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8zayFMxm3vA",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx41WDcL4eAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "df_homicide_full[\"Victim\"] = df_homicide_full[\"Victim Count\"].apply(lambda val: False if val >= 1 else val == 0)\n",
        "\n",
        "# Replace False by 1 and True by 0\n",
        "df_homicide_full[\"Victim\"] = df_homicide_full[\"Victim\"].replace({False: 0, True: 1})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_homicide_full = df_homicide_full.drop(['Record ID', \"Agency Code\", \"Perpetrator Count\", \"Victim Count\", \"Victim Sex\", \"Victim Age\", \"Victim Race\", \"Victim Ethnicity\"], axis=1)\n",
        "\n",
        "# Replace values by NaNs\n",
        "df_homicide_full = df_homicide_full.replace('Unknown', np.nan)\n",
        "df_homicide_full[\"Perpetrator Age\"] = df_homicide_full[\"Perpetrator Age\"].replace(0, np.nan)\n",
        "\n",
        "# Filter dataset based on year to reduce file size\n",
        "df_homicide = df_homicide_full[df_homicide_full[\"Year\"] > 2012]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4A2rRdBdDcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "40f01393-5ec4-4c6b-cee0-7839dd349e4d"
      },
      "source": [
        "df_homicide.set_index('Perpetrator Race').isna().sum(level=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Agency Name</th>\n",
              "      <th>Agency Type</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Incident</th>\n",
              "      <th>Crime Type</th>\n",
              "      <th>Crime Solved</th>\n",
              "      <th>Victim Sex</th>\n",
              "      <th>Victim Age</th>\n",
              "      <th>Victim Race</th>\n",
              "      <th>Victim Ethnicity</th>\n",
              "      <th>Perpetrator Sex</th>\n",
              "      <th>Perpetrator Age</th>\n",
              "      <th>Perpetrator Ethnicity</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Weapon</th>\n",
              "      <th>Record Source</th>\n",
              "      <th>Victim</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perpetrator Race</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Native American/Alaska Native</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>5265.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5208.0</td>\n",
              "      <td>2261.0</td>\n",
              "      <td>827.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Black</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>6064.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6136.0</td>\n",
              "      <td>3831.0</td>\n",
              "      <td>552.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian/Pacific Islander</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Agency Name  Agency Type  City  State  Year  \\\n",
              "Perpetrator Race                                                             \n",
              "Native American/Alaska Native          0.0          0.0   0.0    0.0   0.0   \n",
              "White                                  2.0          0.0   0.0    0.0   0.0   \n",
              "Black                                  7.0          0.0   0.0    0.0   0.0   \n",
              "Asian/Pacific Islander                 0.0          0.0   0.0    0.0   0.0   \n",
              "\n",
              "                               Month  Incident  Crime Type  Crime Solved  \\\n",
              "Perpetrator Race                                                           \n",
              "Native American/Alaska Native    0.0       0.0         0.0           0.0   \n",
              "White                            0.0       0.0         0.0           0.0   \n",
              "Black                            0.0       0.0         0.0           0.0   \n",
              "Asian/Pacific Islander           0.0       0.0         0.0           0.0   \n",
              "\n",
              "                               Victim Sex  Victim Age  Victim Race  \\\n",
              "Perpetrator Race                                                     \n",
              "Native American/Alaska Native         0.0         0.0          1.0   \n",
              "White                                14.0         0.0        101.0   \n",
              "Black                                12.0         0.0         59.0   \n",
              "Asian/Pacific Islander                0.0         0.0          5.0   \n",
              "\n",
              "                               Victim Ethnicity  Perpetrator Sex  \\\n",
              "Perpetrator Race                                                   \n",
              "Native American/Alaska Native             104.0              2.0   \n",
              "White                                    5265.0              6.0   \n",
              "Black                                    6064.0             12.0   \n",
              "Asian/Pacific Islander                    155.0              1.0   \n",
              "\n",
              "                               Perpetrator Age  Perpetrator Ethnicity  \\\n",
              "Perpetrator Race                                                        \n",
              "Native American/Alaska Native              0.0                  104.0   \n",
              "White                                      0.0                 5208.0   \n",
              "Black                                      0.0                 6136.0   \n",
              "Asian/Pacific Islander                     0.0                  156.0   \n",
              "\n",
              "                               Relationship  Weapon  Record Source  Victim  \n",
              "Perpetrator Race                                                            \n",
              "Native American/Alaska Native          39.0    12.0            0.0     0.0  \n",
              "White                                2261.0   827.0            0.0     0.0  \n",
              "Black                                3831.0   552.0            0.0     0.0  \n",
              "Asian/Pacific Islander                 58.0    23.0            0.0     0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNViB1x8W96j",
        "colab_type": "text"
      },
      "source": [
        "**Dataframe Size Reduction Efforts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2s6DoTaZLOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi0QBN_GHPEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_homicide_copy = df_homicide.select_dtypes(include=['object']).copy()\n",
        "df_homicide_copy.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ClezErWV94Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24f4a144-4d08-4127-cb48-102b1fe33eb2"
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_homicide, NAlist = reduce_mem_usage(df_homicide)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)\n",
        "\n",
        "# Reduce the size of the string columns\n",
        "df_homicide[\"Agency Name\"]=df_homicide[\"Agency Name\"].astype(\"category\")\n",
        "df_homicide[\"Agency Type\"]=df_homicide[\"Agency Type\"].astype(\"category\")\n",
        "df_homicide[\"City\"]=df_homicide[\"City\"].astype(\"category\")\n",
        "df_homicide[\"State\"]=df_homicide[\"State\"].astype(\"category\")\n",
        "df_homicide[\"Month\"]=df_homicide[\"Month\"].astype(\"category\")\n",
        "df_homicide[\"Crime Type\"]=df_homicide[\"Crime Type\"].astype(\"category\")\n",
        "df_homicide[\"Crime Solved\"]=df_homicide[\"Crime Solved\"].astype(\"category\")\n",
        "#df_homicide[\"Victim Sex\"]=df_homicide[\"Victim Sex\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Age\"]=df_homicide[\"Perpetrator Age\"].astype(\"category\")\n",
        "df_homicide[\"Relationship\"]=df_homicide[\"Relationship\"].astype(\"category\")\n",
        "df_homicide[\"Weapon\"]=df_homicide[\"Weapon\"].astype(\"category\")\n",
        "df_homicide[\"Record Source\"]=df_homicide[\"Record Source\"].astype(\"category\")\n",
        "\n",
        "## Columns that a part of further preprocessing methods\n",
        "#df_homicide[\"Victim Race\"]=df_homicide[\"Victim Race\"].astype(\"category\")\n",
        "#df_homicide[\"Victim Ethnicity\"]=df_homicide[\"Victim Ethnicity\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Race\"]=df_homicide[\"Perpetrator Race\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Sex\"]=df_homicide[\"Perpetrator Sex\"].astype(\"category\")\n",
        "df_homicide[\"Perpetrator Ethnicity\"]=df_homicide[\"Perpetrator Ethnicity\"].astype(\"category\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of properties dataframe is : 3.9517822265625  MB\n",
            "******************************\n",
            "Column:  Year\n",
            "dtype before:  int64\n",
            "dtype after:  uint16\n",
            "******************************\n",
            "******************************\n",
            "Column:  Incident\n",
            "dtype before:  int64\n",
            "dtype after:  uint16\n",
            "******************************\n",
            "******************************\n",
            "Column:  Victim\n",
            "dtype before:  int64\n",
            "dtype after:  uint8\n",
            "******************************\n",
            "___MEMORY USAGE AFTER COMPLETION:___\n",
            "Memory usage is:  3.4303665161132812  MB\n",
            "This is  86.80555555555556 % of the initial size\n",
            "_________________\n",
            "\n",
            "Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \n",
            "_________________\n",
            "\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:522: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:520: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_72jile7c6Dq",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDhjRVjoozpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "44d2667b-212f-44f4-d7a8-af85c82e5dd7"
      },
      "source": [
        "# Analyze class balance for dataset with only unprivileged and privileged dataset\n",
        "is_unpriv = df_homicide[\"Perpetrator Race\"].isin([\"Black\"])\n",
        "is_priv = df_homicide[\"Perpetrator Race\"].isin([\"White\"])\n",
        "df_unpriv_homicide = df_homicide[is_unpriv]\n",
        "df_priv_homicide = df_homicide[is_priv]\n",
        "\n",
        "df_unpriv_priv_complete_homicide = pd.concat([df_unpriv_homicide, df_priv_homicide])\n",
        "\n",
        "print(df_unpriv_priv_complete_homicide[\"Victim\"].value_counts(dropna=False))\n",
        "print(df_unpriv_priv_complete_homicide[\"Victim\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    17202\n",
            "0     1993\n",
            "Name: Victim, dtype: int64\n",
            "1    0.896171\n",
            "0    0.103829\n",
            "Name: Victim, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQDodWCXA8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "8b71209a-4338-488d-9765-61be73b0c0bf"
      },
      "source": [
        "df_homicide.set_index('Perpetrator Race').isna().sum(level=0)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Agency Name</th>\n",
              "      <th>Agency Type</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Incident</th>\n",
              "      <th>Crime Type</th>\n",
              "      <th>Crime Solved</th>\n",
              "      <th>Victim Sex</th>\n",
              "      <th>Victim Age</th>\n",
              "      <th>Victim Race</th>\n",
              "      <th>Victim Ethnicity</th>\n",
              "      <th>Perpetrator Sex</th>\n",
              "      <th>Perpetrator Age</th>\n",
              "      <th>Perpetrator Ethnicity</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Weapon</th>\n",
              "      <th>Record Source</th>\n",
              "      <th>Victim</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perpetrator Race</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Native American/Alaska Native</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>5265.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5208.0</td>\n",
              "      <td>2261.0</td>\n",
              "      <td>827.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Black</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>6064.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6136.0</td>\n",
              "      <td>3831.0</td>\n",
              "      <td>552.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian/Pacific Islander</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Agency Name  Agency Type  City  State  Year  \\\n",
              "Perpetrator Race                                                             \n",
              "Native American/Alaska Native          0.0          0.0   0.0    0.0   0.0   \n",
              "White                                  2.0          0.0   0.0    0.0   0.0   \n",
              "Black                                  7.0          0.0   0.0    0.0   0.0   \n",
              "Asian/Pacific Islander                 0.0          0.0   0.0    0.0   0.0   \n",
              "\n",
              "                               Month  Incident  Crime Type  Crime Solved  \\\n",
              "Perpetrator Race                                                           \n",
              "Native American/Alaska Native    0.0       0.0         0.0           0.0   \n",
              "White                            0.0       0.0         0.0           0.0   \n",
              "Black                            0.0       0.0         0.0           0.0   \n",
              "Asian/Pacific Islander           0.0       0.0         0.0           0.0   \n",
              "\n",
              "                               Victim Sex  Victim Age  Victim Race  \\\n",
              "Perpetrator Race                                                     \n",
              "Native American/Alaska Native         0.0         0.0          1.0   \n",
              "White                                14.0         0.0        101.0   \n",
              "Black                                12.0         0.0         59.0   \n",
              "Asian/Pacific Islander                0.0         0.0          5.0   \n",
              "\n",
              "                               Victim Ethnicity  Perpetrator Sex  \\\n",
              "Perpetrator Race                                                   \n",
              "Native American/Alaska Native             104.0              2.0   \n",
              "White                                    5265.0              6.0   \n",
              "Black                                    6064.0             12.0   \n",
              "Asian/Pacific Islander                    155.0              1.0   \n",
              "\n",
              "                               Perpetrator Age  Perpetrator Ethnicity  \\\n",
              "Perpetrator Race                                                        \n",
              "Native American/Alaska Native              0.0                  104.0   \n",
              "White                                      0.0                 5208.0   \n",
              "Black                                      0.0                 6136.0   \n",
              "Asian/Pacific Islander                     0.0                  156.0   \n",
              "\n",
              "                               Relationship  Weapon  Record Source  Victim  \n",
              "Perpetrator Race                                                            \n",
              "Native American/Alaska Native          39.0    12.0            0.0     0.0  \n",
              "White                                2261.0   827.0            0.0     0.0  \n",
              "Black                                3831.0   552.0            0.0     0.0  \n",
              "Asian/Pacific Islander                 58.0    23.0            0.0     0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYfLDcqBU81J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpbuBrUd3aOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.groupby([\"Perpetrator\"]).agg({\"Perpetrator\": 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzCYLwOCEzZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_homicide.info)\n",
        "print(df_homicide.describe)\n",
        "print(df_homicide.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M963RlbU9eWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.head(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSHfWmP4k1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f123191-2a7f-4dc6-fa7b-f933d07e1ce9"
      },
      "source": [
        "eda_descr_stats(data = df_homicide, disc_feature=\"Perpetrator Race\", disc_min_value=\"Black\", label = \"Perpetrator\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Sensitive Attribute: One or more of the following features are sensitive ones: Index(['Agency Code', 'Agency Name', 'Agency Type', 'City', 'State', 'Year',\n",
            "       'Month', 'Incident', 'Crime Type', 'Crime Solved', 'Victim Sex',\n",
            "       'Victim Age', 'Victim Race', 'Victim Ethnicity', 'Perpetrator Sex',\n",
            "       'Perpetrator Age', 'Perpetrator Race', 'Perpetrator Ethnicity',\n",
            "       'Relationship', 'Weapon', 'Record Source', 'Perpetrator'],\n",
            "      dtype='object').\n",
            "1. Sensitive Attribute: These are the individual values for the sensitive attribute: ['Native American/Alaska Native' 'White' nan 'Black'\n",
            " 'Asian/Pacific Islander'].\n",
            "2. Binary Target Variable: The Binary Target Feature has the following values and counts:\n",
            "             Perpetrator\n",
            "Perpetrator             \n",
            "0                   4332\n",
            "1                  24444\n",
            "3. The Total Number of Predictor Features is: 22.\n",
            "4. The Total Number of Training Examples is: 28776.\n",
            "5. The Total Number of Training Examples in the Minority Group is: 9329.\n",
            "6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  White                            9866\n",
            "Black                            9329\n",
            "NaN                              9088\n",
            "Asian/Pacific Islander            317\n",
            "Native American/Alaska Native     176\n",
            "Name: Perpetrator Race, dtype: int64.\n",
            "6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: White                            0.342855\n",
            "Black                            0.324194\n",
            "NaN                              0.315819\n",
            "Asian/Pacific Islander           0.011016\n",
            "Native American/Alaska Native    0.006116\n",
            "Name: Perpetrator Race, dtype: float64.\n",
            "7. Class Balance: The Class Balance looks as follows:\n",
            "1    24444\n",
            "0     4332\n",
            "Name: Perpetrator, dtype: int64\n",
            "1    0.849458\n",
            "0    0.150542\n",
            "Name: Perpetrator, dtype: float64\n",
            "8. Coarseness of Features: Details on missing values of features in the dataset:\n",
            "Agency Code                  0\n",
            "Agency Name                 12\n",
            "Agency Type                  0\n",
            "City                         0\n",
            "State                        0\n",
            "Year                         0\n",
            "Month                        0\n",
            "Incident                     0\n",
            "Crime Type                   0\n",
            "Crime Solved                 0\n",
            "Victim Sex                  61\n",
            "Victim Age                   0\n",
            "Victim Race                336\n",
            "Victim Ethnicity         16376\n",
            "Perpetrator Sex           8808\n",
            "Perpetrator Age              0\n",
            "Perpetrator Race          9088\n",
            "Perpetrator Ethnicity    20658\n",
            "Relationship             14931\n",
            "Weapon                    2007\n",
            "Record Source                0\n",
            "Perpetrator                  0\n",
            "dtype: int64\n",
            "                               Agency Code  Agency Name  Agency Type  City  \\\n",
            "Perpetrator Race                                                             \n",
            "Asian/Pacific Islander                   0            0            0     0   \n",
            "Black                                    0            7            0     0   \n",
            "Native American/Alaska Native            0            0            0     0   \n",
            "White                                    0            2            0     0   \n",
            "\n",
            "                               State  Year  Month  Incident  Crime Type  \\\n",
            "Perpetrator Race                                                          \n",
            "Asian/Pacific Islander             0     0      0         0           0   \n",
            "Black                              0     0      0         0           0   \n",
            "Native American/Alaska Native      0     0      0         0           0   \n",
            "White                              0     0      0         0           0   \n",
            "\n",
            "                               Crime Solved  Victim Sex  Victim Age  \\\n",
            "Perpetrator Race                                                      \n",
            "Asian/Pacific Islander                    0           0           0   \n",
            "Black                                     0          12           0   \n",
            "Native American/Alaska Native             0           0           0   \n",
            "White                                     0          14           0   \n",
            "\n",
            "                               Victim Race  Victim Ethnicity  Perpetrator Sex  \\\n",
            "Perpetrator Race                                                                \n",
            "Asian/Pacific Islander                   5               155                1   \n",
            "Black                                   59              6064               12   \n",
            "Native American/Alaska Native            1               104                2   \n",
            "White                                  101              5265                6   \n",
            "\n",
            "                               Perpetrator Age  Perpetrator Race  \\\n",
            "Perpetrator Race                                                   \n",
            "Asian/Pacific Islander                       0                 0   \n",
            "Black                                        0                 0   \n",
            "Native American/Alaska Native                0                 0   \n",
            "White                                        0                 0   \n",
            "\n",
            "                               Perpetrator Ethnicity  Relationship  Weapon  \\\n",
            "Perpetrator Race                                                             \n",
            "Asian/Pacific Islander                           156            58      23   \n",
            "Black                                           6136          3831     552   \n",
            "Native American/Alaska Native                    104            39      12   \n",
            "White                                           5208          2261     827   \n",
            "\n",
            "                               Record Source  Perpetrator  \n",
            "Perpetrator Race                                           \n",
            "Asian/Pacific Islander                     0            0  \n",
            "Black                                      0            0  \n",
            "Native American/Alaska Native              0            0  \n",
            "White                                      0            0  \n",
            "                               Agency Code  Agency Name  Agency Type  City  \\\n",
            "Perpetrator Race                                                             \n",
            "Asian/Pacific Islander                 0.0     0.000000          0.0   0.0   \n",
            "Black                                  0.0     0.000750          0.0   0.0   \n",
            "Native American/Alaska Native          0.0     0.000000          0.0   0.0   \n",
            "White                                  0.0     0.000203          0.0   0.0   \n",
            "\n",
            "                               State  Year  Month  Incident  Crime Type  \\\n",
            "Perpetrator Race                                                          \n",
            "Asian/Pacific Islander           0.0   0.0    0.0       0.0         0.0   \n",
            "Black                            0.0   0.0    0.0       0.0         0.0   \n",
            "Native American/Alaska Native    0.0   0.0    0.0       0.0         0.0   \n",
            "White                            0.0   0.0    0.0       0.0         0.0   \n",
            "\n",
            "                               Crime Solved  Victim Sex  Victim Age  \\\n",
            "Perpetrator Race                                                      \n",
            "Asian/Pacific Islander                  0.0    0.000000         0.0   \n",
            "Black                                   0.0    0.001286         0.0   \n",
            "Native American/Alaska Native           0.0    0.000000         0.0   \n",
            "White                                   0.0    0.001419         0.0   \n",
            "\n",
            "                               Victim Race  Victim Ethnicity  Perpetrator Sex  \\\n",
            "Perpetrator Race                                                                \n",
            "Asian/Pacific Islander            0.015773          0.488959         0.003155   \n",
            "Black                             0.006324          0.650016         0.001286   \n",
            "Native American/Alaska Native     0.005682          0.590909         0.011364   \n",
            "White                             0.010237          0.533651         0.000608   \n",
            "\n",
            "                               Perpetrator Age  Perpetrator Race  \\\n",
            "Perpetrator Race                                                   \n",
            "Asian/Pacific Islander                     0.0               0.0   \n",
            "Black                                      0.0               0.0   \n",
            "Native American/Alaska Native              0.0               0.0   \n",
            "White                                      0.0               0.0   \n",
            "\n",
            "                               Perpetrator Ethnicity  Relationship    Weapon  \\\n",
            "Perpetrator Race                                                               \n",
            "Asian/Pacific Islander                      0.492114      0.182965  0.072555   \n",
            "Black                                       0.657734      0.410655  0.059170   \n",
            "Native American/Alaska Native               0.590909      0.221591  0.068182   \n",
            "White                                       0.527874      0.229171  0.083823   \n",
            "\n",
            "                               Record Source  Perpetrator  \n",
            "Perpetrator Race                                           \n",
            "Asian/Pacific Islander                   0.0          0.0  \n",
            "Black                                    0.0          0.0  \n",
            "Native American/Alaska Native            0.0          0.0  \n",
            "White                                    0.0          0.0  \n",
            "9. Severity of Outliers for Numeric Features\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAD4CAYAAADYU1DBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU+klEQVR4nO3de5SkdX3n8feHGS7DiFyEZdCEHjAaYUm4pBE2KgurQWENuEoURh1JckLUROTkaAJLcM1hs8Z1s7vHVUSMLnZWLotZN2jOBomCo1EuPTjcHAmgTBJlYPDCcMeZ+e4f9cxYtN3N9ND1q27m/TqnTj/1e27feurp+vTvV09Vp6qQJGnQdhh2AZKk7YOBI0lqwsCRJDVh4EiSmjBwJElNLBx2AXPZ3nvvXUuXLh12GZI0b6xcufKBqtpnsnkGzjSWLl3K+Pj4sMuQpHkjyZqp5jmkJklqwsCRJDVh4EiSmjBwJElbjI2NMTY2NpBtGziSpC1WrFjBihUrBrJtA0eS1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCYMHElSEwaOJKkJA0eS1IT/nkCStMVjjz02sG0bOJKkLapqYNt2SE2S1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCbmTeCk52tJTuhr+40kfzvMuiRJW2fefA6nqirJ24ErklxDr/b/BLxmW7aXZGFVbZjNGiVJU5s3PRyAqroN+DzwR8D7gP8FnJvkhiTfTHIyQJKlSb6a5Kbu9qtd+7Fd+5XAt4b1OCRpezRvejh9/gS4CXgS+ALw5ar6rSR7ADck+TvgfuDXqurxJC8CLgVGu/WPAA6pqu8OoXZJ2m7Nu8CpqkeSXA48DLwR+PUk7+lm7wLsD3wf+EiSw4CNwIv7NnHDdGGT5AzgDID9999/AI9AkrZP8y5wOpu6W4A3VNUd/TOTvB+4DziU3rDh432zH5luw1V1EXARwOjo6OC+VEiStjPz6j2cSVwFvCtJAJIc3rXvDtxbVZuAtwILhlSfJKkz3wPnfGBH4JYkt3f3AS4A3pbkZuAlPE2vRpI0ePNySK2q3t9393cnmX8n8Mt9TX/UtV8LXDvA0iRJU5jvPRxJ0jxh4EiSmjBwJElNGDiSpCYMHElSE/PyKjVJ0mB0H2scCANHkrTFokWLBrZth9QkSU0YOJKkJgwcSVITBo4kqQkDR5LUhIEjSWrCwJEkNWHgSJKaMHAkSU34TQOSpC2OOeaYgW3bwJEkbbF8+fKBbdshNUlSEwaOJKkJA0eS1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCb84OcAnHPOOaxfv56jjjpqoB+ikqT5xMAZgHXr1vHoo4+yZs2aYZciSXOGQ2qSpCYMHElSEwaOJKkJA0eS1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCYMnAF48sknAVi7di1jY2NDrkaS5gYDZwA2bdoEwBNPPOHX20hSx8CRJDVh4EiSmjBwJElNGDiSpCYMHElSEwaOJKkJ/+PnAD366KOsXr2aZcuWDbsUPcuceeaZHH300cMuQ5qRgfdwkjy8jeuNJvnwFPPuSbL3Nm73dUkO3pZ1pbniggsuGHYJ0ozN2SG1qhqvqjMHsOnXAQaO5rUNGzZw3XXXDbsMaUaaBU6SY5Ncm+SzSb6d5DNJ0s07MsnXk9yc5IYku3XLf6Gb/7wkX0xye5K/ANK33bd066xK8vEkC7r2h5P8abfN65Lsm+RXgZOAD3XLv7DV45dmm70czTeteziHA2fR62EcCLwsyU7A5cC7q+pQ4FXAYxPW+w/A16rqXwKfA/YHSHIQ8CbgZVV1GLAReHO3zmLgum6bK4DfqaqvA1cC762qw6rq7okFJjkjyXiS8XXr1s3mY5dm1YYNG4ZdgjQjrS8auKGq/hkgySpgKfAgcG9V3QhQVeu7+f3rHQO8vpv/N0l+1LW/EvgV4MZu+UXA/d28J4EvdNMrgV/bmgKr6iLgIoDR0dGa6QOUWlm40Gt+NL+0PmOf6JveOAv7D/Dpqjpnknk/qarNgTEb+5LmlHe+853DLkGakblw0cAdwH5JjgTo3r+ZGA4rgGXd/BOAPbv2LwGnJPkX3by9kow8zf4eAnabreKlYVi4cKGXRWveGXrgVNWT9N6H+R9JbgauBnaZsNifAMckuZ3e0No/dut+C/hj4ItJbunW3e9pdnkZ8N4k3/SiAc1X9m40H+Wno06aaHR0tMbHx2e83lve8hY2bdrErrvuysjICOedd94AqpOkuSfJyqoanWze0Hs4kqTtg4EjSWrCwJEkNWHgSJKaMHAkSU0YOAOwww69w7rzzjszMvJ0HwuSpO2DgTMAO+20EwBLlixh+fLlQ65GkuYGA0eS1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCYMHElSEwaOJKkJA0eS1MTEf+WsWbDPPvuwfv16v9ZGkvoYOAPwgQ98YNglSNKc45CaJKkJA0eS1ISBI0lqwsCRJDVh4EiSmjBwJElNGDiSpCYMHElSE37wc8DGxsa4/vrrAViyZAkjIyMsX758yFVJUnsGzoCtWbOGH/3oxyxYuCPrH7p72OVI0tAYOA0sWLgji5+797DLkKSh8j0cSVITBo4kqQkDR5LUhIEjSWrCwJEkNWHgSJKaMHAkSU0YOAMwNjbG2NjYrC8rSfOZH/wcgDVr1gxkWUmaz+zhSJKaMHAkSU0YOJKkJgwcSVITBo4kqQkDR5LUhJdFzwGrV69m2bJlP9N+ySWXDKEazWX954nnh+abre7hJLkmyasntJ2V5GNJTkpy9jTr7pHknX33n5/ks9tW8lO2uyrJZc90O5KkwZvJkNqlwKkT2k4FLq2qK6vqz6ZZdw9gS+BU1fer6pQZ7PtnJDkIWAC8IsniZ7KtYVq9evWU8ybr9Wj7NfF88PzQfDOTIbXPAv8xyU5V9WSSpcDzga8mOR0YrarfT7IvcCFwYLfeO4AzgRcmWQVcDXwU+EJVHdKt+zpgMfAi4L8AOwFvBZ4ATqyqH05Sz2nAXwIHAScDlwAkORL4JLCp29cJ3X4WAH8GHAvsDHy0qj4+g8e/1dauXcsTTzzB+eefz5o1a9i0cQMAjz/yIGvW/Jjzzz9/ELuVpDltq3s43Yv+DcAJXdOpwP+uqpqw6IeBr1TVocARwO3A2cDdVXVYVb13ks0fArweOBL4U+DRqjoc+AawfIqS3gRcRq/ndVpf+/8EfreqDgM29rX/NvBgVR3Z7ed3khwwcaNJzkgynmR83bp1U+xakjRTM71KrX9Y7dTu/kT/BvgYQFVtrKoHt2K711TVQ1W1DngQ+HzXfiuwdOLCSUaBB6rqH4EvAYcn2SvJHsBuVfWNbtH+d1WPB5Z3vazrgefR61E9RVVdVFWjVTW6zz77bEXpP2vJkiWMjIxw3nnnMTIywg4Leh3JXRbvvqV9802SthczvUrtr4H/luQIYNeqWjlLdTzRN72p7/4mJq/xNOAlSe7p7j8XeANwxTT7CPCuqrrqmZUqSdoWM+rhVNXDwDXAp5i8dwO9Hsc7AJIsSLI78BCw2zOoc4skOwBvBH6pqpZW1VJ67+GcVlU/Bh5KclS3eP9FDlcB70iyY7edF8+Fiw0OOuigKed52av6TTwfPD8032zLBz8vBQ5l6sB5N3BckluBlcDBVfUD4O+T3JbkQ9tW6havAL5XVd/va1sBHJxkP3rv1XyiGzpbTG+IDuAvgG8BNyW5Dfg4fg5JkpqZ8QtuVf1fesNT/W0XAxd30/fR63FMXG/iNZyHTFy3u790su32tX0FOHpC20ZgCUCSh6rql7vps4HxbplNwL/vbnPKQQcd5Ps52ir2ajSfPRv/wv+3Sc6h99jWAKcPtxxJEjwLA6eqLgcuH3YdkqSn8ss7JUlNGDiSpCYMHElSE8+693DmgpGRkYEsK0nzmYEzAMuXT/X1b89sWUmazxxSkyQ1YeBIkpowcCRJTRg4kqQmDBxJUhMGjiSpCQNHktSEgSNJasIPfjawccNPeGT9A707++4+3GIkaUgMnAEbGRlh7dq1ACxZssSvspG03UpVDbuGOWt0dLTGx8eHXYYkzRtJVlbV6GTzfA9HktSEgSNJasLAkSQ1YeBIkpowcCRJTRg4kqQmDBxJUhMGjiSpCQNnAMbGxhgbGxt2GZI0pxg4A7BixQpWrFgx7DIkaU4xcCRJTRg4kqQmDBxJUhMGjiSpCQNHktSEgSNJasLAkSQ1YeBIkpowcCRJTSwcdgHPRo899tiwS5CkOcfAGYCqGnYJkjTnOKQmSWrCwJEkNWHgSJKaMHAkSU0YOJKkJgwcSVITTxs4STYmWZXktiRXJNl1kAUlOWtb9pHk9CTPH0RN22rZsmXDLkGS5oyt6eE8VlWHVdUhwJPA27dmw0m29TM+ZwGTBk6SBdOsdzowo8B5mu1JkmbRTIfUvgr8QpLFST6V5IYk30xyMmzpZVyZ5MvAl5Icm2RFkr9JckeSC5Ps0C17fJJvJLmp6zk9J8mZ9ELjmiTXdMs9nOTPk9wM/Ksk70tyY9fjuig9pwCjwGe63tiiJK/saru1q3Xnbnv3JPlgkpuA35iVo9hnYq/GXo4k9Wx14HQ9lhOAW4FzgS9X1UuB44APJVncLXoEcEpV/evu/kuBdwEHAy8EXp9kb+CPgVdV1RHAOPAHVfVh4PvAcVV1XLf+YuD6qjq0qr4GfKSqjux6XIuA11bVZ7ttvLmqDgMKuBh4U1X9Er1vVHhH38P5QVUdUVWXbe3jlyQ9M1sz7LUoyapu+qvAJ4GvAycleU/Xvguwfzd9dVX9sG/9G6rqOwBJLgVeDjxOL4D+PgnATsA3ptj/RuCv+u4fl+QP6Q277QXcDnx+wjq/CHy3qv6hu/9p4PeA/97dv3yqB5vkDOAMgP3333+qxSRJM7Q1gfNY12vYIr2UeENV3TGh/SjgkQnrT/xisQJCL5hO24r9P15VG7vt7wJcAIxW1T8leT+9sJupiTX+tLiqi4CLAEZHR/1SNEmaJdt6WfRVwLu64CHJ4dMs+9IkB3Tv3bwJ+BpwHfCyJL/Qrb84yYu75R8CdptiW5vD5YEkzwFO6ZvXv94dwNLN2wfeCnxlqx+dJGnWbWvgnA/sCNyS5Pbu/lRuBD4CrAa+C3yuqtbRu6rs0iS30BtOe0m3/EXA326+aKBfVf0Y+ARwG73Qu7Fv9sXAhd3wX4DfBK5IciuwCbhwmx7pDF1yySXT3pek7VUG+VX6SY4F3lNVrx3YTgZodHS0xsfHZ7xe/5VpBo6k7UmSlVU1Otk8v2lggAwbSfqpgf4Dtqq6Frh2kPuQJM0P9nAkSU0YOJKkJgwcSVITA30PZ3vVfTxJktTHwBmARYsWDbsESZpzHFKTJDVh4EiSmjBwJElNGDiSpCYMHElSEwaOJKkJA0eS1ISBI0lqwsCRJDXhNw0MwDHHHDPsEiRpzjFwBmD58uXDLkGS5hyH1CRJTRg4kqQmDBxJUhMGjiSpiVTVsGuYs5KsA9Zs4+p7Aw/MYjmzZS7WNRdrAuuaqblY11ysCZ7ddY1U1T6TzTBwBiTJeFWNDruOieZiXXOxJrCumZqLdc3FmmD7rcshNUlSEwaOJKkJA2dwLhp2AVOYi3XNxZrAumZqLtY1F2uC7bQu38ORJDVhD0eS1ISBI0lqwsCZZUlek+SOJHclObvxvn8+yTVJvpXk9iTv7trfn+R7SVZ1txP71jmnq/WOJK8eYG33JLm12/9417ZXkquT3Nn93LNrT5IPd3XdkuSIAdTzi33HY1WS9UnOGsaxSvKpJPcnua2vbcbHJsnbuuXvTPK2AdX1oSTf7vb9uSR7dO1LkzzWd9wu7FvnV7rn/q6u9gygrhk/b7P9uzpFXZf31XRPklVde5PjNc1rwnDOr6ryNks3YAFwN3AgsBNwM3Bww/3vBxzRTe8G/ANwMPB+4D2TLH9wV+POwAFd7QsGVNs9wN4T2v4zcHY3fTbwwW76ROD/AQGOBq5v8LytBUaGcayAY4AjgNu29dgAewHf6X7u2U3vOYC6jgcWdtMf7Ktraf9yE7ZzQ1drutpPGEBdM3reBvG7OlldE+b/OfC+lsdrmteEoZxf9nBm10uBu6rqO1X1JHAZcHKrnVfVvVV1Uzf9ELAaeME0q5wMXFZVT1TVd4G76D2GVk4GPt1Nfxp4XV/7WPVcB+yRZL8B1vFK4O6qmu5bJQZ2rKpqBfDDSfY3k2PzauDqqvphVf0IuBp4zWzXVVVfrKoN3d3rgJ+bbhtdbc+tquuq98o11vdYZq2uaUz1vM367+p0dXW9lDcCl063jdk+XtO8Jgzl/DJwZtcLgH/qu//PTP+CPzBJlgKHA9d3Tb/fdZE/tbn7TNt6C/hikpVJzuja9q2qe7vptcC+Q6gL4FSe+kIw7GMFMz82wzj3foveX8ObHZDkm0m+kuQVXdsLulpa1DWT56318XoFcF9V3dnX1vR4TXhNGMr5ZeA8CyV5DvBXwFlVtR74GPBC4DDgXnpd+9ZeXlVHACcAv5fkKf8Wtftrrvk1+kl2Ak4Cruia5sKxeophHZvpJDkX2AB8pmu6F9i/qg4H/gC4JMlzG5Y05563CU7jqX/UND1ek7wmbNHy/DJwZtf3gJ/vu/9zXVszSXakd2J9pqr+D0BV3VdVG6tqE/AJfjoU1Kzeqvpe9/N+4HNdDfdtHirrft7fui56AXhTVd3X1Tf0Y9WZ6bFpVl+S04HXAm/uXqzohqx+0E2vpPf+yIu7GvqH3QZS1zY8by2P10Lg9cDlffU2O16TvSYwpPPLwJldNwIvSnJA95fzqcCVrXbejRN/ElhdVf+1r73//Y9/B2y+iuZK4NQkOyc5AHgRvTcsZ7uuxUl22zxN743n27r9b77a5W3AX/fVtby7YuZo4MG+7v9se8pfnsM+Vn1memyuAo5Psmc3nHR81zarkrwG+EPgpKp6tK99nyQLuukD6R2f73S1rU9ydHd+Lu97LLNZ10yft5a/q68Cvl1VW4bKWh2vqV4TGNb5ta1XP3ib8qqQE+ldCXI3cG7jfb+cXtf4FmBVdzsR+Evg1q79SmC/vnXO7Wq9g2d49dA0dR1I7yqgm4HbNx8X4HnAl4A7gb8D9uraA3y0q+tWYHRAdS0GfgDs3tfW/FjRC7x7gZ/QGxv/7W05NvTeU7mru/3mgOq6i95Y/ubz68Ju2Td0z+0q4Cbg1/u2M0ovAO4GPkL3DSezXNeMn7fZ/l2drK6u/WLg7ROWbXK8mPo1YSjnl19tI0lqwiE1SVITBo4kqQkDR5LUhIEjSWrCwJEkNWHgSJKaMHAkSU38f4bGXSaiQd/KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C01BFbz3daQn",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcHFLoqdh5L",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSOE-LXCFGOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_homicide_train_input, df_homicide_train_label = fair_preprocess(data = df_homicide,\n",
        "                                                                   label = \"Victim\",\n",
        "                                                                   neg_class = 0,\n",
        "                                                                   pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_homicide.groupby([\"Victim\"]).agg({\"Victim\": 'count'}))\n",
        "print(df_homicide_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpcq7b0zJU7r",
        "colab_type": "text"
      },
      "source": [
        "###### Attempt to resolve data leakage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF2r2j_9F8dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide_leakage = df_homicide.drop([\"Victim Sex\", \"Victim Age\", \"Victim Race\", \"Victim Ethnicity\"], axis=1)\n",
        "\n",
        "df_homicide_leakage = df_homicide_leakage[df_homicide_leakage[\"Year\"] > 2013]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIphd56mOP8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_homicide_train_input_test, df_homicide_train_label_test = fair_preprocess(data = df_homicide_leakage,\n",
        "                                                                   label = \"Victim\",\n",
        "                                                                   neg_class = 0,\n",
        "                                                                   pos_class = 1)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2etKAzUBf_lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross_val_predict\n",
        "model_homicide = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "df_homicide_leakage = df_homicide.drop([\"Victim Sex\", \"Victim Age\", \"Victim Race\"], axis=1)\n",
        "\n",
        "# df_homicide_leakage = df_homicide.drop([\"Victim Sex\", \"Victim Age\", \"Victim Race\", \"Victim Ethnicity\", \"Incident\", \"Crime Solved\", \"Weapon\", \"Perpetrator Sex\",\t\"Perpetrator Age\",\t\"Perpetrator Race\",\t\"Perpetrator Ethnicity\",\t\"Relationship\",\t\"Record Source\", \"Crime Type\", \"City\", \"Agency Type\", \"Agency Name\"], axis=1)\n",
        "\n",
        "\n",
        "df_homicide_leakage = df_homicide_leakage[df_homicide_leakage[\"Year\"] > 2013]\n",
        "\n",
        "# Define Input and target columns\n",
        "df_homicide_leakage_input = df_homicide_leakage.drop(columns=[\"Victim\"])  # Input\n",
        "df_homicide_leakage_label = df_homicide_leakage[\"Victim\"]                 # Target\n",
        "\n",
        "# Apply dummy coding\n",
        "df_homicide_leakage_input = pd.get_dummies(df_homicide_leakage_input)\n",
        "\n",
        "y_train_pred_homicide = cross_val_predict(model_homicide,\n",
        "                                 df_homicide_leakage_input,\n",
        "                                 df_homicide_leakage_label,\n",
        "                                 cv = 5)\n",
        "\n",
        "# Append prediction labels to original dataset\n",
        "df_homicide_leakage['y_pred'] = y_train_pred_homicide\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E70sUuTiEHQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b7a906d2-0539-47ed-e026-bf88111928ca"
      },
      "source": [
        "# Attempt to resolve data leakage\n",
        "# Evaluate using Cross Validation\n",
        "\n",
        "model_homicide = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "df_homicide_train_input_dummy_test = pd.get_dummies(df_homicide_train_input_test)\n",
        "X = df_homicide_train_input_dummy_test\n",
        "Y = df_homicide_train_label_test\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "f1 = make_scorer(f1_score, average=\"weighted\")\n",
        "\n",
        "# Results\n",
        "#results = model_selection.cross_val_score(model_homicide, X, Y, cv=kfold, scoring = f1)\n",
        "#print(results)\n",
        "#print(results.std())\n",
        "#print(results.mean())\n",
        "\n",
        "scores = cross_validate(model_homicide, X, Y, cv=5,\n",
        "                        scoring=make_scorer(f1_score, average='weighted'),\n",
        "                        return_train_score=True)\n",
        "\n",
        "print(scores[\"test_score\"])\n",
        "print(scores[\"test_score\"].mean())"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.86308685 0.6716179  0.7862174  0.8006427  0.79878932]\n",
            "0.7840708351061081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMFTfD0bJY3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "32790261-9ab5-4ad7-893d-e5c513fae31a"
      },
      "source": [
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_homicide, \n",
        "                    title = \"Homicide Dataset - Performance Learning Curve\", \n",
        "                    X = df_homicide_train_input_dummy_test, y = df_homicide_train_label_test, \n",
        "                    cv = 5, \n",
        "                    scoring = make_scorer(f1_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NRotOolSgeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1e7cc7aa-2e10-42e7-bcbc-eb10a73f8dd5"
      },
      "source": [
        "# Feature Importance\n",
        "\n",
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "homicide_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "homicide_model.fit(df_homicide_train_input_dummy_test, df_homicide_train_label_test)\n",
        "plot_importance(homicide_model)\n",
        "pyplot.show()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBuKJ_7udqW6",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvTp6Q85CMzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide_train_input.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vknxj1bdnisU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "                \"n_estimators\": [10, 50, 100],\n",
        "                \"max_depth\": [3, 7], \n",
        "                \"min_child_weight\": [1, 4],       \n",
        "                \"reg_lambda\": [1]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= recall,\n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_homicide_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4WE7mKdF1UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN65JoIHdxiv",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2V-5rfYVdjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6c087ed9-1a5f-4c0c-c376-8f07368cde0c"
      },
      "source": [
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_homicide[\"Perpetrator Race\"].isin([\"Black\"])\n",
        "is_white = df_homicide[\"Perpetrator Race\"].isin([\"White\"])\n",
        "df_homicide_black = df_homicide[is_black]  # Minority\n",
        "df_homicide_white = df_homicide[is_white]  # Majority\n",
        "\n",
        "list_dfs_homicide = create_datasets(min_data = df_homicide_black, maj_data = df_homicide_white, training_sizes=training_sizes_homicide)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n",
            "[9871, 9876, 9886, 9896, 9906, 9916, 9941, 9966, 9991, 10016, 10041, 10066, 10116, 10166, 10216, 10266, 10316, 10366, 10466, 10566, 10666, 10766, 10866, 11116, 11366, 11616, 11866, 12116, 12366, 12616, 12866, 13116, 13366, 13616, 13866, 14116, 14366, 14616, 14866, 15866, 16866, 17866, 18866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAw_Yu8-fRQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c6d7aa98-c362-4af7-9c54-cd15c13446ec"
      },
      "source": [
        "print(len(df_homicide_black))\n",
        "print(len(df_homicide_white))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9329\n",
            "9866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uIyMWH8d7ma",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbsp9jPVVeHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fc6e38a-6f7a-4009-dfc0-3e9be83981ba"
      },
      "source": [
        "# Define arguments\n",
        "label = \"Victim\"\n",
        "homicide_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                               learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                               min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "                               nthread=4, objective='binary:logistic', random_state=0,\n",
        "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                               silent=None, subsample=1, verbosity=1)\n",
        "cv = 3 \n",
        "discr_feature = \"Perpetrator Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Run function\n",
        "results_df_homicide = metrics_to_df(list_dfs=list_dfs_homicide, label = label, model = homicide_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "results_df_homicide\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:220: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in long_scalars\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:220: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in long_scalars\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9871</td>\n",
              "      <td>5</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941107</td>\n",
              "      <td>0.943972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941075</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9876</td>\n",
              "      <td>10</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941139</td>\n",
              "      <td>0.944002</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941075</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9886</td>\n",
              "      <td>20</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940861</td>\n",
              "      <td>0.943710</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.940905</td>\n",
              "      <td>0.999659</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999658</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>1.000304</td>\n",
              "      <td>0.000171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9896</td>\n",
              "      <td>30</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940981</td>\n",
              "      <td>0.944170</td>\n",
              "      <td>0.983051</td>\n",
              "      <td>0.940848</td>\n",
              "      <td>0.999545</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>1.000406</td>\n",
              "      <td>0.000228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9906</td>\n",
              "      <td>40</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941044</td>\n",
              "      <td>0.943925</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.941019</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.004054</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>1.000101</td>\n",
              "      <td>0.000057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9916</td>\n",
              "      <td>50</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941070</td>\n",
              "      <td>0.944380</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.940981</td>\n",
              "      <td>0.999206</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999202</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994536</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998682</td>\n",
              "      <td>0.005068</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.000798</td>\n",
              "      <td>1.001319</td>\n",
              "      <td>0.003131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9941</td>\n",
              "      <td>75</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940951</td>\n",
              "      <td>0.944250</td>\n",
              "      <td>0.951049</td>\n",
              "      <td>0.940873</td>\n",
              "      <td>0.999095</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999088</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998682</td>\n",
              "      <td>0.007602</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>1.001319</td>\n",
              "      <td>0.002733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9966</td>\n",
              "      <td>100</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941164</td>\n",
              "      <td>0.944391</td>\n",
              "      <td>0.963731</td>\n",
              "      <td>0.940930</td>\n",
              "      <td>0.999210</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999202</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998784</td>\n",
              "      <td>0.010136</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.000798</td>\n",
              "      <td>1.001218</td>\n",
              "      <td>0.002676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9991</td>\n",
              "      <td>125</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940871</td>\n",
              "      <td>0.944496</td>\n",
              "      <td>0.945148</td>\n",
              "      <td>0.940816</td>\n",
              "      <td>0.998986</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998974</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998581</td>\n",
              "      <td>0.012670</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>1.001421</td>\n",
              "      <td>0.002790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10016</td>\n",
              "      <td>150</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940747</td>\n",
              "      <td>0.944063</td>\n",
              "      <td>0.943662</td>\n",
              "      <td>0.940703</td>\n",
              "      <td>0.998764</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998745</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998378</td>\n",
              "      <td>0.015204</td>\n",
              "      <td>0.001622</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>1.001624</td>\n",
              "      <td>0.002904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10041</td>\n",
              "      <td>175</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940903</td>\n",
              "      <td>0.944242</td>\n",
              "      <td>0.942598</td>\n",
              "      <td>0.940873</td>\n",
              "      <td>0.999104</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999088</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998682</td>\n",
              "      <td>0.017738</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>1.001319</td>\n",
              "      <td>0.002733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10066</td>\n",
              "      <td>200</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941170</td>\n",
              "      <td>0.944081</td>\n",
              "      <td>0.944591</td>\n",
              "      <td>0.941101</td>\n",
              "      <td>0.999553</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999088</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>1.000913</td>\n",
              "      <td>0.002505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10116</td>\n",
              "      <td>250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941004</td>\n",
              "      <td>0.944125</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.940842</td>\n",
              "      <td>0.999333</td>\n",
              "      <td>0.995575</td>\n",
              "      <td>0.999430</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.996000</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.025340</td>\n",
              "      <td>-0.003392</td>\n",
              "      <td>-0.003855</td>\n",
              "      <td>0.996606</td>\n",
              "      <td>0.002383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>10166</td>\n",
              "      <td>300</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941636</td>\n",
              "      <td>0.944743</td>\n",
              "      <td>0.961806</td>\n",
              "      <td>0.941012</td>\n",
              "      <td>0.999779</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999772</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.030407</td>\n",
              "      <td>-0.003029</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.996970</td>\n",
              "      <td>0.021398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>10216</td>\n",
              "      <td>350</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941695</td>\n",
              "      <td>0.944183</td>\n",
              "      <td>0.959762</td>\n",
              "      <td>0.941044</td>\n",
              "      <td>0.999340</td>\n",
              "      <td>0.996904</td>\n",
              "      <td>0.999430</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>0.994286</td>\n",
              "      <td>0.998986</td>\n",
              "      <td>0.035475</td>\n",
              "      <td>-0.004701</td>\n",
              "      <td>-0.002526</td>\n",
              "      <td>0.995295</td>\n",
              "      <td>0.017505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>10266</td>\n",
              "      <td>400</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941886</td>\n",
              "      <td>0.944820</td>\n",
              "      <td>0.959583</td>\n",
              "      <td>0.941158</td>\n",
              "      <td>0.999562</td>\n",
              "      <td>0.997290</td>\n",
              "      <td>0.999658</td>\n",
              "      <td>0.967742</td>\n",
              "      <td>0.995446</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.999189</td>\n",
              "      <td>0.040543</td>\n",
              "      <td>-0.004189</td>\n",
              "      <td>-0.002368</td>\n",
              "      <td>0.995807</td>\n",
              "      <td>0.015036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>10316</td>\n",
              "      <td>450</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941611</td>\n",
              "      <td>0.944173</td>\n",
              "      <td>0.951049</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.045611</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000203</td>\n",
              "      <td>0.000911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>10366</td>\n",
              "      <td>500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941357</td>\n",
              "      <td>0.944120</td>\n",
              "      <td>0.948367</td>\n",
              "      <td>0.941000</td>\n",
              "      <td>0.999349</td>\n",
              "      <td>0.995575</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.994000</td>\n",
              "      <td>0.999290</td>\n",
              "      <td>0.050679</td>\n",
              "      <td>-0.005290</td>\n",
              "      <td>-0.003969</td>\n",
              "      <td>0.994706</td>\n",
              "      <td>0.011035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10466</td>\n",
              "      <td>600</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942170</td>\n",
              "      <td>0.944411</td>\n",
              "      <td>0.957428</td>\n",
              "      <td>0.941227</td>\n",
              "      <td>0.999893</td>\n",
              "      <td>0.998188</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.060815</td>\n",
              "      <td>-0.001363</td>\n",
              "      <td>-0.001812</td>\n",
              "      <td>0.998637</td>\n",
              "      <td>0.002272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10566</td>\n",
              "      <td>700</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941630</td>\n",
              "      <td>0.944151</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.941221</td>\n",
              "      <td>0.999681</td>\n",
              "      <td>0.996835</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.997143</td>\n",
              "      <td>0.999493</td>\n",
              "      <td>0.070951</td>\n",
              "      <td>-0.002350</td>\n",
              "      <td>-0.003051</td>\n",
              "      <td>0.997648</td>\n",
              "      <td>0.003347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>10666</td>\n",
              "      <td>800</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941498</td>\n",
              "      <td>0.944515</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.941019</td>\n",
              "      <td>0.999895</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.081087</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>1.000101</td>\n",
              "      <td>0.000057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>10766</td>\n",
              "      <td>900</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.940818</td>\n",
              "      <td>0.943846</td>\n",
              "      <td>0.938053</td>\n",
              "      <td>0.941069</td>\n",
              "      <td>0.999791</td>\n",
              "      <td>0.998744</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.998889</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.091222</td>\n",
              "      <td>-0.000908</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>0.999091</td>\n",
              "      <td>0.001026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>10866</td>\n",
              "      <td>1000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941807</td>\n",
              "      <td>0.944810</td>\n",
              "      <td>0.948974</td>\n",
              "      <td>0.941075</td>\n",
              "      <td>0.999587</td>\n",
              "      <td>0.995585</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.989362</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.101358</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.004415</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.007527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>11116</td>\n",
              "      <td>1250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941277</td>\n",
              "      <td>0.943621</td>\n",
              "      <td>0.942918</td>\n",
              "      <td>0.941069</td>\n",
              "      <td>0.999697</td>\n",
              "      <td>0.998209</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.998400</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.126698</td>\n",
              "      <td>-0.001397</td>\n",
              "      <td>-0.001676</td>\n",
              "      <td>0.998602</td>\n",
              "      <td>0.001294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>11366</td>\n",
              "      <td>1500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942896</td>\n",
              "      <td>0.945056</td>\n",
              "      <td>0.953814</td>\n",
              "      <td>0.941221</td>\n",
              "      <td>0.998817</td>\n",
              "      <td>0.991994</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>0.960317</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.989333</td>\n",
              "      <td>0.999493</td>\n",
              "      <td>0.152037</td>\n",
              "      <td>-0.010160</td>\n",
              "      <td>-0.007892</td>\n",
              "      <td>0.989835</td>\n",
              "      <td>0.021966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>11616</td>\n",
              "      <td>1750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942596</td>\n",
              "      <td>0.944540</td>\n",
              "      <td>0.950525</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.999903</td>\n",
              "      <td>0.999369</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.999429</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.177377</td>\n",
              "      <td>-0.000369</td>\n",
              "      <td>-0.000631</td>\n",
              "      <td>0.999631</td>\n",
              "      <td>0.001226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>11866</td>\n",
              "      <td>2000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941942</td>\n",
              "      <td>0.944232</td>\n",
              "      <td>0.946814</td>\n",
              "      <td>0.940949</td>\n",
              "      <td>0.999527</td>\n",
              "      <td>0.999444</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.999500</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.202716</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>-0.000100</td>\n",
              "      <td>1.000108</td>\n",
              "      <td>0.000961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>12116</td>\n",
              "      <td>2250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942737</td>\n",
              "      <td>0.944963</td>\n",
              "      <td>0.951759</td>\n",
              "      <td>0.940658</td>\n",
              "      <td>0.998983</td>\n",
              "      <td>0.999511</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.995169</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.999111</td>\n",
              "      <td>0.998682</td>\n",
              "      <td>0.228056</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>1.000429</td>\n",
              "      <td>0.001375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>12366</td>\n",
              "      <td>2500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941736</td>\n",
              "      <td>0.944445</td>\n",
              "      <td>0.944162</td>\n",
              "      <td>0.941120</td>\n",
              "      <td>0.998276</td>\n",
              "      <td>0.992000</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>0.984000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.991200</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.253395</td>\n",
              "      <td>-0.008496</td>\n",
              "      <td>-0.007886</td>\n",
              "      <td>0.991501</td>\n",
              "      <td>0.011032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>12616</td>\n",
              "      <td>2750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.943374</td>\n",
              "      <td>0.944854</td>\n",
              "      <td>0.951382</td>\n",
              "      <td>0.941120</td>\n",
              "      <td>0.999822</td>\n",
              "      <td>0.999599</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.999636</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.278735</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>-0.000287</td>\n",
              "      <td>0.999940</td>\n",
              "      <td>0.001054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>12866</td>\n",
              "      <td>3000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.944011</td>\n",
              "      <td>0.945829</td>\n",
              "      <td>0.953245</td>\n",
              "      <td>0.941170</td>\n",
              "      <td>0.999826</td>\n",
              "      <td>0.999634</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.999667</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.304075</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>-0.000252</td>\n",
              "      <td>1.000072</td>\n",
              "      <td>0.001492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>13116</td>\n",
              "      <td>3250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942206</td>\n",
              "      <td>0.944148</td>\n",
              "      <td>0.946003</td>\n",
              "      <td>0.940949</td>\n",
              "      <td>0.999658</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999392</td>\n",
              "      <td>0.329414</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>1.000609</td>\n",
              "      <td>0.001139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>13366</td>\n",
              "      <td>3500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942838</td>\n",
              "      <td>0.944671</td>\n",
              "      <td>0.948159</td>\n",
              "      <td>0.940936</td>\n",
              "      <td>0.999413</td>\n",
              "      <td>0.999683</td>\n",
              "      <td>0.999316</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.999714</td>\n",
              "      <td>0.998986</td>\n",
              "      <td>0.354754</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>1.000729</td>\n",
              "      <td>0.002005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>13616</td>\n",
              "      <td>3750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.943783</td>\n",
              "      <td>0.945457</td>\n",
              "      <td>0.950581</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.999836</td>\n",
              "      <td>0.999411</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994334</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.998933</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.380093</td>\n",
              "      <td>-0.000864</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>0.999136</td>\n",
              "      <td>0.002216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>13866</td>\n",
              "      <td>4000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942850</td>\n",
              "      <td>0.944893</td>\n",
              "      <td>0.947230</td>\n",
              "      <td>0.941063</td>\n",
              "      <td>0.999111</td>\n",
              "      <td>0.997506</td>\n",
              "      <td>0.999772</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>0.997750</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.405433</td>\n",
              "      <td>-0.001845</td>\n",
              "      <td>-0.002266</td>\n",
              "      <td>0.998155</td>\n",
              "      <td>0.002044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>14116</td>\n",
              "      <td>4250</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.943721</td>\n",
              "      <td>0.945360</td>\n",
              "      <td>0.949944</td>\n",
              "      <td>0.941019</td>\n",
              "      <td>0.999366</td>\n",
              "      <td>0.998182</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997882</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.430772</td>\n",
              "      <td>-0.002016</td>\n",
              "      <td>-0.001704</td>\n",
              "      <td>0.997984</td>\n",
              "      <td>0.003352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>14366</td>\n",
              "      <td>4500</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.942925</td>\n",
              "      <td>0.944479</td>\n",
              "      <td>0.947085</td>\n",
              "      <td>0.941019</td>\n",
              "      <td>0.999532</td>\n",
              "      <td>0.998765</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998222</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.456112</td>\n",
              "      <td>-0.001676</td>\n",
              "      <td>-0.001121</td>\n",
              "      <td>0.998323</td>\n",
              "      <td>0.003894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>14616</td>\n",
              "      <td>4750</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.943129</td>\n",
              "      <td>0.945057</td>\n",
              "      <td>0.949115</td>\n",
              "      <td>0.940222</td>\n",
              "      <td>0.998622</td>\n",
              "      <td>0.999301</td>\n",
              "      <td>0.998289</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999368</td>\n",
              "      <td>0.998480</td>\n",
              "      <td>0.481451</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>1.000890</td>\n",
              "      <td>0.000506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>14866</td>\n",
              "      <td>5000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.941654</td>\n",
              "      <td>0.944317</td>\n",
              "      <td>0.945692</td>\n",
              "      <td>0.939596</td>\n",
              "      <td>0.997812</td>\n",
              "      <td>0.999331</td>\n",
              "      <td>0.997035</td>\n",
              "      <td>0.998051</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999200</td>\n",
              "      <td>0.997365</td>\n",
              "      <td>0.506791</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>0.002297</td>\n",
              "      <td>1.001840</td>\n",
              "      <td>0.002123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>15866</td>\n",
              "      <td>6000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.944015</td>\n",
              "      <td>0.945390</td>\n",
              "      <td>0.949010</td>\n",
              "      <td>0.940955</td>\n",
              "      <td>0.999648</td>\n",
              "      <td>0.999631</td>\n",
              "      <td>0.999658</td>\n",
              "      <td>0.996564</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.999333</td>\n",
              "      <td>0.999595</td>\n",
              "      <td>0.608149</td>\n",
              "      <td>-0.000261</td>\n",
              "      <td>-0.000027</td>\n",
              "      <td>0.999739</td>\n",
              "      <td>0.001276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>16866</td>\n",
              "      <td>7000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.945216</td>\n",
              "      <td>0.946311</td>\n",
              "      <td>0.951075</td>\n",
              "      <td>0.941019</td>\n",
              "      <td>0.999802</td>\n",
              "      <td>0.999685</td>\n",
              "      <td>0.999886</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999714</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.709507</td>\n",
              "      <td>-0.000184</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>0.999816</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>17866</td>\n",
              "      <td>8000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.944714</td>\n",
              "      <td>0.945796</td>\n",
              "      <td>0.949373</td>\n",
              "      <td>0.940905</td>\n",
              "      <td>0.999687</td>\n",
              "      <td>0.999723</td>\n",
              "      <td>0.999658</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999750</td>\n",
              "      <td>0.999696</td>\n",
              "      <td>0.810866</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>1.000054</td>\n",
              "      <td>0.000033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>18866</td>\n",
              "      <td>9000</td>\n",
              "      <td>9866</td>\n",
              "      <td>0.945264</td>\n",
              "      <td>0.945938</td>\n",
              "      <td>0.949764</td>\n",
              "      <td>0.941126</td>\n",
              "      <td>0.999882</td>\n",
              "      <td>0.999754</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>0.999778</td>\n",
              "      <td>0.999899</td>\n",
              "      <td>0.912224</td>\n",
              "      <td>-0.000121</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>0.999879</td>\n",
              "      <td>0.000578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0            9871              5           9866     0.941107   \n",
              "1            9876             10           9866     0.941139   \n",
              "2            9886             20           9866     0.940861   \n",
              "3            9896             30           9866     0.940981   \n",
              "4            9906             40           9866     0.941044   \n",
              "5            9916             50           9866     0.941070   \n",
              "6            9941             75           9866     0.940951   \n",
              "7            9966            100           9866     0.941164   \n",
              "8            9991            125           9866     0.940871   \n",
              "9           10016            150           9866     0.940747   \n",
              "10          10041            175           9866     0.940903   \n",
              "11          10066            200           9866     0.941170   \n",
              "12          10116            250           9866     0.941004   \n",
              "13          10166            300           9866     0.941636   \n",
              "14          10216            350           9866     0.941695   \n",
              "15          10266            400           9866     0.941886   \n",
              "16          10316            450           9866     0.941611   \n",
              "17          10366            500           9866     0.941357   \n",
              "18          10466            600           9866     0.942170   \n",
              "19          10566            700           9866     0.941630   \n",
              "20          10666            800           9866     0.941498   \n",
              "21          10766            900           9866     0.940818   \n",
              "22          10866           1000           9866     0.941807   \n",
              "23          11116           1250           9866     0.941277   \n",
              "24          11366           1500           9866     0.942896   \n",
              "25          11616           1750           9866     0.942596   \n",
              "26          11866           2000           9866     0.941942   \n",
              "27          12116           2250           9866     0.942737   \n",
              "28          12366           2500           9866     0.941736   \n",
              "29          12616           2750           9866     0.943374   \n",
              "30          12866           3000           9866     0.944011   \n",
              "31          13116           3250           9866     0.942206   \n",
              "32          13366           3500           9866     0.942838   \n",
              "33          13616           3750           9866     0.943783   \n",
              "34          13866           4000           9866     0.942850   \n",
              "35          14116           4250           9866     0.943721   \n",
              "36          14366           4500           9866     0.942925   \n",
              "37          14616           4750           9866     0.943129   \n",
              "38          14866           5000           9866     0.941654   \n",
              "39          15866           6000           9866     0.944015   \n",
              "40          16866           7000           9866     0.945216   \n",
              "41          17866           8000           9866     0.944714   \n",
              "42          18866           9000           9866     0.945264   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.943972     1.000000     0.941075      1.000000      1.000000   \n",
              "1            0.944002     1.000000     0.941075      1.000000      1.000000   \n",
              "2            0.943710     0.918919     0.940905      0.999659      1.000000   \n",
              "3            0.944170     0.983051     0.940848      0.999545      1.000000   \n",
              "4            0.943925     0.947368     0.941019      0.999886      1.000000   \n",
              "5            0.944380     0.958333     0.940981      0.999206      1.000000   \n",
              "6            0.944250     0.951049     0.940873      0.999095      1.000000   \n",
              "7            0.944391     0.963731     0.940930      0.999210      1.000000   \n",
              "8            0.944496     0.945148     0.940816      0.998986      1.000000   \n",
              "9            0.944063     0.943662     0.940703      0.998764      1.000000   \n",
              "10           0.944242     0.942598     0.940873      0.999104      1.000000   \n",
              "11           0.944081     0.944591     0.941101      0.999553      1.000000   \n",
              "12           0.944125     0.947368     0.940842      0.999333      0.995575   \n",
              "13           0.944743     0.961806     0.941012      0.999779      1.000000   \n",
              "14           0.944183     0.959762     0.941044      0.999340      0.996904   \n",
              "15           0.944820     0.959583     0.941158      0.999562      0.997290   \n",
              "16           0.944173     0.951049     0.941176      1.000000      1.000000   \n",
              "17           0.944120     0.948367     0.941000      0.999349      0.995575   \n",
              "18           0.944411     0.957428     0.941227      0.999893      0.998188   \n",
              "19           0.944151     0.947368     0.941221      0.999681      0.996835   \n",
              "20           0.944515     0.947368     0.941019      0.999895      1.000000   \n",
              "21           0.943846     0.938053     0.941069      0.999791      0.998744   \n",
              "22           0.944810     0.948974     0.941075      0.999587      0.995585   \n",
              "23           0.943621     0.942918     0.941069      0.999697      0.998209   \n",
              "24           0.945056     0.953814     0.941221      0.998817      0.991994   \n",
              "25           0.944540     0.950525     0.941176      0.999903      0.999369   \n",
              "26           0.944232     0.946814     0.940949      0.999527      0.999444   \n",
              "27           0.944963     0.951759     0.940658      0.998983      0.999511   \n",
              "28           0.944445     0.944162     0.941120      0.998276      0.992000   \n",
              "29           0.944854     0.951382     0.941120      0.999822      0.999599   \n",
              "30           0.945829     0.953245     0.941170      0.999826      0.999634   \n",
              "31           0.944148     0.946003     0.940949      0.999658      1.000000   \n",
              "32           0.944671     0.948159     0.940936      0.999413      0.999683   \n",
              "33           0.945457     0.950581     0.941176      0.999836      0.999411   \n",
              "34           0.944893     0.947230     0.941063      0.999111      0.997506   \n",
              "35           0.945360     0.949944     0.941019      0.999366      0.998182   \n",
              "36           0.944479     0.947085     0.941019      0.999532      0.998765   \n",
              "37           0.945057     0.949115     0.940222      0.998622      0.999301   \n",
              "38           0.944317     0.945692     0.939596      0.997812      0.999331   \n",
              "39           0.945390     0.949010     0.940955      0.999648      0.999631   \n",
              "40           0.946311     0.951075     0.941019      0.999802      0.999685   \n",
              "41           0.945796     0.949373     0.940905      0.999687      0.999723   \n",
              "42           0.945938     0.949764     0.941126      0.999882      0.999754   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       1.000000           NaN      1.000000              1.000000   \n",
              "1       1.000000           NaN      1.000000              1.000000   \n",
              "2       0.999658      1.000000      1.000000              1.000000   \n",
              "3       0.999544      1.000000      1.000000              1.000000   \n",
              "4       0.999886      1.000000      1.000000              1.000000   \n",
              "5       0.999202      1.000000      0.994536              1.000000   \n",
              "6       0.999088      1.000000      0.995446              1.000000   \n",
              "7       0.999202      1.000000      0.995446              1.000000   \n",
              "8       0.998974      1.000000      0.995446              1.000000   \n",
              "9       0.998745      1.000000      0.995446              1.000000   \n",
              "10      0.999088      1.000000      0.995446              1.000000   \n",
              "11      0.999544      1.000000      0.995446              1.000000   \n",
              "12      0.999430      1.000000      0.999089              0.996000   \n",
              "13      0.999772      0.956522      0.999089              0.996667   \n",
              "14      0.999430      0.962963      0.995446              0.994286   \n",
              "15      0.999658      0.967742      0.995446              0.995000   \n",
              "16      1.000000      1.000000      0.998179              1.000000   \n",
              "17      0.999544      0.979167      0.997268              0.994000   \n",
              "18      1.000000      1.000000      0.997268              0.998333   \n",
              "19      0.999886      1.000000      0.996357              0.997143   \n",
              "20      0.999886      1.000000      1.000000              1.000000   \n",
              "21      0.999886      1.000000      0.999089              0.998889   \n",
              "22      1.000000      0.989362      1.000000              0.995000   \n",
              "23      0.999886      1.000000      0.999089              0.998400   \n",
              "24      0.999886      0.960317      0.996357              0.989333   \n",
              "25      1.000000      1.000000      0.998179              0.999429   \n",
              "26      0.999544      1.000000      0.998179              0.999500   \n",
              "27      0.998859      0.995169      0.997268              0.999111   \n",
              "28      0.999886      0.984000      0.998179              0.991200   \n",
              "29      0.999886      1.000000      0.998179              0.999636   \n",
              "30      0.999886      1.000000      0.997268              0.999667   \n",
              "31      0.999544      1.000000      0.998179              1.000000   \n",
              "32      0.999316      1.000000      0.996357              0.999714   \n",
              "33      1.000000      0.994334      0.998179              0.998933   \n",
              "34      0.999772      1.000000      0.998179              0.997750   \n",
              "35      0.999886      0.995000      1.000000              0.997882   \n",
              "36      0.999886      0.993333      1.000000              0.998222   \n",
              "37      0.998289      1.000000      1.000000              0.999368   \n",
              "38      0.997035      0.998051      1.000000              0.999200   \n",
              "39      0.999658      0.996564      0.999089              0.999333   \n",
              "40      0.999886      1.000000      1.000000              0.999714   \n",
              "41      0.999658      1.000000      1.000000              0.999750   \n",
              "42      1.000000      1.000000      0.999089              0.999778   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  stat_parity_diff  \\\n",
              "0               1.000000              0.000507          0.000000   \n",
              "1               1.000000              0.001014          0.000000   \n",
              "2               0.999696              0.002027          0.000304   \n",
              "3               0.999595              0.003041          0.000405   \n",
              "4               0.999899              0.004054          0.000101   \n",
              "5               0.998682              0.005068          0.001318   \n",
              "6               0.998682              0.007602          0.001318   \n",
              "7               0.998784              0.010136          0.001216   \n",
              "8               0.998581              0.012670          0.001419   \n",
              "9               0.998378              0.015204          0.001622   \n",
              "10              0.998682              0.017738          0.001318   \n",
              "11              0.999088              0.020272          0.000912   \n",
              "12              0.999392              0.025340         -0.003392   \n",
              "13              0.999696              0.030407         -0.003029   \n",
              "14              0.998986              0.035475         -0.004701   \n",
              "15              0.999189              0.040543         -0.004189   \n",
              "16              0.999797              0.045611          0.000203   \n",
              "17              0.999290              0.050679         -0.005290   \n",
              "18              0.999696              0.060815         -0.001363   \n",
              "19              0.999493              0.070951         -0.002350   \n",
              "20              0.999899              0.081087          0.000101   \n",
              "21              0.999797              0.091222         -0.000908   \n",
              "22              1.000000              0.101358         -0.005000   \n",
              "23              0.999797              0.126698         -0.001397   \n",
              "24              0.999493              0.152037         -0.010160   \n",
              "25              0.999797              0.177377         -0.000369   \n",
              "26              0.999392              0.202716          0.000108   \n",
              "27              0.998682              0.228056          0.000429   \n",
              "28              0.999696              0.253395         -0.008496   \n",
              "29              0.999696              0.278735         -0.000060   \n",
              "30              0.999595              0.304075          0.000072   \n",
              "31              0.999392              0.329414          0.000608   \n",
              "32              0.998986              0.354754          0.000728   \n",
              "33              0.999797              0.380093         -0.000864   \n",
              "34              0.999595              0.405433         -0.001845   \n",
              "35              0.999899              0.430772         -0.002016   \n",
              "36              0.999899              0.456112         -0.001676   \n",
              "37              0.998480              0.481451          0.000889   \n",
              "38              0.997365              0.506791          0.001835   \n",
              "39              0.999595              0.608149         -0.000261   \n",
              "40              0.999899              0.709507         -0.000184   \n",
              "41              0.999696              0.810866          0.000054   \n",
              "42              0.999899              0.912224         -0.000121   \n",
              "\n",
              "    equal_opport_dist  disparate_impact  aver_abs_odds_diff  \n",
              "0            0.000000          1.000000                 NaN  \n",
              "1            0.000000          1.000000                 NaN  \n",
              "2            0.000342          1.000304            0.000171  \n",
              "3            0.000456          1.000406            0.000228  \n",
              "4            0.000114          1.000101            0.000057  \n",
              "5            0.000798          1.001319            0.003131  \n",
              "6            0.000912          1.001319            0.002733  \n",
              "7            0.000798          1.001218            0.002676  \n",
              "8            0.001026          1.001421            0.002790  \n",
              "9            0.001255          1.001624            0.002904  \n",
              "10           0.000912          1.001319            0.002733  \n",
              "11           0.000456          1.000913            0.002505  \n",
              "12          -0.003855          0.996606            0.002383  \n",
              "13           0.000228          0.996970            0.021398  \n",
              "14          -0.002526          0.995295            0.017505  \n",
              "15          -0.002368          0.995807            0.015036  \n",
              "16           0.000000          1.000203            0.000911  \n",
              "17          -0.003969          0.994706            0.011035  \n",
              "18          -0.001812          0.998637            0.002272  \n",
              "19          -0.003051          0.997648            0.003347  \n",
              "20           0.000114          1.000101            0.000057  \n",
              "21          -0.001142          0.999091            0.001026  \n",
              "22          -0.004415          0.995000            0.007527  \n",
              "23          -0.001676          0.998602            0.001294  \n",
              "24          -0.007892          0.989835            0.021966  \n",
              "25          -0.000631          0.999631            0.001226  \n",
              "26          -0.000100          1.000108            0.000961  \n",
              "27           0.000651          1.000429            0.001375  \n",
              "28          -0.007886          0.991501            0.011032  \n",
              "29          -0.000287          0.999940            0.001054  \n",
              "30          -0.000252          1.000072            0.001492  \n",
              "31           0.000456          1.000609            0.001139  \n",
              "32           0.000367          1.000729            0.002005  \n",
              "33          -0.000589          0.999136            0.002216  \n",
              "34          -0.002266          0.998155            0.002044  \n",
              "35          -0.001704          0.997984            0.003352  \n",
              "36          -0.001121          0.998323            0.003894  \n",
              "37           0.001012          1.000890            0.000506  \n",
              "38           0.002297          1.001840            0.002123  \n",
              "39          -0.000027          0.999739            0.001276  \n",
              "40          -0.000201          0.999816            0.000100  \n",
              "41           0.000066          1.000054            0.000033  \n",
              "42          -0.000246          0.999879            0.000578  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9e1lYjrvSBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a98d0d8c-6a97-4478-ad1a-3a84d8bef3fe"
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_homicide.to_csv(\"df_homicide_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_homicide_metrics.csv\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2c08c5c6db22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save metrics csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_df_homicide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_homicide_metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_homicide_metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df_homicide' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFknrG64eQwJ",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3HvVMvSU3Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart_selection(metric_df = results_df_homicide, title=\"Fairness and Performance Metrics for the Homicide Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2nN17MLRMGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performance Learning Curve\n",
        "\n",
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Dummy Coding\n",
        "df_homicide_train_input = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_juvenile, \n",
        "                    title = \"Juvenile Justice Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_homicide_train_input, y = df_homicide_train_label, \n",
        "                    cv = 3, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5PQKjKwpdT",
        "colab_type": "text"
      },
      "source": [
        "## 4) German Credit Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwsnRs9XPxz",
        "colab_type": "text"
      },
      "source": [
        "The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [42].\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9tpy66--lw",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsfLldE3OLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Long dataset version\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# path_credit_uci = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "path_credit_uci = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "column_names_credit = [\"chk_acct\", \"duration\", \"credit_his\", \"purpose\", \"amount\", \"saving_acct\", \n",
        "                       \"present_emp\", \"installment_rate\", \"sex\", \"other_debtor\", \"present_resid\", \n",
        "                       \"property\", \"age\", \"other_install\", \"housing\", \"n_credits\", \"job\", \"n_people\", \n",
        "                       \"telephone\", \"foreign\", \"response\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_credit_long = pd.read_csv(path_credit_uci, delim_whitespace=True, names = column_names_credit)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGCylXv_vSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Short dataset version\n",
        "path_credit = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "df_credit_short = pd.read_csv(path_credit, header = 0, sep = \";\")"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmMJQzTeRjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join Risk and Sex column of the short dataset to rest of columns of the long dataset\n",
        "df_credit_long['id'] = range(0, len(df_credit_long))\n",
        "\n",
        "# Join based on \"id\" column - add risk and Sex column from short dataset to long dataset\n",
        "df_credit_short_reduced_cols = df_credit_short.loc[:, [\"id\", \"Sex\", \"Risk\"]]\n",
        "df_credit = pd.merge(df_credit_long, df_credit_short_reduced_cols, how = \"left\", on = \"id\")\n",
        "df_credit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9tFALHm5sc",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv9rHRxsM-ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop unnecessary columns\n",
        "df_credit = df_credit.drop(['id', \"response\"], axis=1)\n",
        "\n",
        "df_credit[\"Risk\"] = df_credit[\"Risk\"].replace({'good': 1, 'bad': 0})\n",
        "\n",
        "df_credit.rename(columns={'sex':'sex_specific'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqrA4Pbc_as",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiICbX-HE02N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit.info)\n",
        "print(df_credit.describe())\n",
        "print(df_credit.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x9J2edEZ1Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit)\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enom-Sm6gUsV",
        "colab_type": "text"
      },
      "source": [
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9KaFduOowWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze class balance for dataset with only unprivileged and privileged dataset\n",
        "#is_unpriv = df_homicide[\"Perpetrator Race\"].isin([\"Black\"])\n",
        "#is_priv = df_homicide[\"Perpetrator Race\"].isin([\"White\"])\n",
        "#df_unpriv_homicide = df_homicide[is_unpriv]\n",
        "#df_priv_homicide = df_homicide[is_priv]\n",
        "\n",
        "#df_unpriv_priv_complete_homicide = pd.concat([df_unpriv_homicide, df_priv_homicide])\n",
        "\n",
        "#print(df_unpriv_priv_complete_homicide[\"Victim\"].value_counts(dropna=False))\n",
        "#print(df_unpriv_priv_complete_homicide[\"Victim\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB5Mn6BSh5lL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "41684feb-27c5-48c7-dd4a-3bdfc8ac8cee"
      },
      "source": [
        "df_credit.set_index('Sex').isna().sum(level=0)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chk_acct</th>\n",
              "      <th>duration</th>\n",
              "      <th>credit_his</th>\n",
              "      <th>purpose</th>\n",
              "      <th>amount</th>\n",
              "      <th>saving_acct</th>\n",
              "      <th>present_emp</th>\n",
              "      <th>installment_rate</th>\n",
              "      <th>sex</th>\n",
              "      <th>other_debtor</th>\n",
              "      <th>present_resid</th>\n",
              "      <th>property</th>\n",
              "      <th>age</th>\n",
              "      <th>other_install</th>\n",
              "      <th>housing</th>\n",
              "      <th>n_credits</th>\n",
              "      <th>job</th>\n",
              "      <th>n_people</th>\n",
              "      <th>telephone</th>\n",
              "      <th>foreign</th>\n",
              "      <th>Risk</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sex</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>male</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>female</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        chk_acct  duration  credit_his  purpose  amount  saving_acct  \\\n",
              "Sex                                                                    \n",
              "male       False     False       False    False   False        False   \n",
              "female     False     False       False    False   False        False   \n",
              "\n",
              "        present_emp  installment_rate    sex  other_debtor  present_resid  \\\n",
              "Sex                                                                         \n",
              "male          False             False  False         False          False   \n",
              "female        False             False  False         False          False   \n",
              "\n",
              "        property    age  other_install  housing  n_credits    job  n_people  \\\n",
              "Sex                                                                           \n",
              "male       False  False          False    False      False  False     False   \n",
              "female     False  False          False    False      False  False     False   \n",
              "\n",
              "        telephone  foreign   Risk  \n",
              "Sex                                \n",
              "male        False    False  False  \n",
              "female      False    False  False  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njYG0xUTEouo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50f2a91c-05ae-43dd-9977-3af96c11e361"
      },
      "source": [
        "eda_descr_stats(df_credit, disc_feature=\"Sex\", disc_min_value=\"female\", label =\"Risk\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Sensitive Attribute: One or more of the following features are sensitive ones: Index(['chk_acct', 'duration', 'credit_his', 'purpose', 'amount',\n",
            "       'saving_acct', 'present_emp', 'installment_rate', 'sex', 'other_debtor',\n",
            "       'present_resid', 'property', 'age', 'other_install', 'housing',\n",
            "       'n_credits', 'job', 'n_people', 'telephone', 'foreign', 'Sex', 'Risk'],\n",
            "      dtype='object').\n",
            "1. Sensitive Attribute: These are the individual values for the sensitive attribute: ['male' 'female'].\n",
            "2. Binary Target Variable: The Binary Target Feature has the following values and counts:\n",
            "      Risk\n",
            "Risk      \n",
            "0      300\n",
            "1      700\n",
            "3. The Total Number of Predictor Features is: 22.\n",
            "4. The Total Number of Training Examples is: 1000.\n",
            "5. The Total Number of Training Examples in the Minority Group is: 310.\n",
            "6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  male      690\n",
            "female    310\n",
            "Name: Sex, dtype: int64.\n",
            "6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: male      0.69\n",
            "female    0.31\n",
            "Name: Sex, dtype: float64.\n",
            "7. Class Balance: The Class Balance looks as follows:\n",
            "1    700\n",
            "0    300\n",
            "Name: Risk, dtype: int64\n",
            "1    0.7\n",
            "0    0.3\n",
            "Name: Risk, dtype: float64\n",
            "8. Coarseness of Features: Details on missing values of features in the dataset:\n",
            "chk_acct            0\n",
            "duration            0\n",
            "credit_his          0\n",
            "purpose             0\n",
            "amount              0\n",
            "saving_acct         0\n",
            "present_emp         0\n",
            "installment_rate    0\n",
            "sex                 0\n",
            "other_debtor        0\n",
            "present_resid       0\n",
            "property            0\n",
            "age                 0\n",
            "other_install       0\n",
            "housing             0\n",
            "n_credits           0\n",
            "job                 0\n",
            "n_people            0\n",
            "telephone           0\n",
            "foreign             0\n",
            "Sex                 0\n",
            "Risk                0\n",
            "dtype: int64\n",
            "        chk_acct  duration  credit_his  purpose  amount  saving_acct  \\\n",
            "Sex                                                                    \n",
            "female         0         0           0        0       0            0   \n",
            "male           0         0           0        0       0            0   \n",
            "\n",
            "        present_emp  installment_rate  sex  other_debtor  present_resid  \\\n",
            "Sex                                                                       \n",
            "female            0                 0    0             0              0   \n",
            "male              0                 0    0             0              0   \n",
            "\n",
            "        property  age  other_install  housing  n_credits  job  n_people  \\\n",
            "Sex                                                                       \n",
            "female         0    0              0        0          0    0         0   \n",
            "male           0    0              0        0          0    0         0   \n",
            "\n",
            "        telephone  foreign  Sex  Risk  \n",
            "Sex                                    \n",
            "female          0        0    0     0  \n",
            "male            0        0    0     0  \n",
            "        chk_acct  duration  credit_his  purpose  amount  saving_acct  \\\n",
            "Sex                                                                    \n",
            "female       0.0       0.0         0.0      0.0     0.0          0.0   \n",
            "male         0.0       0.0         0.0      0.0     0.0          0.0   \n",
            "\n",
            "        present_emp  installment_rate  sex  other_debtor  present_resid  \\\n",
            "Sex                                                                       \n",
            "female          0.0               0.0  0.0           0.0            0.0   \n",
            "male            0.0               0.0  0.0           0.0            0.0   \n",
            "\n",
            "        property  age  other_install  housing  n_credits  job  n_people  \\\n",
            "Sex                                                                       \n",
            "female       0.0  0.0            0.0      0.0        0.0  0.0       0.0   \n",
            "male         0.0  0.0            0.0      0.0        0.0  0.0       0.0   \n",
            "\n",
            "        telephone  foreign  Sex  Risk  \n",
            "Sex                                    \n",
            "female        0.0      0.0  0.0   0.0  \n",
            "male          0.0      0.0  0.0   0.0  \n",
            "9. Severity of Outliers for Numeric Features\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEIZJoydbve",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxL3Q3hdjgM",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvAybUs5FHo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "5ccdc21f-8dda-439b-8245-d5a0c7d32631"
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_credit_train_input, df_credit_train_label = fair_preprocess(data = df_credit, \n",
        "                                                               label = \"Risk\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))\n",
        "print(df_credit_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Risk\n",
            "Risk      \n",
            "0      300\n",
            "1      700\n",
            "Index(['chk_acct', 'duration', 'credit_his', 'purpose', 'amount',\n",
            "       'saving_acct', 'present_emp', 'installment_rate', 'sex', 'other_debtor',\n",
            "       'present_resid', 'property', 'age', 'other_install', 'housing',\n",
            "       'n_credits', 'job', 'n_people', 'telephone', 'foreign', 'Sex'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXRRU87PIm-m",
        "colab_type": "text"
      },
      "source": [
        "Attempt to resolve data leakage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXeGbWVETX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attempt to resolve data leakage\n",
        "# Evaluate using Cross Validation\n",
        "\n",
        "model_credit = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=9,\n",
        "              min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "df_credit_train_input_dummy_test = pd.get_dummies(df_credit_train_input)\n",
        "X = df_credit_train_input_dummy_test\n",
        "Y = df_credit_train_input_label\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Results\n",
        "results = model_selection.cross_val_score(model_credit, X, Y, cv=kfold, scoring = f1)\n",
        "print(results)\n",
        "print(results.std())\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb8wQtiwIrG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_credit, \n",
        "                    title = \"German Credit Dataset - Performance Learning Curve\", \n",
        "                    X = df_credit_train_input_dummy_test, y = df_credit_train_input_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZBCQDyXVaIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "credit_model.fit(df_credit_train_input, df_credit_train_label)\n",
        "plot_importance(credit_model)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buM8oHaXdrdm",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7mw8mSvF2lB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "edae66a0-5c9a-4f63-cfa9-b058d527838a"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.2],\n",
        "              \"n_estimators\": [10, 20, 30, 40, 50],\n",
        "              \"max_depth\": [6, 9, 12], \n",
        "              \"min_child_weight\": [0.5, 1, 3, 5],   \n",
        "              \"reg_lambda\": [1, 1.2]}\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=5\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= recall,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_credit_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.2, 'max_depth': 9, 'min_child_weight': 1, 'n_estimators': 30, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.2, max_delta_step=0, max_depth=9,\n",
            "              min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ0lfdI0dzit",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOicTRmoQ8qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e2543a86-3444-4162-d2ae-5f52864819f3"
      },
      "source": [
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_credit[\"Sex\"].isin([\"female\"])\n",
        "is_priv = df_credit[\"Sex\"].isin([\"male\"])\n",
        "df_credit_unpriv = df_credit[is_unpriv]  # Minority\n",
        "df_credit_priv = df_credit[is_priv]  # Majority\n",
        "\n",
        "list_dfs_credit = create_datasets(min_data = df_credit_unpriv, maj_data = df_credit_priv, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "[695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPC2GQkWfyV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "77f81f69-f3db-4e00-c0ae-1fcfe5a329a9"
      },
      "source": [
        "print(len(df_credit_unpriv))\n",
        "print(len(df_credit_priv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "310\n",
            "690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArInhVkxd9N0",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4-M2nsFd9FA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3dd3b925-3559-4de8-b84c-c8262fb7d3f1"
      },
      "source": [
        "list_dfs = list_dfs_credit\n",
        "label = \"Risk\"\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                             learning_rate=0.2, max_delta_step=0, max_depth=9,\n",
        "                             min_child_weight=1, missing=None, n_estimators=30, n_jobs=1,\n",
        "                             nthread=4, objective='binary:logistic', random_state=0,\n",
        "                             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                             silent=None, subsample=1, verbosity=1)\n",
        "cv = 3\n",
        "discr_feature = \"Sex\"\n",
        "min_value = \"female\"\n",
        "maj_value = \"male\"\n",
        "\n",
        "results_df_credit = metrics_to_df(list_dfs=list_dfs_credit, label = label, model = credit_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "\n",
        "results_df_credit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>695</td>\n",
              "      <td>5</td>\n",
              "      <td>690</td>\n",
              "      <td>0.832861</td>\n",
              "      <td>0.999255</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.832381</td>\n",
              "      <td>0.876740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875752</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.596859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.007246</td>\n",
              "      <td>0.263695</td>\n",
              "      <td>0.201449</td>\n",
              "      <td>0.124248</td>\n",
              "      <td>1.252269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700</td>\n",
              "      <td>10</td>\n",
              "      <td>690</td>\n",
              "      <td>0.848256</td>\n",
              "      <td>0.999013</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.848138</td>\n",
              "      <td>0.889328</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.544503</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.794203</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.121903</td>\n",
              "      <td>-0.094203</td>\n",
              "      <td>-0.032637</td>\n",
              "      <td>0.881387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>710</td>\n",
              "      <td>20</td>\n",
              "      <td>690</td>\n",
              "      <td>0.842686</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.843100</td>\n",
              "      <td>0.892788</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.893788</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.591623</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.810145</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>0.064134</td>\n",
              "      <td>-0.060145</td>\n",
              "      <td>-0.036645</td>\n",
              "      <td>0.925760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>720</td>\n",
              "      <td>30</td>\n",
              "      <td>690</td>\n",
              "      <td>0.831025</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.836364</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.875752</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.570681</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.791304</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.084820</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.052820</td>\n",
              "      <td>1.010989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>730</td>\n",
              "      <td>40</td>\n",
              "      <td>690</td>\n",
              "      <td>0.843155</td>\n",
              "      <td>0.998812</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.847619</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.891784</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.554974</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.057971</td>\n",
              "      <td>0.088763</td>\n",
              "      <td>-0.123551</td>\n",
              "      <td>-0.122553</td>\n",
              "      <td>0.845281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>740</td>\n",
              "      <td>50</td>\n",
              "      <td>690</td>\n",
              "      <td>0.848057</td>\n",
              "      <td>0.999534</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.848197</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.895792</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.026920</td>\n",
              "      <td>0.015652</td>\n",
              "      <td>-0.003900</td>\n",
              "      <td>1.019459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>765</td>\n",
              "      <td>75</td>\n",
              "      <td>690</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.999091</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.845420</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.887776</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.554974</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.795652</td>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.048625</td>\n",
              "      <td>0.017681</td>\n",
              "      <td>0.012224</td>\n",
              "      <td>1.022222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>790</td>\n",
              "      <td>100</td>\n",
              "      <td>690</td>\n",
              "      <td>0.843252</td>\n",
              "      <td>0.999128</td>\n",
              "      <td>0.813793</td>\n",
              "      <td>0.847328</td>\n",
              "      <td>0.879371</td>\n",
              "      <td>0.808219</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.481481</td>\n",
              "      <td>0.549738</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.795652</td>\n",
              "      <td>0.144928</td>\n",
              "      <td>0.074909</td>\n",
              "      <td>-0.075652</td>\n",
              "      <td>-0.081560</td>\n",
              "      <td>0.904918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>815</td>\n",
              "      <td>125</td>\n",
              "      <td>690</td>\n",
              "      <td>0.828383</td>\n",
              "      <td>0.999566</td>\n",
              "      <td>0.759036</td>\n",
              "      <td>0.839388</td>\n",
              "      <td>0.871528</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.879760</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>0.712000</td>\n",
              "      <td>0.792754</td>\n",
              "      <td>0.181159</td>\n",
              "      <td>0.042678</td>\n",
              "      <td>-0.080754</td>\n",
              "      <td>-0.061578</td>\n",
              "      <td>0.898135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>840</td>\n",
              "      <td>150</td>\n",
              "      <td>690</td>\n",
              "      <td>0.831210</td>\n",
              "      <td>0.998960</td>\n",
              "      <td>0.757282</td>\n",
              "      <td>0.845714</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.772277</td>\n",
              "      <td>0.889780</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.560209</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.798551</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.063346</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>-0.117502</td>\n",
              "      <td>0.876588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>865</td>\n",
              "      <td>175</td>\n",
              "      <td>690</td>\n",
              "      <td>0.805259</td>\n",
              "      <td>0.999387</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.830739</td>\n",
              "      <td>0.801964</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.855711</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.534031</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.253623</td>\n",
              "      <td>0.302510</td>\n",
              "      <td>-0.326667</td>\n",
              "      <td>-0.293211</td>\n",
              "      <td>0.573913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>890</td>\n",
              "      <td>200</td>\n",
              "      <td>690</td>\n",
              "      <td>0.818824</td>\n",
              "      <td>0.999206</td>\n",
              "      <td>0.715517</td>\n",
              "      <td>0.841802</td>\n",
              "      <td>0.829889</td>\n",
              "      <td>0.638462</td>\n",
              "      <td>0.879760</td>\n",
              "      <td>0.271429</td>\n",
              "      <td>0.549738</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.788406</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.259804</td>\n",
              "      <td>-0.278406</td>\n",
              "      <td>-0.241298</td>\n",
              "      <td>0.646875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>940</td>\n",
              "      <td>250</td>\n",
              "      <td>690</td>\n",
              "      <td>0.823964</td>\n",
              "      <td>0.999049</td>\n",
              "      <td>0.719472</td>\n",
              "      <td>0.854147</td>\n",
              "      <td>0.847793</td>\n",
              "      <td>0.689873</td>\n",
              "      <td>0.897796</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>0.534031</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.797101</td>\n",
              "      <td>0.362319</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>-0.217101</td>\n",
              "      <td>-0.207922</td>\n",
              "      <td>0.727636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>990</td>\n",
              "      <td>300</td>\n",
              "      <td>690</td>\n",
              "      <td>0.829368</td>\n",
              "      <td>0.999280</td>\n",
              "      <td>0.796117</td>\n",
              "      <td>0.842304</td>\n",
              "      <td>0.878963</td>\n",
              "      <td>0.841026</td>\n",
              "      <td>0.893788</td>\n",
              "      <td>0.504762</td>\n",
              "      <td>0.596859</td>\n",
              "      <td>0.723333</td>\n",
              "      <td>0.811594</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.072429</td>\n",
              "      <td>-0.088261</td>\n",
              "      <td>-0.052762</td>\n",
              "      <td>0.891250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0             695              5            690     0.832861   \n",
              "1             700             10            690     0.848256   \n",
              "2             710             20            690     0.842686   \n",
              "3             720             30            690     0.831025   \n",
              "4             730             40            690     0.843155   \n",
              "5             740             50            690     0.848057   \n",
              "6             765             75            690     0.842105   \n",
              "7             790            100            690     0.843252   \n",
              "8             815            125            690     0.828383   \n",
              "9             840            150            690     0.831210   \n",
              "10            865            175            690     0.805259   \n",
              "11            890            200            690     0.818824   \n",
              "12            940            250            690     0.823964   \n",
              "13            990            300            690     0.829368   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.999255     0.888889     0.832381      0.876740      1.000000   \n",
              "1            0.999013     0.857143     0.848138      0.889328      0.857143   \n",
              "2            0.999270     0.827586     0.843100      0.892788      0.857143   \n",
              "3            0.999270     0.684211     0.836364      0.877193      0.928571   \n",
              "4            0.998812     0.754717     0.847619      0.885714      0.769231   \n",
              "5            0.999534     0.846154     0.848197      0.895522      0.891892   \n",
              "6            0.999091     0.810811     0.845420      0.888889      0.900000   \n",
              "7            0.999128     0.813793     0.847328      0.879371      0.808219   \n",
              "8            0.999566     0.759036     0.839388      0.871528      0.818182   \n",
              "9            0.998960     0.757282     0.845714      0.870000      0.772277   \n",
              "10           0.999387     0.666667     0.830739      0.801964      0.562500   \n",
              "11           0.999206     0.715517     0.841802      0.829889      0.638462   \n",
              "12           0.999049     0.719472     0.854147      0.847793      0.689873   \n",
              "13           0.999280     0.796117     0.842304      0.878963      0.841026   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.875752      1.000000      0.596859              1.000000   \n",
              "1       0.889780      0.333333      0.544503              0.700000   \n",
              "2       0.893788      0.500000      0.591623              0.750000   \n",
              "3       0.875752      0.687500      0.570681              0.800000   \n",
              "4       0.891784      0.500000      0.554974              0.675000   \n",
              "5       0.895792      0.615385      0.565445              0.820000   \n",
              "6       0.887776      0.640000      0.554974              0.813333   \n",
              "7       0.889780      0.481481      0.549738              0.720000   \n",
              "8       0.879760      0.541667      0.565445              0.712000   \n",
              "9       0.889780      0.551020      0.560209              0.700000   \n",
              "10      0.855711      0.222222      0.534031              0.440000   \n",
              "11      0.879760      0.271429      0.549738              0.510000   \n",
              "12      0.897796      0.391304      0.534031              0.580000   \n",
              "13      0.893788      0.504762      0.596859              0.723333   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  aver_abs_odds_diff  \\\n",
              "0               0.798551              0.007246            0.263695   \n",
              "1               0.794203              0.014493            0.121903   \n",
              "2               0.810145              0.028986            0.064134   \n",
              "3               0.791304              0.043478            0.084820   \n",
              "4               0.798551              0.057971            0.088763   \n",
              "5               0.804348              0.072464            0.026920   \n",
              "6               0.795652              0.108696            0.048625   \n",
              "7               0.795652              0.144928            0.074909   \n",
              "8               0.792754              0.181159            0.042678   \n",
              "9               0.798551              0.217391            0.063346   \n",
              "10              0.766667              0.253623            0.302510   \n",
              "11              0.788406              0.289855            0.259804   \n",
              "12              0.797101              0.362319            0.175325   \n",
              "13              0.811594              0.434783            0.072429   \n",
              "\n",
              "    stat_parity_diff  equal_opport_dist  disparate_impact  \n",
              "0           0.201449           0.124248          1.252269  \n",
              "1          -0.094203          -0.032637          0.881387  \n",
              "2          -0.060145          -0.036645          0.925760  \n",
              "3           0.008696           0.052820          1.010989  \n",
              "4          -0.123551          -0.122553          0.845281  \n",
              "5           0.015652          -0.003900          1.019459  \n",
              "6           0.017681           0.012224          1.022222  \n",
              "7          -0.075652          -0.081560          0.904918  \n",
              "8          -0.080754          -0.061578          0.898135  \n",
              "9          -0.098551          -0.117502          0.876588  \n",
              "10         -0.326667          -0.293211          0.573913  \n",
              "11         -0.278406          -0.241298          0.646875  \n",
              "12         -0.217101          -0.207922          0.727636  \n",
              "13         -0.088261          -0.052762          0.891250  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-PhSga0SMUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_credit.to_csv(\"df_credit_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_credit_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLhop7beSsL",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XxyeSYGVJEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart_selection(metric_df = results_df_credit, title=\"Fairness and Performance Metrics for the German Credit Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0krkR9GRNrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "8e8abdf1-6faa-42ba-b06e-b12387bd0f04"
      },
      "source": [
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# Dummy Coding\n",
        "df_credit_train_input = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_juvenile, \n",
        "                    title = \"German Credit Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_credit_train_input, y = df_credit_train_label, \n",
        "                    cv = 3, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRUHBMJRTiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0470f150-b236-4785-ef44-c5c2a6068366"
      },
      "source": [
        "min_metrics_line_chart(results_df_credit, title = \"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f4690177-5cac-47a8-9f14-f6e1f1631ad3',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.03157624148821203, 0.03157624148821203, 0.029572233472179965, 0.028570229464163932, 0.03319203852731645, 0.03319203852731645, 0.04705746571677388, 0.041433652645605334, 0.025417448802389886, 0.003245936395879445, 0.39914604224008365, 0.5509887854708386, 0.030733453168714786, 0.03777228250180936]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.024637681159420333, 0.024637681159420333, 0.021739130434782594, 0.020289855072463725, 0.024637681159420333, 0.024637681159420333, 0.03768115942028982, 0.03188405797101446, 0.02168115942028981, -0.0008695652173913437, -0.38252587991718423, -0.5544202898550724, 0.017275362318840526, 0.02681159420289847]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.01603206412825653, 0.01603206412825653, 0.012024048096192397, 0.01002004008016033, 0.014028056112224463, 0.014028056112224463, 0.02605210420841686, 0.02004008016032066, 0.024048096192384794, 0.003429539491353828, -0.32233558025141185, -0.5453764672201546, 0.0038136879820246383, 0.015951094107406805]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0252600297176822, 1.0252600297176822, 1.0222222222222221, 1.0207100591715976, 1.0252600297176822, 1.0252600297176822, 1.0391566265060241, 1.0329341317365268, 1.0225301204819277, 0.9990950226244344, 0.6083933870284018, 0.42212990936555894, 1.018170731707317, 1.0280303030303028]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f4690177-5cac-47a8-9f14-f6e1f1631ad3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxycTP8pRYzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "a81e47e1-fab2-48a4-9642-167f27a78b65"
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_credit, title = \"Majority and Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd5e52a13-47e6-4c9f-8dee-1a5e6363abb8',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.8378839590443685, 0.8378839590443685, 0.8398637137989777, 0.8408510638297872, 0.8395904436860069, 0.8395904436860069, 0.8357695614789338, 0.8380462724935732, 0.8374892519346518, 0.8382099827882961, 0.8388746803069054, 0.8372093023255814, 0.8398268398268398, 0.8386540120793787]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9839679358717435, 0.9839679358717435, 0.9879759519038076, 0.9899799599198397, 0.9859719438877755, 0.9859719438877755, 0.9739478957915831, 0.9799599198396793, 0.9759519038076152, 0.9759519038076152, 0.9859719438877755, 0.9739478957915831, 0.9719438877755511, 0.9739478957915831]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9476439790575916, 0.9476439790575916, 0.9319371727748691, 0.93717277486911, 0.9267015706806283, 0.9214659685863874, 0.9528795811518325, 0.9214659685863874, 0.8952879581151832, 0.9109947643979057]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d5e52a13-47e6-4c9f-8dee-1a5e6363abb8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgjHGsyBv5q",
        "colab_type": "text"
      },
      "source": [
        "## 5) Home Credit Default Risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JokphHiwjPhH",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/c/home-credit-default-risk/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14pe6cBdFMR",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpIKmP03oKYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Master Thesis/Data/application_train.csv\"\n",
        "df_application = pd.read_csv(path)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0rjQsPtmTKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yL5zLpHm977",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QrkTKrr-0Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application = df_application.drop([\"SK_ID_CURR\"], axis=1)\n",
        "\n",
        "# 1 should be positive (= good consequence, predict to repay load), 0 should be negative (= bad consequence, predict not to repay the loan)\n",
        "df_application[\"TARGET\"] = df_application[\"TARGET\"].replace({1: \"no_repay\", 0: \"repay\"})\n",
        "df_application[\"TARGET\"] = df_application[\"TARGET\"].replace({\"no_repay\": 0, \"repay\": 1})\n",
        "\n",
        "# Replace XNA for CODE_GENDER by NaN\n",
        "df_application[\"CODE_GENDER\"] = df_application[\"CODE_GENDER\"].replace('XNA', np.nan)\n",
        "df_application = df_application.replace(r'^\\s*$', np.nan, regex=True)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBMUx46IHssC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "4cd5f623-5b21-4425-e7cc-8413dad4f24a"
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_application.groupby([\"TARGET\"]).agg({\"TARGET\": 'count'}))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        TARGET\n",
            "TARGET        \n",
            "0        24825\n",
            "1       282686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9wvVDK-HiBd",
        "colab_type": "text"
      },
      "source": [
        "**Dataframe Size Reduction Efforts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFQisM73Hul8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phrZc26CHcRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "9d83ff64-f901-452c-afa4-eabc7f7eb632"
      },
      "source": [
        "# Check which columns are suitable for conversion in \"category\" data format\n",
        "## We should stick to using the category type primarily for object columns where less than 50% of the values are unique.\n",
        "df_application_copy = df_application.select_dtypes(include=['object']).copy()\n",
        "df_application_copy.describe()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME_CONTRACT_TYPE</th>\n",
              "      <th>CODE_GENDER</th>\n",
              "      <th>FLAG_OWN_CAR</th>\n",
              "      <th>FLAG_OWN_REALTY</th>\n",
              "      <th>NAME_TYPE_SUITE</th>\n",
              "      <th>NAME_INCOME_TYPE</th>\n",
              "      <th>NAME_EDUCATION_TYPE</th>\n",
              "      <th>NAME_FAMILY_STATUS</th>\n",
              "      <th>NAME_HOUSING_TYPE</th>\n",
              "      <th>OCCUPATION_TYPE</th>\n",
              "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
              "      <th>ORGANIZATION_TYPE</th>\n",
              "      <th>FONDKAPREMONT_MODE</th>\n",
              "      <th>HOUSETYPE_MODE</th>\n",
              "      <th>WALLSMATERIAL_MODE</th>\n",
              "      <th>EMERGENCYSTATE_MODE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>307511</td>\n",
              "      <td>307507</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>306219</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>211120</td>\n",
              "      <td>307511</td>\n",
              "      <td>307511</td>\n",
              "      <td>97216</td>\n",
              "      <td>153214</td>\n",
              "      <td>151170</td>\n",
              "      <td>161756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Cash loans</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Unaccompanied</td>\n",
              "      <td>Working</td>\n",
              "      <td>Secondary / secondary special</td>\n",
              "      <td>Married</td>\n",
              "      <td>House / apartment</td>\n",
              "      <td>Laborers</td>\n",
              "      <td>TUESDAY</td>\n",
              "      <td>Business Entity Type 3</td>\n",
              "      <td>reg oper account</td>\n",
              "      <td>block of flats</td>\n",
              "      <td>Panel</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>278232</td>\n",
              "      <td>202448</td>\n",
              "      <td>202924</td>\n",
              "      <td>213312</td>\n",
              "      <td>248526</td>\n",
              "      <td>158774</td>\n",
              "      <td>218391</td>\n",
              "      <td>196432</td>\n",
              "      <td>272868</td>\n",
              "      <td>55186</td>\n",
              "      <td>53901</td>\n",
              "      <td>67992</td>\n",
              "      <td>73830</td>\n",
              "      <td>150503</td>\n",
              "      <td>66040</td>\n",
              "      <td>159428</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  \\\n",
              "count              307511      307507       307511          307511   \n",
              "unique                  2           2            2               2   \n",
              "top            Cash loans           F            N               Y   \n",
              "freq               278232      202448       202924          213312   \n",
              "\n",
              "       NAME_TYPE_SUITE NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
              "count           306219           307511                         307511   \n",
              "unique               7                8                              5   \n",
              "top      Unaccompanied          Working  Secondary / secondary special   \n",
              "freq            248526           158774                         218391   \n",
              "\n",
              "       NAME_FAMILY_STATUS  NAME_HOUSING_TYPE OCCUPATION_TYPE  \\\n",
              "count              307511             307511          211120   \n",
              "unique                  6                  6              18   \n",
              "top               Married  House / apartment        Laborers   \n",
              "freq               196432             272868           55186   \n",
              "\n",
              "       WEEKDAY_APPR_PROCESS_START       ORGANIZATION_TYPE FONDKAPREMONT_MODE  \\\n",
              "count                      307511                  307511              97216   \n",
              "unique                          7                      58                  4   \n",
              "top                       TUESDAY  Business Entity Type 3   reg oper account   \n",
              "freq                        53901                   67992              73830   \n",
              "\n",
              "        HOUSETYPE_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE  \n",
              "count           153214             151170              161756  \n",
              "unique               3                  7                   2  \n",
              "top     block of flats              Panel                  No  \n",
              "freq            150503              66040              159428  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lm-m7IeH59S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the size of the numeric columns\n",
        "\n",
        "df_application, NAlist = reduce_mem_usage(df_application)\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\n",
        "print(\"_________________\")\n",
        "print(\"\")\n",
        "print(NAlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3m20uYXIyiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_application.select_dtypes(include=['object']).columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qkLqZNcH60e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "            'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
        "            'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
        "            'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE',\n",
        "            'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']:\n",
        "    df_application[col] = df_application[col].astype('category')"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEVy0agdEPj",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpuRlqDCRbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "84787af4-679c-4043-8c36-b8b9e78771c5"
      },
      "source": [
        "df_application.set_index('CODE_GENDER').isna().sum(level=0)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TARGET</th>\n",
              "      <th>NAME_CONTRACT_TYPE</th>\n",
              "      <th>FLAG_OWN_CAR</th>\n",
              "      <th>FLAG_OWN_REALTY</th>\n",
              "      <th>CNT_CHILDREN</th>\n",
              "      <th>AMT_INCOME_TOTAL</th>\n",
              "      <th>AMT_CREDIT</th>\n",
              "      <th>AMT_ANNUITY</th>\n",
              "      <th>AMT_GOODS_PRICE</th>\n",
              "      <th>NAME_TYPE_SUITE</th>\n",
              "      <th>NAME_INCOME_TYPE</th>\n",
              "      <th>NAME_EDUCATION_TYPE</th>\n",
              "      <th>NAME_FAMILY_STATUS</th>\n",
              "      <th>NAME_HOUSING_TYPE</th>\n",
              "      <th>REGION_POPULATION_RELATIVE</th>\n",
              "      <th>DAYS_BIRTH</th>\n",
              "      <th>DAYS_EMPLOYED</th>\n",
              "      <th>DAYS_REGISTRATION</th>\n",
              "      <th>DAYS_ID_PUBLISH</th>\n",
              "      <th>OWN_CAR_AGE</th>\n",
              "      <th>FLAG_MOBIL</th>\n",
              "      <th>FLAG_EMP_PHONE</th>\n",
              "      <th>FLAG_WORK_PHONE</th>\n",
              "      <th>FLAG_CONT_MOBILE</th>\n",
              "      <th>FLAG_PHONE</th>\n",
              "      <th>FLAG_EMAIL</th>\n",
              "      <th>OCCUPATION_TYPE</th>\n",
              "      <th>CNT_FAM_MEMBERS</th>\n",
              "      <th>REGION_RATING_CLIENT</th>\n",
              "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
              "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
              "      <th>HOUR_APPR_PROCESS_START</th>\n",
              "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
              "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
              "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
              "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
              "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
              "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
              "      <th>ORGANIZATION_TYPE</th>\n",
              "      <th>EXT_SOURCE_1</th>\n",
              "      <th>EXT_SOURCE_2</th>\n",
              "      <th>EXT_SOURCE_3</th>\n",
              "      <th>APARTMENTS_AVG</th>\n",
              "      <th>BASEMENTAREA_AVG</th>\n",
              "      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n",
              "      <th>YEARS_BUILD_AVG</th>\n",
              "      <th>COMMONAREA_AVG</th>\n",
              "      <th>ELEVATORS_AVG</th>\n",
              "      <th>ENTRANCES_AVG</th>\n",
              "      <th>FLOORSMAX_AVG</th>\n",
              "      <th>FLOORSMIN_AVG</th>\n",
              "      <th>LANDAREA_AVG</th>\n",
              "      <th>LIVINGAPARTMENTS_AVG</th>\n",
              "      <th>LIVINGAREA_AVG</th>\n",
              "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
              "      <th>NONLIVINGAREA_AVG</th>\n",
              "      <th>APARTMENTS_MODE</th>\n",
              "      <th>BASEMENTAREA_MODE</th>\n",
              "      <th>YEARS_BEGINEXPLUATATION_MODE</th>\n",
              "      <th>YEARS_BUILD_MODE</th>\n",
              "      <th>COMMONAREA_MODE</th>\n",
              "      <th>ELEVATORS_MODE</th>\n",
              "      <th>ENTRANCES_MODE</th>\n",
              "      <th>FLOORSMAX_MODE</th>\n",
              "      <th>FLOORSMIN_MODE</th>\n",
              "      <th>LANDAREA_MODE</th>\n",
              "      <th>LIVINGAPARTMENTS_MODE</th>\n",
              "      <th>LIVINGAREA_MODE</th>\n",
              "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
              "      <th>NONLIVINGAREA_MODE</th>\n",
              "      <th>APARTMENTS_MEDI</th>\n",
              "      <th>BASEMENTAREA_MEDI</th>\n",
              "      <th>YEARS_BEGINEXPLUATATION_MEDI</th>\n",
              "      <th>YEARS_BUILD_MEDI</th>\n",
              "      <th>COMMONAREA_MEDI</th>\n",
              "      <th>ELEVATORS_MEDI</th>\n",
              "      <th>ENTRANCES_MEDI</th>\n",
              "      <th>FLOORSMAX_MEDI</th>\n",
              "      <th>FLOORSMIN_MEDI</th>\n",
              "      <th>LANDAREA_MEDI</th>\n",
              "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
              "      <th>LIVINGAREA_MEDI</th>\n",
              "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
              "      <th>NONLIVINGAREA_MEDI</th>\n",
              "      <th>FONDKAPREMONT_MODE</th>\n",
              "      <th>HOUSETYPE_MODE</th>\n",
              "      <th>TOTALAREA_MODE</th>\n",
              "      <th>WALLSMATERIAL_MODE</th>\n",
              "      <th>EMERGENCYSTATE_MODE</th>\n",
              "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
              "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
              "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
              "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
              "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
              "      <th>FLAG_DOCUMENT_2</th>\n",
              "      <th>FLAG_DOCUMENT_3</th>\n",
              "      <th>FLAG_DOCUMENT_4</th>\n",
              "      <th>FLAG_DOCUMENT_5</th>\n",
              "      <th>FLAG_DOCUMENT_6</th>\n",
              "      <th>FLAG_DOCUMENT_7</th>\n",
              "      <th>FLAG_DOCUMENT_8</th>\n",
              "      <th>FLAG_DOCUMENT_9</th>\n",
              "      <th>FLAG_DOCUMENT_10</th>\n",
              "      <th>FLAG_DOCUMENT_11</th>\n",
              "      <th>FLAG_DOCUMENT_12</th>\n",
              "      <th>FLAG_DOCUMENT_13</th>\n",
              "      <th>FLAG_DOCUMENT_14</th>\n",
              "      <th>FLAG_DOCUMENT_15</th>\n",
              "      <th>FLAG_DOCUMENT_16</th>\n",
              "      <th>FLAG_DOCUMENT_17</th>\n",
              "      <th>FLAG_DOCUMENT_18</th>\n",
              "      <th>FLAG_DOCUMENT_19</th>\n",
              "      <th>FLAG_DOCUMENT_20</th>\n",
              "      <th>FLAG_DOCUMENT_21</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CODE_GENDER</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>487.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23223.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72973.0</td>\n",
              "      <td>54747.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55389.0</td>\n",
              "      <td>52001.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>805.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73166.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137320.0</td>\n",
              "      <td>99549.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100951.0</td>\n",
              "      <td>93753.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             TARGET  NAME_CONTRACT_TYPE  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
              "CODE_GENDER                                                              \n",
              "M               0.0                 0.0           0.0              0.0   \n",
              "F               0.0                 0.0           0.0              0.0   \n",
              "\n",
              "             CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
              "CODE_GENDER                                                            \n",
              "M                     0.0               0.0         0.0          0.0   \n",
              "F                     0.0               0.0         0.0          0.0   \n",
              "\n",
              "             AMT_GOODS_PRICE  NAME_TYPE_SUITE  NAME_INCOME_TYPE  \\\n",
              "CODE_GENDER                                                       \n",
              "M                        0.0            487.0               0.0   \n",
              "F                        0.0            805.0               0.0   \n",
              "\n",
              "             NAME_EDUCATION_TYPE  NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
              "CODE_GENDER                                                               \n",
              "M                            0.0                 0.0                0.0   \n",
              "F                            0.0                 0.0                0.0   \n",
              "\n",
              "             REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
              "CODE_GENDER                                                          \n",
              "M                                   0.0         0.0            0.0   \n",
              "F                                   0.0         0.0            0.0   \n",
              "\n",
              "             DAYS_REGISTRATION  DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  \\\n",
              "CODE_GENDER                                                                \n",
              "M                          0.0              0.0          0.0         0.0   \n",
              "F                          0.0              0.0          0.0         0.0   \n",
              "\n",
              "             FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_CONT_MOBILE  FLAG_PHONE  \\\n",
              "CODE_GENDER                                                                  \n",
              "M                       0.0              0.0               0.0         0.0   \n",
              "F                       0.0              0.0               0.0         0.0   \n",
              "\n",
              "             FLAG_EMAIL  OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
              "CODE_GENDER                                                 \n",
              "M                   0.0          23223.0              0.0   \n",
              "F                   0.0          73166.0              0.0   \n",
              "\n",
              "             REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
              "CODE_GENDER                                                      \n",
              "M                             0.0                          0.0   \n",
              "F                             0.0                          0.0   \n",
              "\n",
              "             WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
              "CODE_GENDER                                                        \n",
              "M                                   0.0                      0.0   \n",
              "F                                   0.0                      0.0   \n",
              "\n",
              "             REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
              "CODE_GENDER                                                           \n",
              "M                                   0.0                         0.0   \n",
              "F                                   0.0                         0.0   \n",
              "\n",
              "             LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
              "CODE_GENDER                                                        \n",
              "M                                    0.0                     0.0   \n",
              "F                                    0.0                     0.0   \n",
              "\n",
              "             REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \\\n",
              "CODE_GENDER                                                    \n",
              "M                               0.0                      0.0   \n",
              "F                               0.0                      0.0   \n",
              "\n",
              "             ORGANIZATION_TYPE  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
              "CODE_GENDER                                                                \n",
              "M                          0.0           0.0           0.0           0.0   \n",
              "F                          0.0           0.0           0.0           0.0   \n",
              "\n",
              "             APARTMENTS_AVG  BASEMENTAREA_AVG  YEARS_BEGINEXPLUATATION_AVG  \\\n",
              "CODE_GENDER                                                                  \n",
              "M                       0.0               0.0                          0.0   \n",
              "F                       0.0               0.0                          0.0   \n",
              "\n",
              "             YEARS_BUILD_AVG  COMMONAREA_AVG  ELEVATORS_AVG  ENTRANCES_AVG  \\\n",
              "CODE_GENDER                                                                  \n",
              "M                        0.0             0.0            0.0            0.0   \n",
              "F                        0.0             0.0            0.0            0.0   \n",
              "\n",
              "             FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  LIVINGAPARTMENTS_AVG  \\\n",
              "CODE_GENDER                                                                     \n",
              "M                      0.0            0.0           0.0                   0.0   \n",
              "F                      0.0            0.0           0.0                   0.0   \n",
              "\n",
              "             LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  NONLIVINGAREA_AVG  \\\n",
              "CODE_GENDER                                                               \n",
              "M                       0.0                      0.0                0.0   \n",
              "F                       0.0                      0.0                0.0   \n",
              "\n",
              "             APARTMENTS_MODE  BASEMENTAREA_MODE  YEARS_BEGINEXPLUATATION_MODE  \\\n",
              "CODE_GENDER                                                                     \n",
              "M                        0.0                0.0                           0.0   \n",
              "F                        0.0                0.0                           0.0   \n",
              "\n",
              "             YEARS_BUILD_MODE  COMMONAREA_MODE  ELEVATORS_MODE  \\\n",
              "CODE_GENDER                                                      \n",
              "M                         0.0              0.0             0.0   \n",
              "F                         0.0              0.0             0.0   \n",
              "\n",
              "             ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  LANDAREA_MODE  \\\n",
              "CODE_GENDER                                                                  \n",
              "M                       0.0             0.0             0.0            0.0   \n",
              "F                       0.0             0.0             0.0            0.0   \n",
              "\n",
              "             LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  NONLIVINGAPARTMENTS_MODE  \\\n",
              "CODE_GENDER                                                                     \n",
              "M                              0.0              0.0                       0.0   \n",
              "F                              0.0              0.0                       0.0   \n",
              "\n",
              "             NONLIVINGAREA_MODE  APARTMENTS_MEDI  BASEMENTAREA_MEDI  \\\n",
              "CODE_GENDER                                                           \n",
              "M                           0.0              0.0                0.0   \n",
              "F                           0.0              0.0                0.0   \n",
              "\n",
              "             YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  COMMONAREA_MEDI  \\\n",
              "CODE_GENDER                                                                    \n",
              "M                                     0.0               0.0              0.0   \n",
              "F                                     0.0               0.0              0.0   \n",
              "\n",
              "             ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  FLOORSMIN_MEDI  \\\n",
              "CODE_GENDER                                                                   \n",
              "M                       0.0             0.0             0.0             0.0   \n",
              "F                       0.0             0.0             0.0             0.0   \n",
              "\n",
              "             LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  \\\n",
              "CODE_GENDER                                                          \n",
              "M                      0.0                    0.0              0.0   \n",
              "F                      0.0                    0.0              0.0   \n",
              "\n",
              "             NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI  FONDKAPREMONT_MODE  \\\n",
              "CODE_GENDER                                                                     \n",
              "M                                 0.0                 0.0             72973.0   \n",
              "F                                 0.0                 0.0            137320.0   \n",
              "\n",
              "             HOUSETYPE_MODE  TOTALAREA_MODE  WALLSMATERIAL_MODE  \\\n",
              "CODE_GENDER                                                       \n",
              "M                   54747.0             0.0             55389.0   \n",
              "F                   99549.0             0.0            100951.0   \n",
              "\n",
              "             EMERGENCYSTATE_MODE  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
              "CODE_GENDER                                                  \n",
              "M                        52001.0                       0.0   \n",
              "F                        93753.0                       0.0   \n",
              "\n",
              "             DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
              "CODE_GENDER                                                       \n",
              "M                                 0.0                       0.0   \n",
              "F                                 0.0                       0.0   \n",
              "\n",
              "             DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  \\\n",
              "CODE_GENDER                                                     \n",
              "M                                 0.0                     0.0   \n",
              "F                                 0.0                     0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  \\\n",
              "CODE_GENDER                                                      \n",
              "M                        0.0              0.0              0.0   \n",
              "F                        0.0              0.0              0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  \\\n",
              "CODE_GENDER                                                      \n",
              "M                        0.0              0.0              0.0   \n",
              "F                        0.0              0.0              0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  FLAG_DOCUMENT_10  \\\n",
              "CODE_GENDER                                                       \n",
              "M                        0.0              0.0               0.0   \n",
              "F                        0.0              0.0               0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  \\\n",
              "CODE_GENDER                                                         \n",
              "M                         0.0               0.0               0.0   \n",
              "F                         0.0               0.0               0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  \\\n",
              "CODE_GENDER                                                         \n",
              "M                         0.0               0.0               0.0   \n",
              "F                         0.0               0.0               0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  \\\n",
              "CODE_GENDER                                                         \n",
              "M                         0.0               0.0               0.0   \n",
              "F                         0.0               0.0               0.0   \n",
              "\n",
              "             FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
              "CODE_GENDER                                                                   \n",
              "M                         0.0               0.0                         0.0   \n",
              "F                         0.0               0.0                         0.0   \n",
              "\n",
              "             AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
              "CODE_GENDER                                                          \n",
              "M                                  0.0                         0.0   \n",
              "F                                  0.0                         0.0   \n",
              "\n",
              "             AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
              "CODE_GENDER                                                         \n",
              "M                                  0.0                        0.0   \n",
              "F                                  0.0                        0.0   \n",
              "\n",
              "             AMT_REQ_CREDIT_BUREAU_YEAR  \n",
              "CODE_GENDER                              \n",
              "M                                   0.0  \n",
              "F                                   0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7m1tuRXnxha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "21b6ddc9-9805-4b12-b1a2-7cc1578549a3"
      },
      "source": [
        "# Analyze class balance for dataset with only unprivileged and privileged dataset\n",
        "is_unpriv = df_application[\"CODE_GENDER\"].isin([\"F\"])\n",
        "is_priv = df_application[\"CODE_GENDER\"].isin([\"M\"])\n",
        "df_unpriv_application = df_application[is_unpriv]\n",
        "df_priv_application = df_application[is_priv]\n",
        "\n",
        "df_unpriv_priv_complete_application = pd.concat([df_unpriv_application, df_priv_application])\n",
        "\n",
        "print(df_unpriv_priv_complete_application[\"TARGET\"].value_counts(dropna=False))\n",
        "print(df_unpriv_priv_complete_application[\"TARGET\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    282682\n",
            "0     24825\n",
            "Name: TARGET, dtype: int64\n",
            "1    0.91927\n",
            "0    0.08073\n",
            "Name: TARGET, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wh0GXBzEsM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(df_application, disc_feature= \"CODE_GENDER\", disc_min_value=\"F\", label=\"TARGET\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frEeeyOcdeia",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbfAc6dAdmp5",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjSg39N2FKik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_application_train_input, df_application_train_label = fair_preprocess(data = df_application, \n",
        "                                                                        label = \"TARGET\", \n",
        "                                                                        neg_class = 0, \n",
        "                                                                        pos_class = 1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsdydnKbG3DU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WwtFde3EXef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attempt to resolve data leakage\n",
        "\n",
        "model_application = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "df_application_train_input_dummy_test = pd.get_dummies(df_application_train_input)\n",
        "X = df_application_train_input_dummy_test\n",
        "Y = df_application_train_label\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Results\n",
        "results = model_selection.cross_val_score(model_application, X, Y, cv=kfold, scoring = f1)\n",
        "print(results)\n",
        "print(results.std())\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMcD8TCeILZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_application, \n",
        "                    title = \"Home Credit Default Risk Dataset - Performance Learning Curve\", \n",
        "                    X = df_application_train_input, y = df_application_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIaa-Iwodmg9",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsCtMQYF5LE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "132888f4-ac67-40db-92e7-7c570394d9cd"
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "                \"n_estimators\": [60],\n",
        "                \"max_depth\": [3], \n",
        "                \"min_child_weight\": [4],       \n",
        "                \"reg_lambda\": [1, 1.2, 1.4]}\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=3\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_application_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= recall,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_application_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 60, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxo5ErlDd1sE",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFW3Y7yiQ_1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5c37a3af-4066-4cbd-feab-b2f42d5c5082"
      },
      "source": [
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_application[\"CODE_GENDER\"].isin([\"F\"])\n",
        "is_priv = df_application[\"CODE_GENDER\"].isin([\"M\"])\n",
        "df_application_unpriv = df_application[is_unpriv]  # Minority\n",
        "df_application_priv = df_application[is_priv]  # Majority\n",
        "\n",
        "list_dfs_application = create_datasets(min_data = df_application_unpriv, maj_data = df_application_priv, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "[105064, 105069, 105079, 105089, 105099, 105109, 105134, 105159, 105184, 105209, 105234, 105259, 105309, 105359, 105409, 105459, 105509, 105559, 105659, 105759, 105859, 105959, 106059, 106309, 106559, 106809, 107059, 107309, 107559, 107809, 108059, 108309, 108559, 108809, 109059, 109309, 109559, 109809, 110059, 111059, 112059, 113059, 114059, 115059, 120059]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdMwRVu6gMuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8244716c-8011-4791-8cf4-9464fae943ef"
      },
      "source": [
        "print(len(df_application_unpriv))\n",
        "print(len(df_application_priv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "202448\n",
            "105059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogKyufJNeArd",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77EkDXXQRG2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "7dcb7132-bc3c-42a5-8b2a-22d960048142"
      },
      "source": [
        "list_dfs = list_dfs_application\n",
        "label = \"TARGET\"\n",
        "cv = 3 \n",
        "discr_feature = \"CODE_GENDER\"\n",
        "min_value = \"F\"\n",
        "maj_value = \"M\"\n",
        "application_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                                  colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                                  learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                                  min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
        "                                  nthread=4, objective='binary:logistic', random_state=0,\n",
        "                                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                                  silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "results_df_application = metrics_to_df(list_dfs=list_dfs, label = label, model = application_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:219: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in long_scalars\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrb_-JsWz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_df_application"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vpO_uoIW7Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_application.to_csv(\"df_application_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_application_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsugkMieWV6",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOdv-Uy4VuT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart_selection(metric_df = results_df_credit, title=\"Fairness and Performance Metrics for the Home Credit Default Risk Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6BW1hQRQXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "bbeadc27-0aeb-412c-cc52-d2badda5a45f"
      },
      "source": [
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "application_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                                  colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                                  learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "                                  min_child_weight=4, missing=None, n_estimators=60, n_jobs=1,\n",
        "                                  nthread=4, objective='binary:logistic', random_state=0,\n",
        "                                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                                  silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# Dummy Coding\n",
        "df_application_train_input = pd.get_dummies(df_application_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = application_model, \n",
        "                    title = \"Home Credit Default Risk Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_application_train_input, y = df_application_train_label, \n",
        "                    cv = 3, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C2H-qopRWLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_application)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp3ZnNGmRbK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_application) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i07ZtFVpOwcv"
      },
      "source": [
        "# Generalizable Rule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wse3LOyiH_2",
        "colab_type": "text"
      },
      "source": [
        "**Approach**:\n",
        "1. Rename columns of each metrics_df to a format in which the name of the original dataset is stored. \n",
        "2. Merge data frames with metrics (column bind) on minority row index via left join. Start with the dataframe with the most rows_minority and then merge step by step with the df with second most, etc.\n",
        "3. Determine slop change per row in comparison to previous row. \n",
        "4. Then plot only relevant fairness metrics. \n",
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/).\n",
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSgVqfuZiHRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Rename columns of each results_df\n",
        "\n",
        "# results_df_adult\n",
        "results_df_adult.columns = [str(col) + '_adult' for col in results_df_adult.columns]\n",
        "# Rename rows_minority_adult column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_adult\":\"rows_minority\"})\n",
        "\n",
        "# results_df_compas\n",
        "results_df_compas.columns = [str(col) + '_compas' for col in results_df_compas.columns]\n",
        "# Rename rows_minority_compas column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_compas\":\"rows_minority\"})\n",
        "\n",
        "# results_df_homicide\n",
        "results_df_homicide.columns = [str(col) + '_homicide' for col in results_df_homicide.columns]\n",
        "# Rename rows_minority_homicide column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_homicide\":\"rows_minority\"})\n",
        "\n",
        "# results_df_credit\n",
        "results_df_credit.columns = [str(col) + '_credit' for col in results_df_credit.columns]\n",
        "# Rename rows_minority_credit column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_credit\":\"rows_minority\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUm1gEXwMjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Left join all result dfs into one\n",
        "merge_1 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "merge_2 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "complete_results_df = pd.merge(data1, data2, \n",
        "                               how='left', on='X1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6vRFnW3ebk",
        "colab_type": "text"
      },
      "source": [
        "Determine slop change per row in comparison to previous row: \n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
        "\n",
        "\n",
        "**Ideas:**\n",
        "- Get diff for rows_minority and the metric of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sLcz2OqzLcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Determine slope change \n",
        "\n",
        "# Calculate slope (rise(y-axis change)/run(x-axis change))\n",
        "\n",
        "# Run\n",
        "complete_results_df[\"rows_minority_diff\"] = complete_results_df[\"rows_minority\"].diff()\n",
        "\n",
        "# Rise\n",
        "# Calculate slopes for aver_abs_odds_diff\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_diff\"] = complete_results_df[\"aver_abs_odds_diff_adult\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_slope\"] = complete_results_df[\"aver_abs_odds_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_diff\"] = complete_results_df[\"aver_abs_odds_diff_compas\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_slope\"] = complete_results_df[\"aver_abs_odds_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] = complete_results_df[\"aver_abs_odds_diff_homicide\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_slope\"] = complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_diff\"] = complete_results_df[\"aver_abs_odds_diff_credit\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_slope\"] = complete_results_df[\"aver_abs_odds_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for stat_parity_diff\n",
        "complete_results_df[\"stat_parity_diff_adult_diff\"] = complete_results_df[\"stat_parity_diff_adult\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_adult_slope\"] = complete_results_df[\"stat_parity_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_compas_diff\"] = complete_results_df[\"stat_parity_diff_compas\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_compas_slope\"] = complete_results_df[\"stat_parity_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_homicide_diff\"] = complete_results_df[\"stat_parity_diff_homicide\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_homicide_slope\"] = complete_results_df[\"stat_parity_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_credit_diff\"] = complete_results_df[\"stat_parity_diff_credit\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_credit_slope\"] = complete_results_df[\"stat_parity_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for equal_opport_dist\n",
        "complete_results_df[\"equal_opport_dist_adult_diff\"] = complete_results_df[\"equal_opport_dist_adult\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_adult_slope\"] = complete_results_df[\"equal_opport_dist_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_compas_diff\"] = complete_results_df[\"equal_opport_dist_compas\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_compas_slope\"] = complete_results_df[\"equal_opport_dist_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_homicide_diff\"] = complete_results_df[\"equal_opport_dist_homicide\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_homicide_slope\"] = complete_results_df[\"equal_opport_dist_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_credit_diff\"] = complete_results_df[\"equal_opport_dist_credit\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_credit_slope\"] = complete_results_df[\"equal_opport_dist_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for disparate_impact\n",
        "complete_results_df[\"disparate_impact_adult_diff\"] = complete_results_df[\"disparate_impact_adult\"].diff()\n",
        "complete_results_df[\"disparate_impact_adult_slope\"] = complete_results_df[\"disparate_impact_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_compas_diff\"] = complete_results_df[\"disparate_impact_compas\"].diff()\n",
        "complete_results_df[\"disparate_impact_compas_slope\"] = complete_results_df[\"disparate_impact_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_homicide_diff\"] = complete_results_df[\"disparate_impact_homicide\"].diff()\n",
        "complete_results_df[\"disparate_impact_homicide_slope\"] = complete_results_df[\"disparate_impact_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_credit_diff\"] = complete_results_df[\"disparate_impact_credit\"].diff()\n",
        "complete_results_df[\"disparate_impact_credit_slope\"] = complete_results_df[\"disparate_impact_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CB0JtaoKaJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDFjNB0FJTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. Plot all fairness metrics \n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJOmyc6VFswT",
        "colab_type": "text"
      },
      "source": [
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0jr3FtCFfRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add shapes\n",
        "fig.add_shape(\n",
        "        # Line Vertical\n",
        "        dict(\n",
        "            type=\"line\",\n",
        "            x0=1,\n",
        "            y0=0,\n",
        "            x1=1,\n",
        "            y1=2,\n",
        "            line=dict(\n",
        "                color=\"RoyalBlue\",\n",
        "                width=3, \n",
        "                dash=\"dot\",\n",
        "                opacity=0.7\n",
        "            )\n",
        "\n",
        "fig.add_shape(\n",
        "        # Line reference to the axes\n",
        "            type=\"line\",\n",
        "            xref=\"x\",\n",
        "            yref=\"y\",\n",
        "            x0=4,\n",
        "            y0=0,\n",
        "            x1=8,\n",
        "            y1=1,\n",
        "            line=dict(\n",
        "                color=\"LightSeaGreen\",\n",
        "                width=3,\n",
        "            ),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxH0zA8eGNB8",
        "colab_type": "text"
      },
      "source": [
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM-X6rYBGQZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "# Add shape regions\n",
        "fig.update_layout(\n",
        "    shapes=[\n",
        "        # 1st highlight during Feb 4 - Feb 6\n",
        "        dict(\n",
        "            type=\"rect\",\n",
        "            # x-reference is assigned to the x-values\n",
        "            xref=\"x\",\n",
        "            # y-reference is assigned to the plot paper [0,1]\n",
        "            yref=\"paper\",\n",
        "            x0=\"2015-02-04\", # First value of corridor\n",
        "            y0=0,\n",
        "            x1=\"2015-02-06\", # Second value of corridor \n",
        "            y1=1,\n",
        "            fillcolor=\"LightSalmon\",\n",
        "            opacity=0.5,\n",
        "            layer=\"below\",\n",
        "            line_width=0,\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC806v-lnXrg",
        "colab_type": "text"
      },
      "source": [
        "# Other Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SAZPLhwuUD",
        "colab_type": "text"
      },
      "source": [
        "## Communities and Crime Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRiwaHW2XE43",
        "colab_type": "text"
      },
      "source": [
        "The Communities and Crime dataset gathers information from different communities in the United States related to several factors that can highly influence some common crimes such as robberies, murders or rapes. The data includes crime data obtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains socio-economic data from the 1990 US Census."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v2ws3jO_65w",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RypNWRj_D8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "37e0d43d-2e6d-4d41-95ec-458884c994c8"
      },
      "source": [
        "path_communities = \"/content/drive/My Drive/Master Thesis/Data/communities_and_crime_dataset.csv\"\n",
        "df_communities = pd.read_csv(path_communities, encoding = \"utf-8\")\n",
        "df_communities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>communityname</th>\n",
              "      <th>state</th>\n",
              "      <th>countyCode</th>\n",
              "      <th>communityCode</th>\n",
              "      <th>fold</th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>PctOccupManu</th>\n",
              "      <th>PctOccupMgmtProf</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>FemalePctDiv</th>\n",
              "      <th>TotalPctDiv</th>\n",
              "      <th>PersPerFam</th>\n",
              "      <th>PctFam2Par</th>\n",
              "      <th>PctKids2Par</th>\n",
              "      <th>PctYoungKids2Par</th>\n",
              "      <th>PctTeen2Par</th>\n",
              "      <th>PctWorkMomYoungKids</th>\n",
              "      <th>PctWorkMom</th>\n",
              "      <th>NumKidsBornNeverMar</th>\n",
              "      <th>PctKidsBornNeverMar</th>\n",
              "      <th>NumImmig</th>\n",
              "      <th>PctImmigRecent</th>\n",
              "      <th>PctImmigRec5</th>\n",
              "      <th>PctImmigRec8</th>\n",
              "      <th>PctImmigRec10</th>\n",
              "      <th>PctRecentImmig</th>\n",
              "      <th>PctRecImmig5</th>\n",
              "      <th>PctRecImmig8</th>\n",
              "      <th>PctRecImmig10</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>OwnOccQrange</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>RentQrange</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LemasSwornFT</th>\n",
              "      <th>LemasSwFTPerPop</th>\n",
              "      <th>LemasSwFTFieldOps</th>\n",
              "      <th>LemasSwFTFieldPerPop</th>\n",
              "      <th>LemasTotalReq</th>\n",
              "      <th>LemasTotReqPerPop</th>\n",
              "      <th>PolicReqPerOffic</th>\n",
              "      <th>PolicPerPop</th>\n",
              "      <th>RacialMatchCommPol</th>\n",
              "      <th>PctPolicWhite</th>\n",
              "      <th>PctPolicBlack</th>\n",
              "      <th>PctPolicHisp</th>\n",
              "      <th>PctPolicAsian</th>\n",
              "      <th>PctPolicMinor</th>\n",
              "      <th>OfficAssgnDrugUnits</th>\n",
              "      <th>NumKindsDrugsSeiz</th>\n",
              "      <th>PolicAveOTWorked</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "      <th>PolicCars</th>\n",
              "      <th>PolicOperBudg</th>\n",
              "      <th>LemasPctPolicOnPatr</th>\n",
              "      <th>LemasGangUnitDeploy</th>\n",
              "      <th>LemasPctOfficDrugUn</th>\n",
              "      <th>PolicBudgPerPop</th>\n",
              "      <th>murders</th>\n",
              "      <th>murdPerPop</th>\n",
              "      <th>rapes</th>\n",
              "      <th>rapesPerPop</th>\n",
              "      <th>robberies</th>\n",
              "      <th>robbbPerPop</th>\n",
              "      <th>assaults</th>\n",
              "      <th>assaultPerPop</th>\n",
              "      <th>burglaries</th>\n",
              "      <th>burglPerPop</th>\n",
              "      <th>larcenies</th>\n",
              "      <th>larcPerPop</th>\n",
              "      <th>autoTheft</th>\n",
              "      <th>autoTheftPerPop</th>\n",
              "      <th>arsons</th>\n",
              "      <th>arsonsPerPop</th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "      <th>nonViolPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BerkeleyHeightstownship</td>\n",
              "      <td>NJ</td>\n",
              "      <td>39</td>\n",
              "      <td>5320</td>\n",
              "      <td>1</td>\n",
              "      <td>11980</td>\n",
              "      <td>3.10</td>\n",
              "      <td>1.37</td>\n",
              "      <td>91.78</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1.88</td>\n",
              "      <td>12.47</td>\n",
              "      <td>21.44</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.33</td>\n",
              "      <td>11980</td>\n",
              "      <td>100.00</td>\n",
              "      <td>75122</td>\n",
              "      <td>89.24</td>\n",
              "      <td>1.55</td>\n",
              "      <td>70.20</td>\n",
              "      <td>23.62</td>\n",
              "      <td>1.03</td>\n",
              "      <td>18.39</td>\n",
              "      <td>79584</td>\n",
              "      <td>29711</td>\n",
              "      <td>30233</td>\n",
              "      <td>13600</td>\n",
              "      <td>5725</td>\n",
              "      <td>27101</td>\n",
              "      <td>5115</td>\n",
              "      <td>22838</td>\n",
              "      <td>227</td>\n",
              "      <td>1.96</td>\n",
              "      <td>5.81</td>\n",
              "      <td>9.90</td>\n",
              "      <td>48.18</td>\n",
              "      <td>2.70</td>\n",
              "      <td>64.55</td>\n",
              "      <td>14.65</td>\n",
              "      <td>28.82</td>\n",
              "      <td>5.49</td>\n",
              "      <td>50.73</td>\n",
              "      <td>3.67</td>\n",
              "      <td>26.38</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.47</td>\n",
              "      <td>3.22</td>\n",
              "      <td>91.43</td>\n",
              "      <td>90.17</td>\n",
              "      <td>95.78</td>\n",
              "      <td>95.81</td>\n",
              "      <td>44.56</td>\n",
              "      <td>58.88</td>\n",
              "      <td>31</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1277</td>\n",
              "      <td>8.69</td>\n",
              "      <td>13.00</td>\n",
              "      <td>20.99</td>\n",
              "      <td>30.93</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.24</td>\n",
              "      <td>3.30</td>\n",
              "      <td>85.68</td>\n",
              "      <td>1.37</td>\n",
              "      <td>4.81</td>\n",
              "      <td>4.17</td>\n",
              "      <td>2.99</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.84</td>\n",
              "      <td>91.46</td>\n",
              "      <td>0.39</td>\n",
              "      <td>11.06</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>98.37</td>\n",
              "      <td>91.01</td>\n",
              "      <td>3.12</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1959</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.28</td>\n",
              "      <td>215900</td>\n",
              "      <td>262600</td>\n",
              "      <td>326900</td>\n",
              "      <td>111000</td>\n",
              "      <td>685</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>316</td>\n",
              "      <td>1001</td>\n",
              "      <td>23.8</td>\n",
              "      <td>21.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>10.66</td>\n",
              "      <td>53.72</td>\n",
              "      <td>65.29</td>\n",
              "      <td>78.09</td>\n",
              "      <td>89.14</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.5</td>\n",
              "      <td>1845.9</td>\n",
              "      <td>9.63</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>4</td>\n",
              "      <td>32.81</td>\n",
              "      <td>14</td>\n",
              "      <td>114.85</td>\n",
              "      <td>138</td>\n",
              "      <td>1132.08</td>\n",
              "      <td>16</td>\n",
              "      <td>131.26</td>\n",
              "      <td>2</td>\n",
              "      <td>16.41</td>\n",
              "      <td>41.02</td>\n",
              "      <td>1394.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Marpletownship</td>\n",
              "      <td>PA</td>\n",
              "      <td>45</td>\n",
              "      <td>47616</td>\n",
              "      <td>1</td>\n",
              "      <td>23123</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.80</td>\n",
              "      <td>95.57</td>\n",
              "      <td>3.44</td>\n",
              "      <td>0.85</td>\n",
              "      <td>11.01</td>\n",
              "      <td>21.30</td>\n",
              "      <td>10.48</td>\n",
              "      <td>17.18</td>\n",
              "      <td>23123</td>\n",
              "      <td>100.00</td>\n",
              "      <td>47917</td>\n",
              "      <td>78.99</td>\n",
              "      <td>1.11</td>\n",
              "      <td>64.11</td>\n",
              "      <td>35.50</td>\n",
              "      <td>2.75</td>\n",
              "      <td>22.85</td>\n",
              "      <td>55323</td>\n",
              "      <td>20148</td>\n",
              "      <td>20191</td>\n",
              "      <td>18137</td>\n",
              "      <td>0</td>\n",
              "      <td>20074</td>\n",
              "      <td>5250</td>\n",
              "      <td>12222</td>\n",
              "      <td>885</td>\n",
              "      <td>3.98</td>\n",
              "      <td>5.61</td>\n",
              "      <td>13.72</td>\n",
              "      <td>29.89</td>\n",
              "      <td>2.43</td>\n",
              "      <td>61.96</td>\n",
              "      <td>12.26</td>\n",
              "      <td>29.28</td>\n",
              "      <td>6.39</td>\n",
              "      <td>37.64</td>\n",
              "      <td>4.23</td>\n",
              "      <td>27.99</td>\n",
              "      <td>6.45</td>\n",
              "      <td>5.42</td>\n",
              "      <td>3.11</td>\n",
              "      <td>86.91</td>\n",
              "      <td>85.33</td>\n",
              "      <td>96.82</td>\n",
              "      <td>86.46</td>\n",
              "      <td>51.14</td>\n",
              "      <td>62.43</td>\n",
              "      <td>43</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1920</td>\n",
              "      <td>5.21</td>\n",
              "      <td>8.65</td>\n",
              "      <td>13.33</td>\n",
              "      <td>22.50</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.87</td>\n",
              "      <td>87.79</td>\n",
              "      <td>1.81</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.34</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.83</td>\n",
              "      <td>1.96</td>\n",
              "      <td>89.03</td>\n",
              "      <td>1.01</td>\n",
              "      <td>23.60</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>97.15</td>\n",
              "      <td>84.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>18.33</td>\n",
              "      <td>1958</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.14</td>\n",
              "      <td>136300</td>\n",
              "      <td>164200</td>\n",
              "      <td>199900</td>\n",
              "      <td>63600</td>\n",
              "      <td>467</td>\n",
              "      <td>560</td>\n",
              "      <td>672</td>\n",
              "      <td>205</td>\n",
              "      <td>627</td>\n",
              "      <td>27.6</td>\n",
              "      <td>20.7</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.30</td>\n",
              "      <td>77.17</td>\n",
              "      <td>71.27</td>\n",
              "      <td>90.22</td>\n",
              "      <td>96.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2186.7</td>\n",
              "      <td>3.84</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>5</td>\n",
              "      <td>21.26</td>\n",
              "      <td>24</td>\n",
              "      <td>102.05</td>\n",
              "      <td>57</td>\n",
              "      <td>242.37</td>\n",
              "      <td>376</td>\n",
              "      <td>1598.78</td>\n",
              "      <td>26</td>\n",
              "      <td>110.55</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>127.56</td>\n",
              "      <td>1955.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tigardcity</td>\n",
              "      <td>OR</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>29344</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.74</td>\n",
              "      <td>94.33</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.35</td>\n",
              "      <td>11.36</td>\n",
              "      <td>25.88</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.28</td>\n",
              "      <td>29344</td>\n",
              "      <td>100.00</td>\n",
              "      <td>35669</td>\n",
              "      <td>82.00</td>\n",
              "      <td>1.15</td>\n",
              "      <td>55.73</td>\n",
              "      <td>22.25</td>\n",
              "      <td>2.94</td>\n",
              "      <td>14.56</td>\n",
              "      <td>42112</td>\n",
              "      <td>16946</td>\n",
              "      <td>17103</td>\n",
              "      <td>16644</td>\n",
              "      <td>21606</td>\n",
              "      <td>15528</td>\n",
              "      <td>5954</td>\n",
              "      <td>8405</td>\n",
              "      <td>1389</td>\n",
              "      <td>4.75</td>\n",
              "      <td>2.80</td>\n",
              "      <td>9.09</td>\n",
              "      <td>30.13</td>\n",
              "      <td>4.01</td>\n",
              "      <td>69.80</td>\n",
              "      <td>15.95</td>\n",
              "      <td>21.52</td>\n",
              "      <td>8.79</td>\n",
              "      <td>32.48</td>\n",
              "      <td>10.10</td>\n",
              "      <td>25.78</td>\n",
              "      <td>14.76</td>\n",
              "      <td>12.55</td>\n",
              "      <td>2.95</td>\n",
              "      <td>78.54</td>\n",
              "      <td>78.85</td>\n",
              "      <td>92.37</td>\n",
              "      <td>75.72</td>\n",
              "      <td>66.08</td>\n",
              "      <td>74.19</td>\n",
              "      <td>164</td>\n",
              "      <td>0.88</td>\n",
              "      <td>1468</td>\n",
              "      <td>16.42</td>\n",
              "      <td>23.98</td>\n",
              "      <td>32.08</td>\n",
              "      <td>35.63</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.78</td>\n",
              "      <td>93.11</td>\n",
              "      <td>1.14</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.42</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.06</td>\n",
              "      <td>64.18</td>\n",
              "      <td>2.03</td>\n",
              "      <td>47.46</td>\n",
              "      <td>3</td>\n",
              "      <td>544</td>\n",
              "      <td>95.68</td>\n",
              "      <td>57.79</td>\n",
              "      <td>0.92</td>\n",
              "      <td>7.54</td>\n",
              "      <td>1976</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.12</td>\n",
              "      <td>74700</td>\n",
              "      <td>90400</td>\n",
              "      <td>112000</td>\n",
              "      <td>37300</td>\n",
              "      <td>370</td>\n",
              "      <td>428</td>\n",
              "      <td>520</td>\n",
              "      <td>150</td>\n",
              "      <td>484</td>\n",
              "      <td>24.1</td>\n",
              "      <td>21.7</td>\n",
              "      <td>11.6</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>44.77</td>\n",
              "      <td>36.60</td>\n",
              "      <td>61.26</td>\n",
              "      <td>82.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2780.9</td>\n",
              "      <td>4.37</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>3</td>\n",
              "      <td>8.30</td>\n",
              "      <td>6</td>\n",
              "      <td>16.6</td>\n",
              "      <td>56</td>\n",
              "      <td>154.95</td>\n",
              "      <td>14</td>\n",
              "      <td>38.74</td>\n",
              "      <td>274</td>\n",
              "      <td>758.14</td>\n",
              "      <td>1797</td>\n",
              "      <td>4972.19</td>\n",
              "      <td>136</td>\n",
              "      <td>376.3</td>\n",
              "      <td>22</td>\n",
              "      <td>60.87</td>\n",
              "      <td>218.59</td>\n",
              "      <td>6167.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gloversvillecity</td>\n",
              "      <td>NY</td>\n",
              "      <td>35</td>\n",
              "      <td>29443</td>\n",
              "      <td>1</td>\n",
              "      <td>16656</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1.70</td>\n",
              "      <td>97.35</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.70</td>\n",
              "      <td>12.55</td>\n",
              "      <td>25.20</td>\n",
              "      <td>12.19</td>\n",
              "      <td>17.57</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20580</td>\n",
              "      <td>68.15</td>\n",
              "      <td>0.24</td>\n",
              "      <td>38.95</td>\n",
              "      <td>39.48</td>\n",
              "      <td>11.71</td>\n",
              "      <td>18.33</td>\n",
              "      <td>26501</td>\n",
              "      <td>10810</td>\n",
              "      <td>10909</td>\n",
              "      <td>9984</td>\n",
              "      <td>4941</td>\n",
              "      <td>3541</td>\n",
              "      <td>2451</td>\n",
              "      <td>4391</td>\n",
              "      <td>2831</td>\n",
              "      <td>17.23</td>\n",
              "      <td>11.05</td>\n",
              "      <td>33.68</td>\n",
              "      <td>10.81</td>\n",
              "      <td>9.86</td>\n",
              "      <td>54.74</td>\n",
              "      <td>31.22</td>\n",
              "      <td>27.43</td>\n",
              "      <td>26.76</td>\n",
              "      <td>22.71</td>\n",
              "      <td>10.98</td>\n",
              "      <td>28.15</td>\n",
              "      <td>14.47</td>\n",
              "      <td>12.91</td>\n",
              "      <td>2.98</td>\n",
              "      <td>64.02</td>\n",
              "      <td>62.36</td>\n",
              "      <td>65.38</td>\n",
              "      <td>67.43</td>\n",
              "      <td>59.59</td>\n",
              "      <td>70.27</td>\n",
              "      <td>561</td>\n",
              "      <td>3.84</td>\n",
              "      <td>339</td>\n",
              "      <td>13.86</td>\n",
              "      <td>13.86</td>\n",
              "      <td>15.34</td>\n",
              "      <td>15.34</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>94.98</td>\n",
              "      <td>0.56</td>\n",
              "      <td>3.93</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.51</td>\n",
              "      <td>2.20</td>\n",
              "      <td>58.18</td>\n",
              "      <td>1.21</td>\n",
              "      <td>45.66</td>\n",
              "      <td>3</td>\n",
              "      <td>669</td>\n",
              "      <td>91.19</td>\n",
              "      <td>54.89</td>\n",
              "      <td>2.54</td>\n",
              "      <td>57.85</td>\n",
              "      <td>1939</td>\n",
              "      <td>7.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>36400</td>\n",
              "      <td>49600</td>\n",
              "      <td>66500</td>\n",
              "      <td>30100</td>\n",
              "      <td>195</td>\n",
              "      <td>250</td>\n",
              "      <td>309</td>\n",
              "      <td>114</td>\n",
              "      <td>333</td>\n",
              "      <td>28.7</td>\n",
              "      <td>20.6</td>\n",
              "      <td>14.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.04</td>\n",
              "      <td>88.71</td>\n",
              "      <td>56.70</td>\n",
              "      <td>90.17</td>\n",
              "      <td>96.24</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>5.2</td>\n",
              "      <td>3217.7</td>\n",
              "      <td>3.31</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>33</td>\n",
              "      <td>190.93</td>\n",
              "      <td>225</td>\n",
              "      <td>1301.78</td>\n",
              "      <td>716</td>\n",
              "      <td>4142.56</td>\n",
              "      <td>47</td>\n",
              "      <td>271.93</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>306.64</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bemidjicity</td>\n",
              "      <td>MN</td>\n",
              "      <td>7</td>\n",
              "      <td>5068</td>\n",
              "      <td>1</td>\n",
              "      <td>11245</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.53</td>\n",
              "      <td>89.16</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.52</td>\n",
              "      <td>24.46</td>\n",
              "      <td>40.53</td>\n",
              "      <td>28.69</td>\n",
              "      <td>12.65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>17390</td>\n",
              "      <td>69.33</td>\n",
              "      <td>0.55</td>\n",
              "      <td>42.82</td>\n",
              "      <td>32.16</td>\n",
              "      <td>11.21</td>\n",
              "      <td>14.43</td>\n",
              "      <td>24018</td>\n",
              "      <td>8483</td>\n",
              "      <td>9009</td>\n",
              "      <td>887</td>\n",
              "      <td>4425</td>\n",
              "      <td>3352</td>\n",
              "      <td>3000</td>\n",
              "      <td>1328</td>\n",
              "      <td>2855</td>\n",
              "      <td>29.99</td>\n",
              "      <td>12.15</td>\n",
              "      <td>23.06</td>\n",
              "      <td>25.28</td>\n",
              "      <td>9.08</td>\n",
              "      <td>52.44</td>\n",
              "      <td>6.89</td>\n",
              "      <td>36.54</td>\n",
              "      <td>10.94</td>\n",
              "      <td>27.80</td>\n",
              "      <td>7.51</td>\n",
              "      <td>50.66</td>\n",
              "      <td>11.64</td>\n",
              "      <td>9.73</td>\n",
              "      <td>2.98</td>\n",
              "      <td>58.59</td>\n",
              "      <td>55.20</td>\n",
              "      <td>66.51</td>\n",
              "      <td>79.17</td>\n",
              "      <td>61.22</td>\n",
              "      <td>68.94</td>\n",
              "      <td>402</td>\n",
              "      <td>4.70</td>\n",
              "      <td>196</td>\n",
              "      <td>46.94</td>\n",
              "      <td>56.12</td>\n",
              "      <td>67.86</td>\n",
              "      <td>69.90</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.22</td>\n",
              "      <td>94.64</td>\n",
              "      <td>0.39</td>\n",
              "      <td>5.23</td>\n",
              "      <td>3.11</td>\n",
              "      <td>2.35</td>\n",
              "      <td>2.55</td>\n",
              "      <td>2.12</td>\n",
              "      <td>58.13</td>\n",
              "      <td>2.94</td>\n",
              "      <td>55.64</td>\n",
              "      <td>2</td>\n",
              "      <td>333</td>\n",
              "      <td>92.45</td>\n",
              "      <td>53.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>42.64</td>\n",
              "      <td>1958</td>\n",
              "      <td>7.45</td>\n",
              "      <td>0.82</td>\n",
              "      <td>30600</td>\n",
              "      <td>43200</td>\n",
              "      <td>59500</td>\n",
              "      <td>28900</td>\n",
              "      <td>202</td>\n",
              "      <td>283</td>\n",
              "      <td>362</td>\n",
              "      <td>160</td>\n",
              "      <td>332</td>\n",
              "      <td>32.2</td>\n",
              "      <td>23.2</td>\n",
              "      <td>12.9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.74</td>\n",
              "      <td>73.75</td>\n",
              "      <td>42.22</td>\n",
              "      <td>60.34</td>\n",
              "      <td>89.02</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>11.5</td>\n",
              "      <td>974.2</td>\n",
              "      <td>0.38</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>4</td>\n",
              "      <td>32.04</td>\n",
              "      <td>14</td>\n",
              "      <td>112.14</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>1060</td>\n",
              "      <td>8490.87</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>5</td>\n",
              "      <td>40.05</td>\n",
              "      <td>?</td>\n",
              "      <td>9988.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2210</th>\n",
              "      <td>Mercedcity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>56216</td>\n",
              "      <td>3.07</td>\n",
              "      <td>6.87</td>\n",
              "      <td>61.68</td>\n",
              "      <td>15.23</td>\n",
              "      <td>29.86</td>\n",
              "      <td>15.46</td>\n",
              "      <td>30.16</td>\n",
              "      <td>14.34</td>\n",
              "      <td>8.08</td>\n",
              "      <td>56216</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24727</td>\n",
              "      <td>75.05</td>\n",
              "      <td>1.12</td>\n",
              "      <td>31.42</td>\n",
              "      <td>21.45</td>\n",
              "      <td>19.98</td>\n",
              "      <td>14.41</td>\n",
              "      <td>27388</td>\n",
              "      <td>10237</td>\n",
              "      <td>13041</td>\n",
              "      <td>8344</td>\n",
              "      <td>8590</td>\n",
              "      <td>3399</td>\n",
              "      <td>6470</td>\n",
              "      <td>6644</td>\n",
              "      <td>13804</td>\n",
              "      <td>25.06</td>\n",
              "      <td>17.12</td>\n",
              "      <td>30.87</td>\n",
              "      <td>15.79</td>\n",
              "      <td>9.99</td>\n",
              "      <td>55.53</td>\n",
              "      <td>13.47</td>\n",
              "      <td>27.18</td>\n",
              "      <td>16.38</td>\n",
              "      <td>25.02</td>\n",
              "      <td>10.22</td>\n",
              "      <td>31.91</td>\n",
              "      <td>16.28</td>\n",
              "      <td>13.34</td>\n",
              "      <td>3.56</td>\n",
              "      <td>67.04</td>\n",
              "      <td>64.81</td>\n",
              "      <td>76.19</td>\n",
              "      <td>72.78</td>\n",
              "      <td>47.24</td>\n",
              "      <td>55.38</td>\n",
              "      <td>1960</td>\n",
              "      <td>4.49</td>\n",
              "      <td>10623</td>\n",
              "      <td>22.97</td>\n",
              "      <td>35.12</td>\n",
              "      <td>42.10</td>\n",
              "      <td>60.31</td>\n",
              "      <td>4.34</td>\n",
              "      <td>6.64</td>\n",
              "      <td>7.96</td>\n",
              "      <td>11.40</td>\n",
              "      <td>65.33</td>\n",
              "      <td>11.87</td>\n",
              "      <td>13.49</td>\n",
              "      <td>9.91</td>\n",
              "      <td>3.03</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.19</td>\n",
              "      <td>41.69</td>\n",
              "      <td>16.89</td>\n",
              "      <td>57.23</td>\n",
              "      <td>2</td>\n",
              "      <td>683</td>\n",
              "      <td>96.40</td>\n",
              "      <td>44.63</td>\n",
              "      <td>1.46</td>\n",
              "      <td>13.18</td>\n",
              "      <td>1973</td>\n",
              "      <td>4.91</td>\n",
              "      <td>0.55</td>\n",
              "      <td>71200</td>\n",
              "      <td>91100</td>\n",
              "      <td>118900</td>\n",
              "      <td>47700</td>\n",
              "      <td>298</td>\n",
              "      <td>374</td>\n",
              "      <td>455</td>\n",
              "      <td>157</td>\n",
              "      <td>438</td>\n",
              "      <td>29.8</td>\n",
              "      <td>22.6</td>\n",
              "      <td>11.7</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>18.90</td>\n",
              "      <td>52.67</td>\n",
              "      <td>39.19</td>\n",
              "      <td>74.58</td>\n",
              "      <td>85.88</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>16.7</td>\n",
              "      <td>3365.4</td>\n",
              "      <td>0.59</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>16.49</td>\n",
              "      <td>30</td>\n",
              "      <td>49.46</td>\n",
              "      <td>121</td>\n",
              "      <td>199.5</td>\n",
              "      <td>170</td>\n",
              "      <td>280.29</td>\n",
              "      <td>1376</td>\n",
              "      <td>2268.72</td>\n",
              "      <td>2563</td>\n",
              "      <td>4225.82</td>\n",
              "      <td>489</td>\n",
              "      <td>806.25</td>\n",
              "      <td>34</td>\n",
              "      <td>56.06</td>\n",
              "      <td>545.75</td>\n",
              "      <td>7356.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2211</th>\n",
              "      <td>Pinevillecity</td>\n",
              "      <td>LA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>12251</td>\n",
              "      <td>2.68</td>\n",
              "      <td>21.18</td>\n",
              "      <td>76.65</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.29</td>\n",
              "      <td>17.36</td>\n",
              "      <td>31.23</td>\n",
              "      <td>16.97</td>\n",
              "      <td>12.57</td>\n",
              "      <td>12251</td>\n",
              "      <td>100.00</td>\n",
              "      <td>20321</td>\n",
              "      <td>75.06</td>\n",
              "      <td>0.47</td>\n",
              "      <td>33.25</td>\n",
              "      <td>27.63</td>\n",
              "      <td>8.85</td>\n",
              "      <td>18.23</td>\n",
              "      <td>25000</td>\n",
              "      <td>9995</td>\n",
              "      <td>11353</td>\n",
              "      <td>5768</td>\n",
              "      <td>10910</td>\n",
              "      <td>1718</td>\n",
              "      <td>11471</td>\n",
              "      <td>4883</td>\n",
              "      <td>2364</td>\n",
              "      <td>20.79</td>\n",
              "      <td>12.51</td>\n",
              "      <td>27.71</td>\n",
              "      <td>19.28</td>\n",
              "      <td>7.90</td>\n",
              "      <td>54.64</td>\n",
              "      <td>7.81</td>\n",
              "      <td>37.06</td>\n",
              "      <td>10.37</td>\n",
              "      <td>28.73</td>\n",
              "      <td>10.86</td>\n",
              "      <td>29.51</td>\n",
              "      <td>16.12</td>\n",
              "      <td>13.77</td>\n",
              "      <td>3.12</td>\n",
              "      <td>68.57</td>\n",
              "      <td>63.66</td>\n",
              "      <td>80.29</td>\n",
              "      <td>73.68</td>\n",
              "      <td>64.20</td>\n",
              "      <td>66.67</td>\n",
              "      <td>277</td>\n",
              "      <td>2.98</td>\n",
              "      <td>275</td>\n",
              "      <td>2.91</td>\n",
              "      <td>45.09</td>\n",
              "      <td>65.45</td>\n",
              "      <td>75.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.70</td>\n",
              "      <td>92.78</td>\n",
              "      <td>0.86</td>\n",
              "      <td>5.03</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.49</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.39</td>\n",
              "      <td>56.06</td>\n",
              "      <td>3.99</td>\n",
              "      <td>54.48</td>\n",
              "      <td>2</td>\n",
              "      <td>523</td>\n",
              "      <td>89.72</td>\n",
              "      <td>54.24</td>\n",
              "      <td>4.59</td>\n",
              "      <td>46.08</td>\n",
              "      <td>1966</td>\n",
              "      <td>7.56</td>\n",
              "      <td>0.12</td>\n",
              "      <td>33600</td>\n",
              "      <td>52000</td>\n",
              "      <td>72700</td>\n",
              "      <td>39100</td>\n",
              "      <td>176</td>\n",
              "      <td>248</td>\n",
              "      <td>297</td>\n",
              "      <td>121</td>\n",
              "      <td>330</td>\n",
              "      <td>23.8</td>\n",
              "      <td>17.3</td>\n",
              "      <td>14.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.24</td>\n",
              "      <td>75.16</td>\n",
              "      <td>49.12</td>\n",
              "      <td>78.79</td>\n",
              "      <td>92.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>7.3</td>\n",
              "      <td>1682.8</td>\n",
              "      <td>1.15</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4</td>\n",
              "      <td>33.09</td>\n",
              "      <td>1</td>\n",
              "      <td>8.27</td>\n",
              "      <td>10</td>\n",
              "      <td>82.73</td>\n",
              "      <td>104</td>\n",
              "      <td>860.43</td>\n",
              "      <td>574</td>\n",
              "      <td>4748.9</td>\n",
              "      <td>24</td>\n",
              "      <td>198.56</td>\n",
              "      <td>2</td>\n",
              "      <td>16.55</td>\n",
              "      <td>124.1</td>\n",
              "      <td>5824.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2212</th>\n",
              "      <td>Yucaipacity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>32824</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>92.62</td>\n",
              "      <td>0.98</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.81</td>\n",
              "      <td>20.96</td>\n",
              "      <td>9.53</td>\n",
              "      <td>20.73</td>\n",
              "      <td>32824</td>\n",
              "      <td>100.00</td>\n",
              "      <td>27182</td>\n",
              "      <td>59.79</td>\n",
              "      <td>0.51</td>\n",
              "      <td>44.72</td>\n",
              "      <td>43.40</td>\n",
              "      <td>9.01</td>\n",
              "      <td>23.56</td>\n",
              "      <td>34973</td>\n",
              "      <td>14131</td>\n",
              "      <td>14416</td>\n",
              "      <td>13630</td>\n",
              "      <td>13197</td>\n",
              "      <td>17313</td>\n",
              "      <td>8532</td>\n",
              "      <td>9398</td>\n",
              "      <td>2460</td>\n",
              "      <td>7.56</td>\n",
              "      <td>7.82</td>\n",
              "      <td>26.14</td>\n",
              "      <td>12.42</td>\n",
              "      <td>5.18</td>\n",
              "      <td>50.54</td>\n",
              "      <td>9.34</td>\n",
              "      <td>23.36</td>\n",
              "      <td>13.53</td>\n",
              "      <td>23.54</td>\n",
              "      <td>9.89</td>\n",
              "      <td>20.56</td>\n",
              "      <td>12.38</td>\n",
              "      <td>11.23</td>\n",
              "      <td>2.99</td>\n",
              "      <td>76.77</td>\n",
              "      <td>74.20</td>\n",
              "      <td>76.92</td>\n",
              "      <td>82.42</td>\n",
              "      <td>55.73</td>\n",
              "      <td>62.62</td>\n",
              "      <td>434</td>\n",
              "      <td>1.60</td>\n",
              "      <td>2414</td>\n",
              "      <td>6.63</td>\n",
              "      <td>9.03</td>\n",
              "      <td>17.15</td>\n",
              "      <td>26.72</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.66</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.97</td>\n",
              "      <td>88.95</td>\n",
              "      <td>1.70</td>\n",
              "      <td>5.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.67</td>\n",
              "      <td>74.61</td>\n",
              "      <td>4.39</td>\n",
              "      <td>61.03</td>\n",
              "      <td>2</td>\n",
              "      <td>957</td>\n",
              "      <td>93.30</td>\n",
              "      <td>76.81</td>\n",
              "      <td>0.84</td>\n",
              "      <td>29.47</td>\n",
              "      <td>1967</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.27</td>\n",
              "      <td>91700</td>\n",
              "      <td>123900</td>\n",
              "      <td>164000</td>\n",
              "      <td>72300</td>\n",
              "      <td>347</td>\n",
              "      <td>451</td>\n",
              "      <td>551</td>\n",
              "      <td>204</td>\n",
              "      <td>514</td>\n",
              "      <td>30.5</td>\n",
              "      <td>23.9</td>\n",
              "      <td>13.1</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>7.35</td>\n",
              "      <td>48.66</td>\n",
              "      <td>46.73</td>\n",
              "      <td>75.54</td>\n",
              "      <td>92.30</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>27.5</td>\n",
              "      <td>1195.2</td>\n",
              "      <td>0.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>24</td>\n",
              "      <td>65.32</td>\n",
              "      <td>96</td>\n",
              "      <td>261.29</td>\n",
              "      <td>628</td>\n",
              "      <td>1709.26</td>\n",
              "      <td>895</td>\n",
              "      <td>2435.97</td>\n",
              "      <td>179</td>\n",
              "      <td>487.19</td>\n",
              "      <td>8</td>\n",
              "      <td>21.77</td>\n",
              "      <td>353.83</td>\n",
              "      <td>4654.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>Beevillecity</td>\n",
              "      <td>TX</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>13547</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.37</td>\n",
              "      <td>69.91</td>\n",
              "      <td>0.90</td>\n",
              "      <td>62.11</td>\n",
              "      <td>17.16</td>\n",
              "      <td>30.01</td>\n",
              "      <td>14.73</td>\n",
              "      <td>10.42</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19899</td>\n",
              "      <td>71.67</td>\n",
              "      <td>1.70</td>\n",
              "      <td>21.94</td>\n",
              "      <td>26.44</td>\n",
              "      <td>13.05</td>\n",
              "      <td>10.29</td>\n",
              "      <td>22103</td>\n",
              "      <td>8100</td>\n",
              "      <td>9555</td>\n",
              "      <td>6437</td>\n",
              "      <td>8271</td>\n",
              "      <td>171</td>\n",
              "      <td>4436</td>\n",
              "      <td>5338</td>\n",
              "      <td>4021</td>\n",
              "      <td>30.32</td>\n",
              "      <td>24.37</td>\n",
              "      <td>39.63</td>\n",
              "      <td>12.40</td>\n",
              "      <td>12.12</td>\n",
              "      <td>52.53</td>\n",
              "      <td>8.22</td>\n",
              "      <td>25.16</td>\n",
              "      <td>10.63</td>\n",
              "      <td>20.87</td>\n",
              "      <td>10.35</td>\n",
              "      <td>29.18</td>\n",
              "      <td>14.36</td>\n",
              "      <td>12.48</td>\n",
              "      <td>3.46</td>\n",
              "      <td>67.76</td>\n",
              "      <td>63.45</td>\n",
              "      <td>87.82</td>\n",
              "      <td>74.12</td>\n",
              "      <td>50.57</td>\n",
              "      <td>60.14</td>\n",
              "      <td>279</td>\n",
              "      <td>2.35</td>\n",
              "      <td>309</td>\n",
              "      <td>4.85</td>\n",
              "      <td>8.09</td>\n",
              "      <td>11.00</td>\n",
              "      <td>20.71</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.47</td>\n",
              "      <td>48.92</td>\n",
              "      <td>6.66</td>\n",
              "      <td>9.83</td>\n",
              "      <td>7.10</td>\n",
              "      <td>2.84</td>\n",
              "      <td>2.93</td>\n",
              "      <td>2.73</td>\n",
              "      <td>60.11</td>\n",
              "      <td>9.64</td>\n",
              "      <td>50.28</td>\n",
              "      <td>2</td>\n",
              "      <td>802</td>\n",
              "      <td>85.39</td>\n",
              "      <td>58.39</td>\n",
              "      <td>5.61</td>\n",
              "      <td>67.21</td>\n",
              "      <td>1964</td>\n",
              "      <td>15.91</td>\n",
              "      <td>0.87</td>\n",
              "      <td>26000</td>\n",
              "      <td>37800</td>\n",
              "      <td>52100</td>\n",
              "      <td>26100</td>\n",
              "      <td>135</td>\n",
              "      <td>227</td>\n",
              "      <td>317</td>\n",
              "      <td>182</td>\n",
              "      <td>316</td>\n",
              "      <td>26.2</td>\n",
              "      <td>23.3</td>\n",
              "      <td>14.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.28</td>\n",
              "      <td>82.26</td>\n",
              "      <td>54.05</td>\n",
              "      <td>79.72</td>\n",
              "      <td>94.06</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2142.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>15.71</td>\n",
              "      <td>7</td>\n",
              "      <td>54.98</td>\n",
              "      <td>79</td>\n",
              "      <td>620.48</td>\n",
              "      <td>192</td>\n",
              "      <td>1508.01</td>\n",
              "      <td>474</td>\n",
              "      <td>3722.9</td>\n",
              "      <td>13</td>\n",
              "      <td>102.1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.85</td>\n",
              "      <td>691.17</td>\n",
              "      <td>5340.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>WestSacramentocity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>28898</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.39</td>\n",
              "      <td>71.27</td>\n",
              "      <td>9.09</td>\n",
              "      <td>24.43</td>\n",
              "      <td>12.99</td>\n",
              "      <td>25.21</td>\n",
              "      <td>11.63</td>\n",
              "      <td>12.12</td>\n",
              "      <td>28664</td>\n",
              "      <td>99.19</td>\n",
              "      <td>23287</td>\n",
              "      <td>68.89</td>\n",
              "      <td>1.20</td>\n",
              "      <td>27.54</td>\n",
              "      <td>28.62</td>\n",
              "      <td>19.05</td>\n",
              "      <td>17.29</td>\n",
              "      <td>27897</td>\n",
              "      <td>11510</td>\n",
              "      <td>13074</td>\n",
              "      <td>8163</td>\n",
              "      <td>9874</td>\n",
              "      <td>6827</td>\n",
              "      <td>7540</td>\n",
              "      <td>8275</td>\n",
              "      <td>5287</td>\n",
              "      <td>18.50</td>\n",
              "      <td>13.93</td>\n",
              "      <td>33.68</td>\n",
              "      <td>8.86</td>\n",
              "      <td>9.27</td>\n",
              "      <td>53.35</td>\n",
              "      <td>9.44</td>\n",
              "      <td>17.75</td>\n",
              "      <td>19.61</td>\n",
              "      <td>17.44</td>\n",
              "      <td>15.77</td>\n",
              "      <td>29.72</td>\n",
              "      <td>18.84</td>\n",
              "      <td>17.31</td>\n",
              "      <td>3.20</td>\n",
              "      <td>62.64</td>\n",
              "      <td>60.23</td>\n",
              "      <td>72.43</td>\n",
              "      <td>66.15</td>\n",
              "      <td>50.44</td>\n",
              "      <td>57.32</td>\n",
              "      <td>1152</td>\n",
              "      <td>4.85</td>\n",
              "      <td>4765</td>\n",
              "      <td>31.54</td>\n",
              "      <td>39.50</td>\n",
              "      <td>46.32</td>\n",
              "      <td>55.97</td>\n",
              "      <td>5.20</td>\n",
              "      <td>6.51</td>\n",
              "      <td>7.64</td>\n",
              "      <td>9.23</td>\n",
              "      <td>74.98</td>\n",
              "      <td>7.76</td>\n",
              "      <td>8.58</td>\n",
              "      <td>5.70</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.76</td>\n",
              "      <td>52.72</td>\n",
              "      <td>11.31</td>\n",
              "      <td>61.63</td>\n",
              "      <td>2</td>\n",
              "      <td>600</td>\n",
              "      <td>94.85</td>\n",
              "      <td>55.68</td>\n",
              "      <td>3.67</td>\n",
              "      <td>28.67</td>\n",
              "      <td>1960</td>\n",
              "      <td>7.37</td>\n",
              "      <td>1.43</td>\n",
              "      <td>68400</td>\n",
              "      <td>87600</td>\n",
              "      <td>114900</td>\n",
              "      <td>46500</td>\n",
              "      <td>272</td>\n",
              "      <td>369</td>\n",
              "      <td>449</td>\n",
              "      <td>177</td>\n",
              "      <td>426</td>\n",
              "      <td>30.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>11.6</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>16.49</td>\n",
              "      <td>55.29</td>\n",
              "      <td>48.74</td>\n",
              "      <td>66.20</td>\n",
              "      <td>89.08</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>21.7</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>1.39</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>16.53</td>\n",
              "      <td>19</td>\n",
              "      <td>62.8</td>\n",
              "      <td>102</td>\n",
              "      <td>337.15</td>\n",
              "      <td>152</td>\n",
              "      <td>502.41</td>\n",
              "      <td>791</td>\n",
              "      <td>2614.53</td>\n",
              "      <td>1458</td>\n",
              "      <td>4819.2</td>\n",
              "      <td>405</td>\n",
              "      <td>1338.67</td>\n",
              "      <td>20</td>\n",
              "      <td>66.11</td>\n",
              "      <td>918.89</td>\n",
              "      <td>8838.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2215 rows × 147 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                communityname state countyCode communityCode  fold  \\\n",
              "0     BerkeleyHeightstownship    NJ         39          5320     1   \n",
              "1              Marpletownship    PA         45         47616     1   \n",
              "2                  Tigardcity    OR          ?             ?     1   \n",
              "3            Gloversvillecity    NY         35         29443     1   \n",
              "4                 Bemidjicity    MN          7          5068     1   \n",
              "...                       ...   ...        ...           ...   ...   \n",
              "2210               Mercedcity    CA          ?             ?    10   \n",
              "2211            Pinevillecity    LA          ?             ?    10   \n",
              "2212              Yucaipacity    CA          ?             ?    10   \n",
              "2213             Beevillecity    TX          ?             ?    10   \n",
              "2214       WestSacramentocity    CA          ?             ?    10   \n",
              "\n",
              "      population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
              "0          11980           3.10          1.37         91.78          6.50   \n",
              "1          23123           2.82          0.80         95.57          3.44   \n",
              "2          29344           2.43          0.74         94.33          3.43   \n",
              "3          16656           2.40          1.70         97.35          0.50   \n",
              "4          11245           2.76          0.53         89.16          1.17   \n",
              "...          ...            ...           ...           ...           ...   \n",
              "2210       56216           3.07          6.87         61.68         15.23   \n",
              "2211       12251           2.68         21.18         76.65          1.52   \n",
              "2212       32824           2.46          0.52         92.62          0.98   \n",
              "2213       13547           2.89          3.37         69.91          0.90   \n",
              "2214       28898           2.61          2.39         71.27          9.09   \n",
              "\n",
              "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
              "0            1.88        12.47        21.44        10.93       11.33   \n",
              "1            0.85        11.01        21.30        10.48       17.18   \n",
              "2            2.35        11.36        25.88        11.01       10.28   \n",
              "3            0.70        12.55        25.20        12.19       17.57   \n",
              "4            0.52        24.46        40.53        28.69       12.65   \n",
              "...           ...          ...          ...          ...         ...   \n",
              "2210        29.86        15.46        30.16        14.34        8.08   \n",
              "2211         1.29        17.36        31.23        16.97       12.57   \n",
              "2212        11.00        11.81        20.96         9.53       20.73   \n",
              "2213        62.11        17.16        30.01        14.73       10.42   \n",
              "2214        24.43        12.99        25.21        11.63       12.12   \n",
              "\n",
              "      numbUrban  pctUrban  medIncome  pctWWage  pctWFarmSelf  pctWInvInc  \\\n",
              "0         11980    100.00      75122     89.24          1.55       70.20   \n",
              "1         23123    100.00      47917     78.99          1.11       64.11   \n",
              "2         29344    100.00      35669     82.00          1.15       55.73   \n",
              "3             0      0.00      20580     68.15          0.24       38.95   \n",
              "4             0      0.00      17390     69.33          0.55       42.82   \n",
              "...         ...       ...        ...       ...           ...         ...   \n",
              "2210      56216    100.00      24727     75.05          1.12       31.42   \n",
              "2211      12251    100.00      20321     75.06          0.47       33.25   \n",
              "2212      32824    100.00      27182     59.79          0.51       44.72   \n",
              "2213          0      0.00      19899     71.67          1.70       21.94   \n",
              "2214      28664     99.19      23287     68.89          1.20       27.54   \n",
              "\n",
              "      pctWSocSec  pctWPubAsst  pctWRetire  medFamInc  perCapInc  whitePerCap  \\\n",
              "0          23.62         1.03       18.39      79584      29711        30233   \n",
              "1          35.50         2.75       22.85      55323      20148        20191   \n",
              "2          22.25         2.94       14.56      42112      16946        17103   \n",
              "3          39.48        11.71       18.33      26501      10810        10909   \n",
              "4          32.16        11.21       14.43      24018       8483         9009   \n",
              "...          ...          ...         ...        ...        ...          ...   \n",
              "2210       21.45        19.98       14.41      27388      10237        13041   \n",
              "2211       27.63         8.85       18.23      25000       9995        11353   \n",
              "2212       43.40         9.01       23.56      34973      14131        14416   \n",
              "2213       26.44        13.05       10.29      22103       8100         9555   \n",
              "2214       28.62        19.05       17.29      27897      11510        13074   \n",
              "\n",
              "      blackPerCap  indianPerCap  AsianPerCap OtherPerCap  HispPerCap  \\\n",
              "0           13600          5725        27101        5115       22838   \n",
              "1           18137             0        20074        5250       12222   \n",
              "2           16644         21606        15528        5954        8405   \n",
              "3            9984          4941         3541        2451        4391   \n",
              "4             887          4425         3352        3000        1328   \n",
              "...           ...           ...          ...         ...         ...   \n",
              "2210         8344          8590         3399        6470        6644   \n",
              "2211         5768         10910         1718       11471        4883   \n",
              "2212        13630         13197        17313        8532        9398   \n",
              "2213         6437          8271          171        4436        5338   \n",
              "2214         8163          9874         6827        7540        8275   \n",
              "\n",
              "      NumUnderPov  PctPopUnderPov  PctLess9thGrade  PctNotHSGrad  PctBSorMore  \\\n",
              "0             227            1.96             5.81          9.90        48.18   \n",
              "1             885            3.98             5.61         13.72        29.89   \n",
              "2            1389            4.75             2.80          9.09        30.13   \n",
              "3            2831           17.23            11.05         33.68        10.81   \n",
              "4            2855           29.99            12.15         23.06        25.28   \n",
              "...           ...             ...              ...           ...          ...   \n",
              "2210        13804           25.06            17.12         30.87        15.79   \n",
              "2211         2364           20.79            12.51         27.71        19.28   \n",
              "2212         2460            7.56             7.82         26.14        12.42   \n",
              "2213         4021           30.32            24.37         39.63        12.40   \n",
              "2214         5287           18.50            13.93         33.68         8.86   \n",
              "\n",
              "      PctUnemployed  PctEmploy  PctEmplManu  PctEmplProfServ  PctOccupManu  \\\n",
              "0              2.70      64.55        14.65            28.82          5.49   \n",
              "1              2.43      61.96        12.26            29.28          6.39   \n",
              "2              4.01      69.80        15.95            21.52          8.79   \n",
              "3              9.86      54.74        31.22            27.43         26.76   \n",
              "4              9.08      52.44         6.89            36.54         10.94   \n",
              "...             ...        ...          ...              ...           ...   \n",
              "2210           9.99      55.53        13.47            27.18         16.38   \n",
              "2211           7.90      54.64         7.81            37.06         10.37   \n",
              "2212           5.18      50.54         9.34            23.36         13.53   \n",
              "2213          12.12      52.53         8.22            25.16         10.63   \n",
              "2214           9.27      53.35         9.44            17.75         19.61   \n",
              "\n",
              "      PctOccupMgmtProf  MalePctDivorce  MalePctNevMarr  FemalePctDiv  \\\n",
              "0                50.73            3.67           26.38          5.22   \n",
              "1                37.64            4.23           27.99          6.45   \n",
              "2                32.48           10.10           25.78         14.76   \n",
              "3                22.71           10.98           28.15         14.47   \n",
              "4                27.80            7.51           50.66         11.64   \n",
              "...                ...             ...             ...           ...   \n",
              "2210             25.02           10.22           31.91         16.28   \n",
              "2211             28.73           10.86           29.51         16.12   \n",
              "2212             23.54            9.89           20.56         12.38   \n",
              "2213             20.87           10.35           29.18         14.36   \n",
              "2214             17.44           15.77           29.72         18.84   \n",
              "\n",
              "      TotalPctDiv  PersPerFam  PctFam2Par  PctKids2Par  PctYoungKids2Par  \\\n",
              "0            4.47        3.22       91.43        90.17             95.78   \n",
              "1            5.42        3.11       86.91        85.33             96.82   \n",
              "2           12.55        2.95       78.54        78.85             92.37   \n",
              "3           12.91        2.98       64.02        62.36             65.38   \n",
              "4            9.73        2.98       58.59        55.20             66.51   \n",
              "...           ...         ...         ...          ...               ...   \n",
              "2210        13.34        3.56       67.04        64.81             76.19   \n",
              "2211        13.77        3.12       68.57        63.66             80.29   \n",
              "2212        11.23        2.99       76.77        74.20             76.92   \n",
              "2213        12.48        3.46       67.76        63.45             87.82   \n",
              "2214        17.31        3.20       62.64        60.23             72.43   \n",
              "\n",
              "      PctTeen2Par  PctWorkMomYoungKids  PctWorkMom  NumKidsBornNeverMar  \\\n",
              "0           95.81                44.56       58.88                   31   \n",
              "1           86.46                51.14       62.43                   43   \n",
              "2           75.72                66.08       74.19                  164   \n",
              "3           67.43                59.59       70.27                  561   \n",
              "4           79.17                61.22       68.94                  402   \n",
              "...           ...                  ...         ...                  ...   \n",
              "2210        72.78                47.24       55.38                 1960   \n",
              "2211        73.68                64.20       66.67                  277   \n",
              "2212        82.42                55.73       62.62                  434   \n",
              "2213        74.12                50.57       60.14                  279   \n",
              "2214        66.15                50.44       57.32                 1152   \n",
              "\n",
              "      PctKidsBornNeverMar  NumImmig  PctImmigRecent  PctImmigRec5  \\\n",
              "0                    0.36      1277            8.69         13.00   \n",
              "1                    0.24      1920            5.21          8.65   \n",
              "2                    0.88      1468           16.42         23.98   \n",
              "3                    3.84       339           13.86         13.86   \n",
              "4                    4.70       196           46.94         56.12   \n",
              "...                   ...       ...             ...           ...   \n",
              "2210                 4.49     10623           22.97         35.12   \n",
              "2211                 2.98       275            2.91         45.09   \n",
              "2212                 1.60      2414            6.63          9.03   \n",
              "2213                 2.35       309            4.85          8.09   \n",
              "2214                 4.85      4765           31.54         39.50   \n",
              "\n",
              "      PctImmigRec8  PctImmigRec10  PctRecentImmig  PctRecImmig5  PctRecImmig8  \\\n",
              "0            20.99          30.93            0.93          1.39          2.24   \n",
              "1            13.33          22.50            0.43          0.72          1.11   \n",
              "2            32.08          35.63            0.82          1.20          1.61   \n",
              "3            15.34          15.34            0.28          0.28          0.31   \n",
              "4            67.86          69.90            0.82          0.98          1.18   \n",
              "...            ...            ...             ...           ...           ...   \n",
              "2210         42.10          60.31            4.34          6.64          7.96   \n",
              "2211         65.45          75.64            0.07          1.01          1.47   \n",
              "2212         17.15          26.72            0.49          0.66          1.26   \n",
              "2213         11.00          20.71            0.11          0.18          0.25   \n",
              "2214         46.32          55.97            5.20          6.51          7.64   \n",
              "\n",
              "      PctRecImmig10  PctSpeakEnglOnly  PctNotSpeakEnglWell  PctLargHouseFam  \\\n",
              "0              3.30             85.68                 1.37             4.81   \n",
              "1              1.87             87.79                 1.81             4.25   \n",
              "2              1.78             93.11                 1.14             2.97   \n",
              "3              0.31             94.98                 0.56             3.93   \n",
              "4              1.22             94.64                 0.39             5.23   \n",
              "...             ...               ...                  ...              ...   \n",
              "2210          11.40             65.33                11.87            13.49   \n",
              "2211           1.70             92.78                 0.86             5.03   \n",
              "2212           1.97             88.95                 1.70             5.10   \n",
              "2213           0.47             48.92                 6.66             9.83   \n",
              "2214           9.23             74.98                 7.76             8.58   \n",
              "\n",
              "      PctLargHouseOccup  PersPerOccupHous  PersPerOwnOccHous  \\\n",
              "0                  4.17              2.99               3.00   \n",
              "1                  3.34              2.70               2.83   \n",
              "2                  2.05              2.42               2.69   \n",
              "3                  2.56              2.37               2.51   \n",
              "4                  3.11              2.35               2.55   \n",
              "...                 ...               ...                ...   \n",
              "2210               9.91              3.03               2.83   \n",
              "2211               3.37              2.49               2.58   \n",
              "2212               3.50              2.44               2.37   \n",
              "2213               7.10              2.84               2.93   \n",
              "2214               5.70              2.58               2.45   \n",
              "\n",
              "      PersPerRentOccHous  PctPersOwnOccup  PctPersDenseHous  PctHousLess3BR  \\\n",
              "0                   2.84            91.46              0.39           11.06   \n",
              "1                   1.96            89.03              1.01           23.60   \n",
              "2                   2.06            64.18              2.03           47.46   \n",
              "3                   2.20            58.18              1.21           45.66   \n",
              "4                   2.12            58.13              2.94           55.64   \n",
              "...                  ...              ...               ...             ...   \n",
              "2210                3.19            41.69             16.89           57.23   \n",
              "2211                2.39            56.06              3.99           54.48   \n",
              "2212                2.67            74.61              4.39           61.03   \n",
              "2213                2.73            60.11              9.64           50.28   \n",
              "2214                2.76            52.72             11.31           61.63   \n",
              "\n",
              "      MedNumBR  HousVacant  PctHousOccup  PctHousOwnOcc  PctVacantBoarded  \\\n",
              "0            3          64         98.37          91.01              3.12   \n",
              "1            3         240         97.15          84.88              0.00   \n",
              "2            3         544         95.68          57.79              0.92   \n",
              "3            3         669         91.19          54.89              2.54   \n",
              "4            2         333         92.45          53.57              3.90   \n",
              "...        ...         ...           ...            ...               ...   \n",
              "2210         2         683         96.40          44.63              1.46   \n",
              "2211         2         523         89.72          54.24              4.59   \n",
              "2212         2         957         93.30          76.81              0.84   \n",
              "2213         2         802         85.39          58.39              5.61   \n",
              "2214         2         600         94.85          55.68              3.67   \n",
              "\n",
              "      PctVacMore6Mos  MedYrHousBuilt  PctHousNoPhone  PctWOFullPlumb  \\\n",
              "0              37.50            1959            0.00            0.28   \n",
              "1              18.33            1958            0.31            0.14   \n",
              "2               7.54            1976            1.55            0.12   \n",
              "3              57.85            1939            7.00            0.87   \n",
              "4              42.64            1958            7.45            0.82   \n",
              "...              ...             ...             ...             ...   \n",
              "2210           13.18            1973            4.91            0.55   \n",
              "2211           46.08            1966            7.56            0.12   \n",
              "2212           29.47            1967            2.51            0.27   \n",
              "2213           67.21            1964           15.91            0.87   \n",
              "2214           28.67            1960            7.37            1.43   \n",
              "\n",
              "      OwnOccLowQuart  OwnOccMedVal  OwnOccHiQuart  OwnOccQrange  RentLowQ  \\\n",
              "0             215900        262600         326900        111000       685   \n",
              "1             136300        164200         199900         63600       467   \n",
              "2              74700         90400         112000         37300       370   \n",
              "3              36400         49600          66500         30100       195   \n",
              "4              30600         43200          59500         28900       202   \n",
              "...              ...           ...            ...           ...       ...   \n",
              "2210           71200         91100         118900         47700       298   \n",
              "2211           33600         52000          72700         39100       176   \n",
              "2212           91700        123900         164000         72300       347   \n",
              "2213           26000         37800          52100         26100       135   \n",
              "2214           68400         87600         114900         46500       272   \n",
              "\n",
              "      RentMedian  RentHighQ  RentQrange  MedRent  MedRentPctHousInc  \\\n",
              "0           1001       1001         316     1001               23.8   \n",
              "1            560        672         205      627               27.6   \n",
              "2            428        520         150      484               24.1   \n",
              "3            250        309         114      333               28.7   \n",
              "4            283        362         160      332               32.2   \n",
              "...          ...        ...         ...      ...                ...   \n",
              "2210         374        455         157      438               29.8   \n",
              "2211         248        297         121      330               23.8   \n",
              "2212         451        551         204      514               30.5   \n",
              "2213         227        317         182      316               26.2   \n",
              "2214         369        449         177      426               30.9   \n",
              "\n",
              "      MedOwnCostPctInc  MedOwnCostPctIncNoMtg  NumInShelters  NumStreet  \\\n",
              "0                 21.1                   14.0             11          0   \n",
              "1                 20.7                   12.5              0          0   \n",
              "2                 21.7                   11.6             16          0   \n",
              "3                 20.6                   14.5              0          0   \n",
              "4                 23.2                   12.9              2          0   \n",
              "...                ...                    ...            ...        ...   \n",
              "2210              22.6                   11.7             64          0   \n",
              "2211              17.3                   14.4              0          0   \n",
              "2212              23.9                   13.1             44          0   \n",
              "2213              23.3                   14.1              0          0   \n",
              "2214              21.2                   11.6             10          2   \n",
              "\n",
              "      PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
              "0              10.66             53.72           65.29          78.09   \n",
              "1               8.30             77.17           71.27          90.22   \n",
              "2               5.00             44.77           36.60          61.26   \n",
              "3               2.04             88.71           56.70          90.17   \n",
              "4               1.74             73.75           42.22          60.34   \n",
              "...              ...               ...             ...            ...   \n",
              "2210           18.90             52.67           39.19          74.58   \n",
              "2211            2.24             75.16           49.12          78.79   \n",
              "2212            7.35             48.66           46.73          75.54   \n",
              "2213            2.28             82.26           54.05          79.72   \n",
              "2214           16.49             55.29           48.74          66.20   \n",
              "\n",
              "      PctSameState85 LemasSwornFT LemasSwFTPerPop LemasSwFTFieldOps  \\\n",
              "0              89.14            ?               ?                 ?   \n",
              "1              96.12            ?               ?                 ?   \n",
              "2              82.85            ?               ?                 ?   \n",
              "3              96.24            ?               ?                 ?   \n",
              "4              89.02            ?               ?                 ?   \n",
              "...              ...          ...             ...               ...   \n",
              "2210           85.88            ?               ?                 ?   \n",
              "2211           92.85            ?               ?                 ?   \n",
              "2212           92.30            ?               ?                 ?   \n",
              "2213           94.06            ?               ?                 ?   \n",
              "2214           89.08            ?               ?                 ?   \n",
              "\n",
              "     LemasSwFTFieldPerPop LemasTotalReq LemasTotReqPerPop PolicReqPerOffic  \\\n",
              "0                       ?             ?                 ?                ?   \n",
              "1                       ?             ?                 ?                ?   \n",
              "2                       ?             ?                 ?                ?   \n",
              "3                       ?             ?                 ?                ?   \n",
              "4                       ?             ?                 ?                ?   \n",
              "...                   ...           ...               ...              ...   \n",
              "2210                    ?             ?                 ?                ?   \n",
              "2211                    ?             ?                 ?                ?   \n",
              "2212                    ?             ?                 ?                ?   \n",
              "2213                    ?             ?                 ?                ?   \n",
              "2214                    ?             ?                 ?                ?   \n",
              "\n",
              "     PolicPerPop RacialMatchCommPol PctPolicWhite PctPolicBlack PctPolicHisp  \\\n",
              "0              ?                  ?             ?             ?            ?   \n",
              "1              ?                  ?             ?             ?            ?   \n",
              "2              ?                  ?             ?             ?            ?   \n",
              "3              ?                  ?             ?             ?            ?   \n",
              "4              ?                  ?             ?             ?            ?   \n",
              "...          ...                ...           ...           ...          ...   \n",
              "2210           ?                  ?             ?             ?            ?   \n",
              "2211           ?                  ?             ?             ?            ?   \n",
              "2212           ?                  ?             ?             ?            ?   \n",
              "2213           ?                  ?             ?             ?            ?   \n",
              "2214           ?                  ?             ?             ?            ?   \n",
              "\n",
              "     PctPolicAsian PctPolicMinor OfficAssgnDrugUnits NumKindsDrugsSeiz  \\\n",
              "0                ?             ?                   ?                 ?   \n",
              "1                ?             ?                   ?                 ?   \n",
              "2                ?             ?                   ?                 ?   \n",
              "3                ?             ?                   ?                 ?   \n",
              "4                ?             ?                   ?                 ?   \n",
              "...            ...           ...                 ...               ...   \n",
              "2210             ?             ?                   ?                 ?   \n",
              "2211             ?             ?                   ?                 ?   \n",
              "2212             ?             ?                   ?                 ?   \n",
              "2213             ?             ?                   ?                 ?   \n",
              "2214             ?             ?                   ?                 ?   \n",
              "\n",
              "     PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans PolicCars  \\\n",
              "0                   ?       6.5   1845.9            9.63         ?   \n",
              "1                   ?      10.6   2186.7            3.84         ?   \n",
              "2                   ?      10.6   2780.9            4.37         ?   \n",
              "3                   ?       5.2   3217.7            3.31         ?   \n",
              "4                   ?      11.5    974.2            0.38         ?   \n",
              "...               ...       ...      ...             ...       ...   \n",
              "2210                ?      16.7   3365.4            0.59         ?   \n",
              "2211                ?       7.3   1682.8            1.15         ?   \n",
              "2212                ?      27.5   1195.2            0.12         ?   \n",
              "2213                ?       6.3   2142.2            0.00         ?   \n",
              "2214                ?      21.7   1331.0            1.39         ?   \n",
              "\n",
              "     PolicOperBudg LemasPctPolicOnPatr LemasGangUnitDeploy  \\\n",
              "0                ?                   ?                   ?   \n",
              "1                ?                   ?                   ?   \n",
              "2                ?                   ?                   ?   \n",
              "3                ?                   ?                   ?   \n",
              "4                ?                   ?                   ?   \n",
              "...            ...                 ...                 ...   \n",
              "2210             ?                   ?                   ?   \n",
              "2211             ?                   ?                   ?   \n",
              "2212             ?                   ?                   ?   \n",
              "2213             ?                   ?                   ?   \n",
              "2214             ?                   ?                   ?   \n",
              "\n",
              "      LemasPctOfficDrugUn PolicBudgPerPop  murders  murdPerPop rapes  \\\n",
              "0                     0.0               ?        0        0.00     0   \n",
              "1                     0.0               ?        0        0.00     1   \n",
              "2                     0.0               ?        3        8.30     6   \n",
              "3                     0.0               ?        0        0.00    10   \n",
              "4                     0.0               ?        0        0.00     ?   \n",
              "...                   ...             ...      ...         ...   ...   \n",
              "2210                  0.0               ?       10       16.49    30   \n",
              "2211                  0.0               ?        0        0.00     4   \n",
              "2212                  0.0               ?        5       13.61     5   \n",
              "2213                  0.0               ?        0        0.00     2   \n",
              "2214                  0.0               ?        5       16.53    19   \n",
              "\n",
              "     rapesPerPop robberies robbbPerPop assaults assaultPerPop burglaries  \\\n",
              "0              0         1         8.2        4         32.81         14   \n",
              "1           4.25         5       21.26       24        102.05         57   \n",
              "2           16.6        56      154.95       14         38.74        274   \n",
              "3          57.86        10       57.86       33        190.93        225   \n",
              "4              ?         4       32.04       14        112.14         91   \n",
              "...          ...       ...         ...      ...           ...        ...   \n",
              "2210       49.46       121       199.5      170        280.29       1376   \n",
              "2211       33.09         1        8.27       10         82.73        104   \n",
              "2212       13.61        24       65.32       96        261.29        628   \n",
              "2213       15.71         7       54.98       79        620.48        192   \n",
              "2214        62.8       102      337.15      152        502.41        791   \n",
              "\n",
              "     burglPerPop larcenies larcPerPop autoTheft autoTheftPerPop arsons  \\\n",
              "0         114.85       138    1132.08        16          131.26      2   \n",
              "1         242.37       376    1598.78        26          110.55      1   \n",
              "2         758.14      1797    4972.19       136           376.3     22   \n",
              "3        1301.78       716    4142.56        47          271.93      ?   \n",
              "4         728.93      1060    8490.87        91          728.93      5   \n",
              "...          ...       ...        ...       ...             ...    ...   \n",
              "2210     2268.72      2563    4225.82       489          806.25     34   \n",
              "2211      860.43       574     4748.9        24          198.56      2   \n",
              "2212     1709.26       895    2435.97       179          487.19      8   \n",
              "2213     1508.01       474     3722.9        13           102.1      1   \n",
              "2214     2614.53      1458     4819.2       405         1338.67     20   \n",
              "\n",
              "     arsonsPerPop ViolentCrimesPerPop nonViolPerPop  \n",
              "0           16.41               41.02       1394.59  \n",
              "1            4.25              127.56       1955.95  \n",
              "2           60.87              218.59       6167.51  \n",
              "3               ?              306.64             ?  \n",
              "4           40.05                   ?       9988.79  \n",
              "...           ...                 ...           ...  \n",
              "2210        56.06              545.75       7356.84  \n",
              "2211        16.55               124.1       5824.44  \n",
              "2212        21.77              353.83        4654.2  \n",
              "2213         7.85              691.17       5340.87  \n",
              "2214        66.11              918.89        8838.5  \n",
              "\n",
              "[2215 rows x 147 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPgz9d88m7uk",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0o5hcj5R_Iv",
        "colab_type": "text"
      },
      "source": [
        "Drop Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAUHyexwSBCL",
        "colab_type": "text"
      },
      "source": [
        "Replace values with NaNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvejj-DXdB4x",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEHAb7dqE2wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_communities.info)\n",
        "print(df_communities.describe())\n",
        "print(df_communities.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rh9V3tHEqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = \"Risk\" # 1 = \"Good\"; 0 = \"Bad\"\n",
        "\n",
        "eda_descr_stats(df_communities, disc_feature=\"Sex\", disc_min_value=\"female\", label = \"Risk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8f4lMHddc9c",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff0bvUYDdk5i",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhjE6ELVFJFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd0EZIuxd4aO",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LXwjKNF33U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgdMZOueEaV",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E8VoqTFQ-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_white = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_black = df_compas[is_black]  # Minority\n",
        "df_compas_white = df_compas[is_white]  # Majority\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_black, maj_data = df_compas_white, training_sizes=training_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXtqNpMaeCVz",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2kvEbOCRFc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_dfs = list_dfs_compas\n",
        "label = \"two_year_recid\"\n",
        "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                              criterion='gini', max_depth=25, max_features='auto',\n",
        "                              max_leaf_nodes=None, max_samples=None,\n",
        "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                              min_samples_leaf=1, min_samples_split=5,\n",
        "                              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                              n_jobs=None, oob_score=False, random_state=None,\n",
        "                              verbose=0, warm_start=False)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "cv = 5 # To Do: For final calculations, change to cv = 10\n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = model, \n",
        "                                  cv = cv, discr_feature = \"race\", min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIcGDseeUin",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF3SKKuaRPGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Communities and Crime Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvuD9nZRU1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtBH_by9RaCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLR_jzFLgHY",
        "colab_type": "text"
      },
      "source": [
        "Ideas\n",
        "- Individual Metric dfs zusammenmergen\n",
        "- Rows Minority als key dafür nehmen\n",
        "- Auf Kernmetriken fokussieren, diese dann umbenennen\n",
        "- Kernmetriken von verschiedenen Datasets in zentralem Line Chart zeigen\n",
        "- Dann Korridor einzeichnen, wo die Knickpunkte sich jeweils befinden\n",
        "- Überlappungen von Korridoren zeigen, indem sich durch alpha die Farbe gegenseitig verstärkt\n",
        "- Dann Tabelle rechts daneben platzieren, die Zusatzinformationen bietet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc56QO8P4HOl",
        "colab_type": "text"
      },
      "source": [
        "## Juvenile Justice Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgHr8ugy2aWG",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glMI4qYt9mKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Master Thesis/Data/Others/reincidenciaJusticiaMenors_processed_deleted_features.csv\"\n",
        "df_juvenile = pd.read_csv(path, sep= \";\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2STi749_dgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_juvenile.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhTqeFTB_xvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_juvenile.head(n=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8rZ2Zro2fZd",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WN_46L9oZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_juvenile = df_juvenile.drop([\"id\"], axis=1)\n",
        "\n",
        "# 1 should be positive (= good consequence, predict to repay load), 0 should be negative (= bad consequence, predict not to repay the loan)\n",
        "df_juvenile[\"Recidivism\"] = df_juvenile[\"Recidivism\"].replace({\"Yes\": 0, \"No\": 1})\n",
        "\n",
        "# Replace XNA for CODE_GENDER by NaN\n",
        "df_juvenile = df_juvenile.replace(\"#NULL!\", np.nan, regex=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kigbc3LP2m0F",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcM75YD2tCk",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeHHhv5M2xq3",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP0jjw88-jHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_juvenile_train_input, df_juvenile_train_label = fair_preprocess(data = df_juvenile, \n",
        "                                                                 label = \"Recidivism\", \n",
        "                                                                 neg_class = 0, \n",
        "                                                                 pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_juvenile.groupby([\"Recidivism\"]).agg({\"Recidivism\": 'count'}))\n",
        "print(df_juvenile_train_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_s5Eoz24oA",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDJRHBemBHr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "bb994d18-49da-4753-aadd-0d316b2547c7"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.3],\n",
        "              \"n_estimators\": [7, 8, 9],\n",
        "              \"max_depth\": [1, 2], \n",
        "              \"min_child_weight\": [1],   \n",
        "              \"reg_lambda\": [1]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "\n",
        "cv=5\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Define model\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_juvenile_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= recall,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_juvenile_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.3, 'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 7, 'reg_lambda': 1}\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
            "              min_child_weight=1, missing=None, n_estimators=7, n_jobs=1,\n",
            "              nthread=4, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lQ-s5c_BKzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fcd372f1-229b-48d6-83f2-982260b05315"
      },
      "source": [
        "# Attempt to resolve data leakage\n",
        "\n",
        "model_juvenile = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=7, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "df_train_input_dummy_test = pd.get_dummies(df_juvenile_train_input)\n",
        "X = df_train_input_dummy_test\n",
        "Y = df_juvenile_train_label\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "f1 = make_scorer(f1_score, average=\"weighted\")\n",
        "\n",
        "# Results\n",
        "#results = model_selection.cross_val_score(model_homicide, X, Y, cv=kfold, scoring = f1)\n",
        "#print(results)\n",
        "#print(results.std())\n",
        "#print(results.mean())\n",
        "\n",
        "scores = cross_validate(model_juvenile, X, Y, cv=5,\n",
        "                        scoring=make_scorer(f1_score, average='weighted'),\n",
        "                        return_train_score=True)\n",
        "\n",
        "print(scores[\"test_score\"])\n",
        "print(scores[\"test_score\"].mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.70557799 0.57483156 0.6359813  0.60535811 0.5668552 ]\n",
            "0.6177208319068953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SCyd0hIuQQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot feature importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "label = \"Recidivism\"\n",
        "model_juvenile = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=7, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "df_train_input_dummy_test = pd.get_dummies(df_juvenile_train_input)\n",
        "X = df_train_input_dummy_test\n",
        "Y = df_juvenile_train_label\n",
        "\n",
        "model_juvenile.fit(df_train_input_dummy_test, df_juvenile_train_label)\n",
        "plot_importance(model_juvenile)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfEznEsj6FRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "56fd1f60-42f0-4d7b-f1c9-836c66f04714"
      },
      "source": [
        "len(df_train_input_dummy_test)-0.2*len(df_train_input_dummy_test)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3802.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEygfq3LEr3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "cd93495e-6010-47d2-ab88-d259852cc8b6"
      },
      "source": [
        ""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-1ea182d12d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_input_dummy_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_juvenile_train_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mylim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     train_sizes = training_sizes_def)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'recall' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7Nb41Y3B1r",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. unpriv. group sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79iW8Fe3KUrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cc42347d-5046-491b-bf85-99e0def95ece"
      },
      "source": [
        "# Create datasets with different minority group sizes \n",
        "\n",
        "training_sizes = [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_juvenile[\"Grouped_Nationality\"].isin([\"Maghreb\"])\n",
        "is_white = df_juvenile[\"Grouped_Nationality\"].isin([\"Spanish\"])\n",
        "df_juvenile_black = df_juvenile[is_black]  # Minority\n",
        "df_juvenile_white = df_juvenile[is_white]  # Majority\n",
        "\n",
        "list_dfs_juvenile = create_datasets(min_data = df_juvenile_black, maj_data = df_juvenile_white, training_sizes=training_sizes)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "[3053, 3078, 3103, 3128, 3153, 3178, 3203, 3228, 3278, 3328, 3378, 3428, 3478, 3528, 3628]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJr5ZXOj3K8B",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCdbeVcTKfcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataframes with different metrics \n",
        "\n",
        "list_dfs = list_dfs_juvenile\n",
        "label = \"Recidivism\"\n",
        "model_juvenile = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=7, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "cv = 3 \n",
        "discr_feature = \"Grouped_Nationality\"\n",
        "min_value = \"Maghreb\"\n",
        "maj_value = \"Spanish\"\n",
        "\n",
        "results_df_juvenile = metrics_to_df(list_dfs=list_dfs, label = label, model = model_juvenile, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5nJMfcYMNIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5ba5e9f3-bb35-4e19-e367-e0dddce38947"
      },
      "source": [
        "# Save metrics csv\n",
        "results_df_juvenile.to_csv(\"df_juvenile_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_juvenile_metrics.csv\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_71fa5cc1-dc66-47e9-94d5-855fa276193c\", \"df_juvenile_metrics.csv\", 4988)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45cQusmJ3Wel",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFasT3tCN47d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "b49b78c9-0cf4-439a-b1ee-4d60fdbf47cc"
      },
      "source": [
        "# Visualization\n",
        "min_metrics_line_chart_selection(results_df_juvenile, title = \"Fairness and Performance Metrics for the Juvenile Justice Dataset - Unprivileged & Privileged Group\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c9f20ccc-cd4f-46b8-b742-0ba9d10f16e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c9f20ccc-cd4f-46b8-b742-0ba9d10f16e7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c9f20ccc-cd4f-46b8-b742-0ba9d10f16e7',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Unprivileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [0.5714285714285714, 0.5555555555555556, 0.5128205128205128, 0.7321428571428571, 0.7922077922077922, 0.8255813953488372, 0.8210526315789474, 0.5576923076923077, 0.8076923076923077, 0.8367346938775511, 0.8100558659217877, 0.9234449760765551, 0.920704845814978, 0.8300395256916996, 0.9198717948717948]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Privileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [0.6707207207207208, 0.6747747747747748, 0.6783783783783783, 0.8009009009009009, 0.8567567567567568, 0.859009009009009, 0.8617117117117117, 0.7157657657657658, 0.9261261261261261, 0.9261261261261261, 0.927027027027027, 0.9509009009009008, 0.9513513513513514, 0.9355855855855856, 0.9563063063063063]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Unprivileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [0.44, 0.38, 0.4266666666666667, 0.66, 0.656, 0.7066666666666667, 0.6571428571428571, 0.445, 0.652, 0.66, 0.6371428571428571, 0.7625, 0.7466666666666667, 0.646, 0.765]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Privileged Group\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [0.6720607661822986, 0.6776750330250991, 0.6829590488771466, 0.8127476882430648, 0.8536988110964333, 0.8560105680317041, 0.8596433289299867, 0.726552179656539, 0.9098414795244386, 0.9144649933949802, 0.916776750330251, 0.9451783355350066, 0.952113606340819, 0.9342800528401585, 0.9498018494055482]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [-0.2320607661822986, -0.2976750330250991, -0.25629238221047995, -0.15274768824306473, -0.19769881109643328, -0.1493439013650374, -0.2025004717871296, -0.28155217965653895, -0.2578414795244386, -0.2544649933949802, -0.27963389318739384, -0.18267833553500668, -0.20544693967415228, -0.28828005284015845, -0.18480184940554822]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600], \"y\": [-0.09929214929214936, -0.11921921921921919, -0.16555786555786556, -0.06875804375804384, -0.06454896454896453, -0.03342761366017177, -0.040659080132764314, -0.15807345807345807, -0.1184338184338184, -0.08939143224857504, -0.11697116110523931, -0.02745592482434578, -0.03064650553637338, -0.10554605989388599, -0.036434511434511485]}],\n",
              "                        {\"font\": {\"size\": 13}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Fairness and Performance Metrics for the Juvenile Justice Dataset - Unprivileged & Privileged Group\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Number of observations in the unprivileged group (privileged group has a fixed number of observations)\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c9f20ccc-cd4f-46b8-b742-0ba9d10f16e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vf0ZFAS_Ios",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "57a8f280-64ff-42a5-efae-980f045b03a1"
      },
      "source": [
        "# Define training sizes\n",
        "training_sizes_def = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000]\n",
        "\n",
        "training_sizes_theo = [25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Dummy Coding\n",
        "df_juvenile_train_input = pd.get_dummies(df_juvenile_train_input)\n",
        "\n",
        "# Plot performance learning curve\n",
        "plot_learning_curve(estimator = model_juvenile, \n",
        "                    title = \"Juvenile Justice Dataset - Performance Learning Curve (TPR)\", \n",
        "                    X = df_juvenile_train_input, y = df_juvenile_train_label, \n",
        "                    cv = 3, \n",
        "                    scoring = make_scorer(recall_score, average='weighted'), \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUBpyP0PcQZX",
        "colab_type": "text"
      },
      "source": [
        "# Deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evqIxPfx550",
        "colab_type": "text"
      },
      "source": [
        "## Alternative Metric df generation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHNebLS7x-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXPERIMENTATION FUNCTION\n",
        "\n",
        "def metrics_to_df_experiment(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  import numpy as np\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize lists for metrics \n",
        "  # COMPLETE\n",
        "  rows_compl_list = [] \n",
        "  f1_compl_list = []\n",
        "  f1_avg_train_score_list = []\n",
        "  tpr_compl_list = []\n",
        "  # MINORITY\n",
        "  rows_min_list = [] \n",
        "  f1_min_list = []\n",
        "  tpr_min_list = []\n",
        "  fpr_min_list = []\n",
        "  prob_y_1_min_list = []\n",
        "  # MAJORITY\n",
        "  rows_maj_list = []\n",
        "  f1_maj_list = []\n",
        "  tpr_maj_list = []\n",
        "  fpr_maj_list = []\n",
        "  prob_y_1_maj_list = []\n",
        "\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "      \n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      test_size = 0.4\n",
        "      X_train, X_test, y_train, y_test = train_test_split(df_train_input, df_train_label, test_size=test_size)\n",
        "\n",
        "      # fit model no training data\n",
        "      model = model\n",
        "      model.fit(X_train, y_train)\n",
        "      # make predictions for test data\n",
        "      y_pred = model.predict(X_test)\n",
        "      y_pred = [round(value) for value in y_pred]\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      df_calculation = pd.concat([X_test, y_test], axis=1)\n",
        "      df_calculation['y_pred'] = y_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = df_calculation[discr_feature].isin([min_value])\n",
        "      df_check_black = df_calculation[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = df_calculation[discr_feature].isin([maj_value])\n",
        "      df_check_white = df_calculation[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      # NUMBER OF ROWS\n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)   \n",
        "      # F1 test scores\n",
        "      f1_compl = f1_score(y_test, y_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "      # Recall\n",
        "      tpr_compl = recall_score(y_test, y_pred)\n",
        "      tpr_compl_list.append(tpr_compl)\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min = len(dataset_var[is_black].index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = fp_min / (fp_min+tn_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "      # Cond. Prob. P(y_min=1|minority)\n",
        "      filter_race_black_y_1 = df_calculation[discr_feature].isin([min_value]) & df_calculation[\"y_pred\"].isin([1])\n",
        "      filter_race_black = df_calculation[discr_feature].isin([min_value])\n",
        "      prob_y_1_min = len(df_calculation[filter_race_black_y_1].index) / len(df_calculation[filter_race_black].index)\n",
        "      prob_y_1_min_list.append(prob_y_1_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj = len(dataset_var[is_white].index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) \n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = fp_maj / (fp_maj+tn_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "      # Cond. Prob. P(y_maj=1|majority)\n",
        "      filter_race_white_y_1 = df_calculation[discr_feature].isin([maj_value]) & df_calculation[\"y_pred\"].isin([1])\n",
        "      filter_race_white = df_calculation[discr_feature].isin([maj_value])\n",
        "      prob_race_white_y_1 = len(df_calculation[filter_race_white_y_1].index) / len(df_calculation[filter_race_white].index)\n",
        "      prob_y_1_maj_list.append(prob_race_white_y_1)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                             \"rows_minority\": rows_min_list,\n",
        "                             \"rows_majority\": rows_maj_list, \n",
        "                             'f1_complete': f1_compl_list,\n",
        "                             \"f1_complete_train\": f1_avg_train_score_list,\n",
        "                             'f1_minority': f1_min_list,\n",
        "                             'f1_majority': f1_maj_list,\n",
        "                             \"tpr_complete\": tpr_compl_list,\n",
        "                             'tpr_minority': tpr_min_list,\n",
        "                             \"tpr_majority\": tpr_maj_list,\n",
        "                             \"fpr_minority\": fpr_min_list,\n",
        "                             \"fpr_majority\": fpr_maj_list,\n",
        "                             \"prob_yhat_1_minority\": prob_y_1_min_list,\n",
        "                             \"prob_yhat_1_majority\": prob_y_1_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"])\n",
        "  # FAIRNESS METRICS\n",
        "\n",
        "  ## Chosen\n",
        "  # Statistical Parity Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"stat_parity_diff\"] =  results_df[\"prob_yhat_1_minority\"] - results_df[\"prob_yhat_1_majority\"]\n",
        "  # Equal Opportunity Distance -> The closer to 0, the fairer.\n",
        "  results_df[\"equal_opport_dist\"] = results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"]  \n",
        "\n",
        "  ## Discarded\n",
        "  # Disparate Impact -> The closer to 1, the fairer.\n",
        "  results_df[\"disparate_impact\"] =  results_df[\"prob_yhat_1_minority\"] / results_df[\"prob_yhat_1_majority\"]\n",
        "  # Average Absolute Odds Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"aver_abs_odds_diff\"] = 0.5*(abs(results_df[\"fpr_minority\"] - results_df[\"fpr_majority\"])+abs(results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"])) \n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqOWar36y6bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest Hyperparameter Tuning Results\n",
        "\n",
        "# model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                               criterion='gini', max_depth=25, max_features='auto',\n",
        "#                               max_leaf_nodes=None, max_samples=None,\n",
        "#                               min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                               min_samples_leaf=1, min_samples_split=5,\n",
        "#                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "#                               n_jobs=None, oob_score=False, random_state=None,\n",
        "#                               verbose=0, warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9y_W-gJcV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RF Hyperparameter Tuning\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.3, 0.1],\n",
        "                \"max_depth\": [3, 6, 9], \n",
        "                \"min_child_weight\": [1, 3, 5],       \n",
        "                \"reg_lambda\": [2, 5, 10, 15, 100]}\n",
        "\n",
        "  # 1. Fundamental hyperparameters\n",
        "  # learning rate & number of trees\n",
        "\n",
        "  xgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
        "                     min_child_weight=1, gamma=0, subsample=0.8,\n",
        "                     colsample_bytree=0.8, objective= 'binary:logistic', \n",
        "                     scale_pos_weight=1, seed=27)\n",
        "\n",
        "\n",
        "  # n_estimators = number of trees in the foreset\n",
        "  # max_features = max number of features considered for splitting a node\n",
        "  # max_depth = max number of levels in each decision tree\n",
        "  # min_samples_split = min number of data points placed in a node before the node is split\n",
        "  # min_samples_leaf = min number of data points allowed in a leaf node\n",
        "  # bootstrap = method for sampling data points (with or without replacement)\n",
        "\n",
        "  # Define classifier\n",
        "  rf_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                          max_features = \"auto\")\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = 5,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = True)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  return(grid_rf_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWQHazJKBKYc",
        "colab_type": "text"
      },
      "source": [
        "**Steps**\n",
        "1. Specify hyperparameters for the chosen classification algorithm by means of grid search using standard ranges. Specify by means of on the dataset as a whole (full min. and maj.).\n",
        "2. Specify what is the majority and what is the minority class based on a value of a feature that identifies groups that are at risk of being discriminated. \n",
        "3. First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. For each step, train a random forest and test by mans of 10-fold cross validation.\n",
        "4. Attach the final outcome predictions  to the individual dataframes of the different datsets with diff. min. group sizes as seperate feature. -> *Open questions: is that manageable because we have different models (e.g. by means of majority vote of the 10 models with regards to classification)? If not possible then necessary to store the metric information directly without data frame as intermediate step (for that check calculation of fairness metric) (-> then also easier to store validation score to have an underfit/overfit check) OR use train/test split instead of k-fold cv (least prefered)*\n",
        "- a) Then filter that dataset so that only the minority group is displayed. Then, calculate  the confusion matrix and the F1-score. \n",
        "- b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric).\n",
        "5. Store the F1-score and the fairness metric for each training set size in a data frame. The data frame should have the following features: \n",
        "- a) Absolute number of training examples for the minority group, \n",
        "- b) percentage share of the minority group in the majority, \n",
        "- c) F-1 score for the min. group (maybe also seperately including std. dev. upper and lower bound), \n",
        "- d) fairness metric based on the min. & maj. group prediction comparisons. **Extended**: in case the fairness metric is not in the range of [0, 1] like the F1-score, normalize/standardize it (make sure that this is not biasing/altering the metrics results).\n",
        "6. Based on the data frame from step 5, plot a learning curve that shows the relationship between the different absolute sizes of training examples and the performance and fairness metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIkI8slaAokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Age\"])\n",
        ")  \n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Hours-per-week\"])\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Capital-Gain\"])\n",
        ")\n",
        "\n",
        "fig.update_layout(height=600, width=800, title_text=\"Boxplots for Numeric Features\")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = px.box(df_3_adult, y=\"Age\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4OQL4mAOv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bar Plot and Pie Plot for Discriminatory Feature \n",
        "\n",
        "def target_distribution(y_var, data):\n",
        "    val = data[y_var]\n",
        "\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "    plt.rcParams.update({'font.size': 13})\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    cnt = val.value_counts().sort_values(ascending=True)\n",
        "    labels = cnt.index.values\n",
        "\n",
        "    sizes = cnt.values\n",
        "    colors = sns.color_palette(\"PuBu\", len(labels))\n",
        "\n",
        "    #------------COUNT-----------------------\n",
        "    ax1.barh(cnt.index.values, cnt.values, color=colors)\n",
        "    ax1.set_title('Count plot of '+y_var)\n",
        "\n",
        "    #------------PERCENTAGE-------------------\n",
        "    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Distribution of '+y_var)\n",
        "    plt.show()\n",
        "\n",
        "# Distribution for Race\n",
        "\n",
        "target_distribution(y_var=\"Race\", data=df_3_adult)\n",
        "\n",
        "# Absolute number of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(dropna=False, sort=True)) # Learning: würde genauso mit df_3_adult[\"Race\"].value_counts(dropna=False) funktionieren\n",
        "\n",
        "# Percentage of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True))\n",
        "\n",
        "# Define Histogram Function\n",
        "def plot_histo(data, col, Y_columns):\n",
        "    df = data.copy()\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    \n",
        "    for i in range(0,2):\n",
        "        cnt = []; y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt += [df[df[y_col] == val][col].values]\n",
        "        bins = df[col].nunique()\n",
        "\n",
        "        axs[i].hist(cnt, bins=bins, stacked=True)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "Y_columns = [\"Race\", \"Sex\"]\n",
        "\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Gain',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Loss',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Hours-per-week',Y_columns=Y_columns))\n",
        "\n",
        "# Define Function for Bar Plot\n",
        "\n",
        "def plot_bar(data, col, Y_columns, max_cat=10):\n",
        "    df = data.copy()\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    cat_val = df[col].value_counts()[0:max_cat].index.values\n",
        "    df = df[df[col].isin(cat_val)]\n",
        "\n",
        "    for i in range(0,2):\n",
        "        y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt = df[df[y_col] == val][col].value_counts().sort_index()\n",
        "            axs[i].barh(cnt.index.values, cnt.values)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Bar plot of the \"+col+\" column by \"+y_col)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "Y_columns = [\"Race\"]\n",
        "\n",
        "print(plot_bar(data = df_3_adult, col='Sex',Y_columns=Y_columns)) # Error because values for two subplots are expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthjEWnrFWCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Column names\n",
        "# 'f1_complete', \"f1_complete_train\", 'f1_minority', 'f1_majority', \"tpr_complete\", \n",
        "# 'tpr_minority', \"tpr_majority\", \"fpr_minority\", \"fpr_majority\", \"prob_yhat_1_minority\", \"prob_yhat_1_majority\"\n",
        "\n",
        "# results_df[\"rel_share_min_of_maj\"]\n",
        "# results_df[\"aver_abs_odds_diff\"]  \n",
        "# results_df[\"stat_parity_diff\"] \n",
        "# results_df[\"equal_opport_dist\"]  \n",
        "# results_df[\"disparate_impact\"] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEP61EfQhFyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  print(data.isna().any())\n",
        "  print(data.isna().sum())\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  if second_disc_feature:\n",
        "    print(\"10. Cross-sectional sample size disparity\")\n",
        "    data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "  # (11. Feature Importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH6m4e6TWiwZ",
        "colab_type": "text"
      },
      "source": [
        "*Outcomes*:\n",
        "1. Understanding of the general structure of the dataset\n",
        "2. Understanding of basic statistics of the features of the dataset\n",
        "3. Identification of **features** on the basis of which subpopulations can be identified and discrimination can happen\n",
        "4. Identification of **values** of these features that are connected to certain subpopulations\n",
        "5. Identification of the percentage of certain subpopulation based on the values of these features -> *Sample Size Disparity I*\n",
        "6. Identification of the percentage of certain cross-sectional subpopulations based on the overall dataset -> *Sample Size Disparity II*\n",
        "7. Identification of the target feature and the type of class balance\n",
        "8. Identification of percentage of missing values per feature\n",
        "9. Identification of severity of outliers per feature\n",
        "10. Identification of the distribution of class values (for majority and minority)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si8Ss0hAWdO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Sensitive Feature\n",
        "print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {df_3_adult.columns}.\")\n",
        "print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {df_3_adult.Race.unique()}.\")\n",
        "\n",
        "# 2. Binary Target Feature\n",
        "print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "\n",
        "# 3. Total Number of Predictor Features\n",
        "print(f\"3. The Total Number of Predictor Features is: {df_3_adult.shape[1]}.\")\n",
        "\n",
        "# 4. Total Number of Training Examples\n",
        "df_3_adult.shape[0]\n",
        "print(f\"4. The Total Number of Training Examples is: {df_3_adult.shape[0]}.\")\n",
        "\n",
        "# 5. Total Number of Training Examples in the Minority Group \n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(df_3_adult[is_black].index)}.\")\n",
        "\n",
        "# 6. Sample Size Disparity\n",
        "# Absolute number of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {df_3_adult.Race.value_counts(dropna=False, sort=True)}.\")\n",
        "# Percentage of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "# 7. Class Balance\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(dropna=False))\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "# 8. Coarseness of Features\n",
        "print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "print(df_3_adult.isna().any())\n",
        "print(df_3_adult.isna().sum())\n",
        "\n",
        "# 9. Severity of Outliers for Numeric Features\n",
        "print(\"9. Severity of Outliers for Numeric Features\")\n",
        "ax = sns.boxplot(data=df_3_adult, orient=\"h\", palette=\"Set2\")\n",
        "print(ax)\n",
        "\n",
        "# (10. Cross-sectional sample size disparity)\n",
        "print(\"10. Cross-sectional sample size disparity\")\n",
        "df_3_adult.groupby(['Sex', 'Race']).agg({\"Over-50K\": 'count'})\n",
        "\n",
        "# (11. Feature Importance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJdQ2el7iHnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Basic Performance Statistics\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix \n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_train_pred) # Validate if the correct data was used\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='k-NN')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('k-NN ROC Curve')\n",
        "plt.show();\n",
        "\n",
        "# AUC \n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_train, y_train_pred)) # Validate if the correct data was used\n",
        "\n",
        "# Full Classification Metrics Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Visual Model Evaluation\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpK9B42QzITz",
        "colab_type": "text"
      },
      "source": [
        "## Test Metric function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t4W0BvffrcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries to study\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))\n",
        "\n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "\n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "display(Markdown('### Bias metrics for the Sex model'))\n",
        "fair = get_fair_metrics_and_plot(data_orig_sex_test, rf_orig_sex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGgBB2XQFSKa",
        "colab_type": "text"
      },
      "source": [
        "Overfitting / underfitting check by means of plotly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3vxb42sFUIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A: Custom plotly visualization - Function definition\n",
        "\n",
        "def compl_fitting_line_chart(metric_df):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete_train\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': \"Learning Curve for the Complete Dataset\",\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# B: Custom plotly visualization - Function Execution\n",
        "compl_fitting_line_chart(results_df)  #Change into results_df_adult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf2CHWVDzHxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST FUNCTION \n",
        "\n",
        "# Import relevant modules\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Get specific Dataframe from list of dataframes\n",
        "df_check = list_dfs[5]\n",
        "\n",
        "# Define Input and target columns\n",
        "label = \"Over-50K\"\n",
        "df_train_input = df_check.drop(columns=[label])  # Input\n",
        "df_train_label = df_check[label]                 # Target\n",
        "\n",
        "# Apply dummy coding\n",
        "df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "## TRAIN & TEST\n",
        "# Define model\n",
        "rf_test = RandomForestClassifier(criterion= \"gini\", \n",
        "                                 max_features = \"auto\",\n",
        "                                 max_depth = 4,\n",
        "                                 min_samples_leaf = 4,\n",
        "                                 n_estimators = 100)\n",
        "# Predict \n",
        "y_train_pred = cross_val_predict(rf_test,\n",
        "                                 df_train_input, \n",
        "                                 df_train_label, \n",
        "                                 cv = 10)\n",
        "\n",
        "# Append prediction labels to original dataset\n",
        "df_check['y_pred'] = y_train_pred\n",
        "\n",
        "# Create dataset version for minority class \n",
        "is_black = df_check[\"Race\"].isin([\"Black\"]) \n",
        "df_check_black = df_check[is_black]  # Minority group\n",
        "# Create dataset version for majority class\n",
        "is_white = df_check[\"Race\"].isin([\"White\"])\n",
        "df_check_white = df_check[is_white] # Majority group\n",
        "\n",
        "\n",
        "## METRICS\n",
        "# Get metrics for the COMPLETE dataset\n",
        "rows_compl_list = [] \n",
        "rows_compl = len(df_check.index)\n",
        "rows_compl_list.append(rows_compl)\n",
        "\n",
        "f1_compl_list = []\n",
        "f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "f1_compl_list.append(f1_compl) \n",
        "\n",
        "# metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)) \n",
        "# -> https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
        "\n",
        "# Get metrics for the MINORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_min_list = [] \n",
        "rows_min = len(df_check_black.index)\n",
        "rows_min_list.append(rows_min)\n",
        "# F1\n",
        "f1_min_list = []\n",
        "f1_min = f1_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "f1_min_list.append(f1_min)\n",
        "# TPR, RECALL\n",
        "tpr_min_list = []\n",
        "tpr_min = recall_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "tpr_min_list.append(tpr_min)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_min_list = []\n",
        "tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_min = tn_min / (tn_min+fp_min)\n",
        "fpr_min_list.append(fpr_min)\n",
        "\n",
        "# Get metrics for the MAJORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_maj_list = [] \n",
        "rows_maj = len(df_check_white.index)\n",
        "rows_maj_list.append(rows_maj)\n",
        "# F1\n",
        "f1_maj_list = []\n",
        "f1_maj = f1_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "f1_maj_list.append(f1_maj)\n",
        "# TPR, RECALL\n",
        "tpr_maj_list = []\n",
        "tpr_maj = recall_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "tpr_maj_list.append(tpr_maj)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_maj_list = []\n",
        "tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "\n",
        "# Store metrics for different iterations in Data Frame\n",
        "results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                           \"rows_minority\": rows_min_list,\n",
        "                           \"rows_majority\": rows_maj_list, \n",
        "                           'f1_complete': f1_compl_list,\n",
        "                           'f1_minority': f1_min_list,\n",
        "                           'f1_majority': f1_maj_list,\n",
        "                           'tpr_minority': tpr_min_list,\n",
        "                           \"tpr_majority\": tpr_maj_list,\n",
        "                           \"fpr_min\": fpr_min_list,\n",
        "                           \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "# Calculate new columns and append to df \n",
        "results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "# TBD if this calculation is correct that way or if (min/(maj+min))*100\n",
        "\n",
        "# results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DceolfFEzQiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train scores\n",
        "\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "cross_val_results = cross_validate(estimator = rf_test, \n",
        "                                  X = df_train_input, y = df_train_label, cv = 10, \n",
        "                                  scoring = f1, return_train_score=True)\n",
        "print(cross_val_results)\n",
        "f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJmXzP_tdJXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join iso alpha codes \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW_4b7rBdJ7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store data in dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "df_3_adult = pd.read_csv(io.BytesIO(uploaded['adult (1).data']), names = csv_columns, skipinitialspace=True)# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDpY_z3mW4kb",
        "colab_type": "text"
      },
      "source": [
        "Variant A: ggplot2\n",
        "\n",
        "- http://www.sthda.com/english/wiki/ggplot2-line-plot-quick-start-guide-r-software-and-data-visualization\n",
        "- https://www.datanovia.com/en/blog/how-to-create-a-ggplot-with-multiple-lines/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjg9oXQ_WyjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "(ggplot(data = results_df)\n",
        "  + aes(x='rows_minority') \n",
        "  + geom_line(aes(y = \"f1_minority\"), color =\"darkred\") \n",
        "  + geom_line(aes(y = \"aver_abs_odds_diff\"), color = \"steelblue\") \n",
        "  + ggtitle(\"Minority Metrics\")\n",
        "  # + scale_color_identity(guide = legend()) # Add legend based on colours\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5YkTvP4Aa4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Groupy by for conditional probabilities \n",
        "\n",
        "# Prepare groupby data \n",
        "filter_race_black_y_1 = df_check[\"Race\"].isin([\"Black\"]) & df_check[\"y_pred\"].isin([1])\n",
        "filter_race_white_y_1 = df_check[\"Race\"].isin([\"White\"]) & df_check[\"y_pred\"].isin([1])\n",
        "\n",
        "filter_race_black = df_check[\"Race\"].isin([\"Black\"])\n",
        "filter_race_white = df_check[\"Race\"].isin([\"White\"])\n",
        "\n",
        "prob_race_black_y_1 = len(df_check[filter_race_black_y_1].index) / len(df_check[filter_race_black].index)\n",
        "prob_race_white_y_1 = len(df_check[filter_race_white_y_1].index) / len(df_check[filter_race_white].index)\n",
        "\n",
        "print(prob_race_black_y_1)\n",
        "print(prob_race_white_y_1)\n",
        "\n",
        "# rating_probs = df_check.groupby(\"Race\").size().div(len(df_check))\n",
        "# groupby_probs = df_check.groupby(['y_pred', \"Race\"]).size().div(len(df_check)).div(rating_probs, axis=0, level=\"Race\")\n",
        "# print(groupby_probs)\n",
        "\n",
        "# Experiments identify how to subset groupby in order to get right conditional probability \n",
        "rating_probs = df_check.groupby('Race').size().div(len(df_check))\n",
        "print(df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race'))\n",
        "group_df_1 = df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race')\n",
        "# list_groupby = list(group_df_1)\n",
        "# print(list_groupby)\n",
        "# print(list_groupby[2])\n",
        "# print(list_groupby[3])\n",
        "\n",
        "# print(group_df_1[[2]])\n",
        "# print(group_df_1[[3]])\n",
        "# test = group_df_1[[3]]\n",
        "# print(test)\n",
        "\n",
        "# # Create list\n",
        "# list_groupby_probs = list(groupby_probs)\n",
        "# print(list_groupby_probs)\n",
        "# prob_y_1_min = list_groupby_probs[2]\n",
        "# print(prob_y_1_min)\n",
        "\n",
        "# prob_y_1_maj = list_groupby_probs[3]\n",
        "# print(prob_y_1_maj)\n",
        "\n",
        "# # Cond. Prob. P(y=1|minority)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zs8SxSqZGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "  \n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    See :ref:`examples/model_selection/plot_learning_curve.py\n",
        "    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
        "    \"\"\"\n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtMtLvu3b7Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      tpr_compl = \n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min_list = [] \n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min_list = []\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min_list = []\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_min_list = []\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = tn_min / (tn_min+fp_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj_list = [] \n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj_list = []\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj_list = []\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_maj_list = []\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                              \"rows_minority\": rows_min_list,\n",
        "                              \"rows_majority\": rows_maj_list, \n",
        "                              'f1_complete': f1_compl_list,\n",
        "                              'f1_minority': f1_min_list,\n",
        "                              'f1_majority': f1_maj_list,\n",
        "                              'tpr_minority': tpr_min_list,\n",
        "                              \"tpr_majority\": tpr_maj_list,\n",
        "                              \"fpr_min\": fpr_min_list,\n",
        "                              \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcvZqMvgw-XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPROCESSING manually\n",
        "# Encoding Binary \n",
        "\n",
        "df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc12Lw3VC-_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" else val == 0)\n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace(to_replace=['<=50K', '>50K'], value=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJsTc_t6-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OLD FUNCTION\n",
        "\n",
        "label = \"Over-50K\"\n",
        "model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "list_dfs = list_dfs\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv = 10):\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "  \n",
        "    ## DATA PREPROCESSING\n",
        "      # Seperate dataset by input features and labels\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]]                # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "    ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input, \n",
        "                                       df_train_label, \n",
        "                                       cv = cv)\n",
        "    \n",
        "    ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "\n",
        "      # confusion_matrix(df_train_label, y_train_pred) \n",
        "\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred)\n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Band for difference in Training and Validation Set\n",
        "        # train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring, \n",
        "                                                                # n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "        train_scores_mean = np.mean(train_scores, axis=1)\n",
        "        train_scores_std  = np.std(train_scores, axis=1)\n",
        "        test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "        test_scores_std   = np.std(test_scores, axis=1)\n",
        "        plt.grid()\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "        \n",
        "        row_min_list = []\n",
        "        rows_min = len(dataset_min.index)\n",
        "        row_min_list.append(rows_min)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "\n",
        "        f1_min_list = []\n",
        "        f1_min = f1_score(df_train_label, y_train_pred)\n",
        "        f1_min_list.append()\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "\n",
        "        # Number of rows\n",
        "        row_maj_list = []\n",
        "        rows_maj = len(dataset_maj.index)\n",
        "        row_maj_list.append(rows_maj)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "      \n",
        "        f1_maj_list = []\n",
        "        f1_maj = f1_score(df_train_label, y_train_pred)\n",
        "        f1_maj_list.append()\n",
        "\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'Total_rows':rows_compl_list, \n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list})\n",
        "  \n",
        "  # Setting a column as an index\n",
        "  # dogs_ind = dogs.set_index(\"name\")\n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFpvWx8bRWi",
        "colab_type": "text"
      },
      "source": [
        "Calculate Confusion Matrix Cell Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-VTE1ryDbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DEPRECATED\n",
        "\n",
        "  # neighbors_list = list(range(5,500, 5))\n",
        "  # accuracy_list = []\n",
        "  # for test_number in neighbors_list:\n",
        "  # model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "  # predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "  # accuracy = accuracy_score(y_test, predictions)\n",
        "  # accuracy_list.append(accuracy)\n",
        "  # results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "\n",
        "  # # Initialize df \n",
        "  # algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs']) # To Do: Change metrics here\n",
        "\n",
        "  # def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "  #     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "  #                                             columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "  #                                             index=[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV1izcpevA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['number_training_examples', 'rel_share_min', 'min_f1', \"total_f1\", 'fairness metric'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "                                            index=[name]))\n",
        "    \n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "    \n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "# Fairness Metric\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai\n",
        "\n",
        "# 1: Average Absolute Odd Difference\n",
        "average_abs_odds_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference\n",
        "    def average_odds_difference(self):\n",
        "        r\"\"\"Average of difference in FPR and TPR for unprivileged and privileged\n",
        "        groups:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "           \\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n",
        "           + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\n",
        "\n",
        "        A value of 0 indicates equality of odds.\n",
        "        \"\"\"\n",
        "        return 0.5 * (self.difference(self.false_positive_rate)\n",
        "                    + self.difference(self.true_positive_rate))\n",
        "\n",
        "# This metric's scale would need to be \"reversed\", presumably.\n",
        "\n",
        "# 2: Equal Opportunity Distance\n",
        "equal_opportunity_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference\n",
        "\n",
        "    def equal_opportunity_difference(self):\n",
        "        \"\"\"Alias of :meth:`true_positive_rate_difference`.\"\"\"\n",
        "        return self.true_positive_rate_difference()\n",
        "\n",
        "# ClassificationMetric and BinaryLabelDatasetMetric.\n",
        "\n",
        "# Template:\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "# algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "# def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "#     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkTibSibQO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_tpr(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FN = 0\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "#     TPR = TP/(TP+FN)\n",
        "\n",
        "# Calculation of confusion matrix rates\n",
        "\n",
        "# # Sensitivity, hit rate, recall, or true positive rate\n",
        "# TPR = TP/(TP+FN)\n",
        "# # Specificity or true negative rate\n",
        "# TNR = TN/(TN+FP) \n",
        "# # Precision or positive predictive value\n",
        "# PPV = TP/(TP+FP)\n",
        "# # Negative predictive value\n",
        "# NPV = TN/(TN+FN)\n",
        "# # Fall out or false positive rate\n",
        "# FPR = FP/(FP+TN)\n",
        "# # False negative rate\n",
        "# FNR = FN/(TP+FN)\n",
        "# # False discovery rate\n",
        "# FDR = FP/(TP+FP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkh5YSJaZej",
        "colab_type": "text"
      },
      "source": [
        "Approach 2: Take slices of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKws3DBaYHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slice code for minority example\n",
        "\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "\n",
        "# Dummy coding for slice\n",
        "\n",
        "df_3_adult_black_dummies = pd.get_dummies(df_3_adult_black)\n",
        "\n",
        "# Features of complete dataset\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_black_input = df_3_adult_black_dummies.drop(columns=[\"Over-50K\"])\n",
        "print(df_3_adult_black_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_black_label = df_3_adult_black_dummies[\"Over-50K\"]\n",
        "print(df_3_adult_black_label)\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "sizes_minority = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "                  350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "                  2000, 2250, 2500, 2750] \n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = random_forest, \n",
        "                    title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_black_input, y = df_3_adult_black_label, \n",
        "                    cv = 10, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = sizes_minority)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "# plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41F18rCYhL1",
        "colab_type": "text"
      },
      "source": [
        "Fairness Metric per Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dZ5qd1YrYS",
        "colab_type": "text"
      },
      "source": [
        "https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0E7etjXYgGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Define protected class\n",
        "dataset_orig = df_3_adult(\n",
        "    protected_attribute_names=['Race'],           \n",
        "    privileged_classes=\"White\",     \n",
        "    features_to_drop=['personal_status', 'sex'] \n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'Race': \"White\"}]\n",
        "unprivileged_groups = [{'Race': \"Black\"}]\n",
        "\n",
        "\n",
        "# Convert dataset into aif360 adequate form\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XdytLDYXUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(regr.predict(diabetes_X_test))\n",
        "\n",
        "rfc_model_3 = RandomForestClassifier(n_estimators=200)\n",
        "rfc_model_3.predict(X_test)\n",
        "\n",
        "X_test['survived'] = rfc_model_3.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKx-nFqaTvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect original Learning Curve function\n",
        "import inspect\n",
        "from sklearn.model_selection import learning_curve\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOolonVAnCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Function to get TPR, FPR, usw.\n",
        "\n",
        "# # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "\n",
        "# def perf_measures(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FP = 0\n",
        "#     TN = 0\n",
        "#     FN = 0\n",
        "\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "#            FP += 1\n",
        "#         if y_actual[i]==y_hat[i]==0:\n",
        "#            TN += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "\n",
        "#     # Calculation of confusion matrix rates\n",
        "#     # Sensitivity, hit rate, recall, or true positive rate\n",
        "#     TPR = TP/(TP+FN)\n",
        "#     # Specificity or true negative rate\n",
        "#     TNR = TN/(TN+FP) \n",
        "#     # Precision or positive predictive value\n",
        "#     PPV = TP/(TP+FP)\n",
        "#     # Negative predictive value\n",
        "#     NPV = TN/(TN+FN)\n",
        "#     # Fall out or false positive rate\n",
        "#     FPR = FP/(FP+TN)\n",
        "#     # False negative rate\n",
        "#     FNR = FN/(TP+FN)\n",
        "#     # False discovery rate\n",
        "#     FDR = FP/(TP+FP)\n",
        "\n",
        "#     return TPR, FPR, TNR, FNR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYvv_KIktiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated CONFUSION MATRIX RATES\n",
        "\n",
        "# tpr_min_list = []\n",
        "# tpr_min = get_tpr(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "\n",
        "# tpr_min_list = [] \n",
        "# fpr_min_list = []\n",
        "# tnr_min_list = []\n",
        "# fnr_min_list = []\n",
        "# tpr_min, fpr_min, tnr_min, fnr_min = perf_measures(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "# fpr_min_list.append(fpr_min)\n",
        "# tnr_min_list.append(tnr_min)\n",
        "# fnr_min_list.append(fnr_min)\n",
        "\n",
        "# tpr_maj_list = []\n",
        "# fpr_maj_list = []\n",
        "# tnr_maj_list = []\n",
        "# fnr_maj_list = []\n",
        "# tpr_maj, fpr_maj, tnr_maj, fnr_maj = perf_measures(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"])\n",
        "# tpr_maj_list.append(tpr_maj)\n",
        "# fpr_maj_list.append(fpr_maj)\n",
        "# tnr_maj_list.append(tnr_maj)\n",
        "# fnr_maj_list.append(fnr_maj)\n",
        "\n",
        "#  \"tpr_majority\": tpr_maj_list\n",
        "#  \"tpr_minority\": tpr_min_list\n",
        "#  \"fpr_majority\": fpr_maj_list\n",
        "#  \"fpr_minority\": fpr_min_list\n",
        "#  \"tnr_majority\": tnr_maj_list\n",
        "#  \"tnr_minority\": tnr_min_list\n",
        "#  \"fnr_majority\": fnr_maj_list\n",
        "#  \"fnr_minority\": fnr_min_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuG9uvV5U19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_datasets_new(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "#     datasets = []\n",
        "#     for training_size in training_sizes:\n",
        "#         \n",
        "#             dataset_min_sample = min_data.sample(n=training_size, random_state=1)\n",
        "#             dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "#             datasets.append(dataset)\n",
        "#     return datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FwySPCFNK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUaC5dzFQcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "    \"\"\"\n",
        "    \n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqnx5tPkCpWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# min_dataset_list = list()\n",
        "# compl_dataset_list = list()\n",
        "\n",
        "# is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "# is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "# def create_datasets(min_data, maj_data, training_sizes):\n",
        "#     for training_size in training_sizes:\n",
        "#         while len(min_data.index) >= training_size: # Code should stop when number of rows of df with min. group is smaller than training size iteration\n",
        "#             dataset_min_slice = min_data.sample(n = training_size, random_state = 1) # Get a random subset of the minority group\n",
        "#             min_dataset_list.append(dataset_min_slice) # Create list of data frames that include observations of minority group of different sizes\n",
        "#             for min_dataset_component in min_dataset_list:\n",
        "#                 dataset_list = maj_data.append(min_dataset_component) # Merge observations of min. group of different sizes with observations from maj. group\n",
        "#                 compl_dataset_list.append(dataset_list) # Create list of data frames that include majority group (fixed size) and minority group (different sizes)\n",
        "#                 return compl_dataset_list\n",
        "\n",
        "# training_sizes_2 = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, \n",
        "                      300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750]\n",
        "\n",
        "# list_dfs_2 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_2)\n",
        "# print(len(list_dfs_2))\n",
        "# print([df.shape[0] for df in list_dfs_2])\n",
        "\n",
        "# training_sizes_3 =  [2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500]\n",
        "\n",
        "# list_dfs_3 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_3)\n",
        "# print(len(list_dfs_3))\n",
        "# print([df.shape[0] for df in list_dfs_3])\n",
        "\n",
        "# training_sizes_4 = [4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# list_dfs_4 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_4)\n",
        "# print(len(list_dfs_4))\n",
        "# print([df.shape[0] for df in list_dfs_4])\n",
        "\n",
        "# # Execute function\n",
        "# training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "#                   350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "#                   2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "#                   4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# df_3_adult_black = df_3_adult[is_black]  # Define minority group based on original data frame\n",
        "# df_3_adult_white = df_3_adult[is_white]  # Define majority group based on original data frame\n",
        "\n",
        "# list_dfs = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhAaGcytjPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_3_adult_train_input\n",
        "# df_3_adult_train_label\n",
        "\n",
        "# # Row bind\n",
        "# df1.append(df2)\n",
        "\n",
        "# def get_min_datasets(data, ):\n",
        "#   for specific_size in min_sizes:\n",
        "#     if n_row(dataset) >= specific_size:\n",
        "#     elif n_row(dataset) < specific_size:\n",
        "#       stop\n",
        "#     dataset_min_slice = training_examples.sample(frac=1) # shuffle dataset with minority group randomely before slicing\n",
        "#     dataset_min_slice = dataset_min_slice.slice[min_sizes]\n",
        "#     diff_min_datasets.append(dataset_min) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_a7-0Dg1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ggf. feature importance für jede Iteration auch noch berechnen und dann ans DF anhängen\n",
        "\n",
        "# Sample Code\n",
        "\n",
        "# neighbors_list = [3,5,10,20,50,75]      # use either the one or the other neighbors_list \n",
        "# neighbors_list = list(range(5,500, 5))\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# accuracy_list = []\n",
        "# for test_number in neighbors_list:\n",
        "#   model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "#   predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "#   accuracy = accuracy_score(y_test, predictions)\n",
        "#   accuracy_list.append(accuracy)\n",
        "\n",
        "  # Because I will be working with k-fold cv, most likely I will need to take the average \n",
        "  # and define the standard deviation (which then can also be shown in the plot as bands)\n",
        "\n",
        "  # Important: Get score both training and testing \n",
        "\n",
        "# results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# Matplotlib plotting code -> Alternatively, use plotly\n",
        "\n",
        "# plt.plot(results_df['neighbors'],\n",
        "# results_df['accuracy'])\n",
        "# # Add the labels and title\n",
        "# plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',\n",
        "# title='Accuracy for different n_neighbors')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Dataframe \n",
        "\n",
        "# results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
        "# print(results_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZywZ_O294Rcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# II. Model setup\n",
        "# a) Linear\n",
        "\n",
        "# b) Nonlinear\n",
        "# K-NN\n",
        "# knn = KNeighborsClassifier(n_neighbors=6) # To Do: Check appropriatness of hyperparameter\n",
        "# knn.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training\n",
        "\n",
        "# c) Others\n",
        "# Random Forest\n",
        "# random_forest = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs =-1)\n",
        "#  random_forest.fit(df_3_adult_train_input, df_3_adult_train_label)\n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLHQERS8pV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute train/test split.\n",
        "\n",
        "# Check: Probably not necessary \n",
        "\n",
        "# Execute Train/test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_3_adult_train_input, \n",
        "#                                                     df_3_adult_train_label, \n",
        "#                                                     test_size=0.3, \n",
        "#                                                     random_state=21) # seed for random number generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMoVDkicTDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 2\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "    go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='group')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Yvu-jJcado",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 1\n",
        "# fig = px.bar(df_3_adult_aggr, x=\"Race\", y=\"counting\", color='Sex', barmode='group')\n",
        "# fig.show()\n",
        "\n",
        "# Hint: Does not work properly.\n",
        "\n",
        "# Plot Alternative 2\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# fig = go.Figure(data=[\n",
        "#     go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "#     go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "# ])\n",
        "# # Change the bar mode\n",
        "# fig.update_layout(barmode='group')\n",
        "# fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7546kms4na",
        "colab_type": "text"
      },
      "source": [
        "## Learning Curve Variants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPJKpqPtfVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Learning Curve Function\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhGUR1hti1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(estimator = random_forest, title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    axes=None, ylim=None, cv=10, train_sizes=sizes)\n",
        " \n",
        "# models = []\n",
        "\n",
        "# for model in models:\n",
        "#   plot_learning_curve(estimator=model, title=\"k-nn Learning Curve\", X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "#                     axes=None, ylim=None, cv=10,\n",
        "#                     n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzmnhnP22Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yellowbrick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Pq7eJW2xjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from yellowbrick.datasets import load_game\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "# Encode the categorical data\n",
        "X = df_3_adult_train_input\n",
        "y = df_3_adult_train_label\n",
        "\n",
        "# Create the learning curve visualizer\n",
        "cv = StratifiedKFold(n_splits=12)\n",
        "# sizes = np.linspace(0.1, 1.0, 10)\n",
        "sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = LearningCurve(\n",
        "    model = random_forest, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
        ")\n",
        "\n",
        "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
        "visualizer.show()           # Finalize and render the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uAPV7p-tja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
        "\n",
        "# train_sizes = [1, 100, 500, 1000, 1500, 2000, 2500, 3000, 5000, 7500, 10000]\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# features = df_3_adult_train_input.columns\n",
        "# target = df_3_adult_train_label\n",
        "\n",
        "train_sizes, train_scores, validation_scores = learning_curve(estimator = random_forest, \n",
        "                                                              X = df_3_adult_train_input, \n",
        "                                                              y = df_3_adult_train_label, \n",
        "                                                              cv = 5)\n",
        "\n",
        "train_scores_mean = train_scores.mean(axis = 1)\n",
        "validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
        "print('\\n', '-' * 20) # separator\n",
        "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('Score', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.title('Learning curves for a knn model', fontsize = 18, y = 1.03)\n",
        "plt.legend()\n",
        "plt.ylim(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxOS8Pdw-z6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Bundling our previous work into a function ###\n",
        "\n",
        "def learning_curves(estimator, data, features, target, train_sizes, cv, scoring):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        estimator, data[features], data[target], train_sizes = train_sizes,\n",
        "        cv = cv, scoring = scoring)\n",
        "    \n",
        "    train_scores_mean = train_scores.mean(axis = 1)\n",
        "    validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "\n",
        "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "\n",
        "    plt.ylabel('Score', fontsize = 14)\n",
        "    plt.xlabel('Training set size', fontsize = 14)\n",
        "    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n",
        "    plt.title(title, fontsize = 18, y = 1.03)\n",
        "    plt.legend()\n",
        "    plt.ylim(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgoEC9cb-27p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plotting the two learning curves ###\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "plt.figure(figsize = (16,5))\n",
        "\n",
        "for model, i in [(RandomForestClassifier(), 1)]:\n",
        "    plt.subplot(1,2,i)\n",
        "    learning_curves(estimator = random_forest, \n",
        "                    data = df_3_adult_dummies, \n",
        "                    features = df_3_adult_train_input.columns, \n",
        "                    target= \"Over-50K\", \n",
        "                    train_sizes = sizes,\n",
        "                    scoring = f1, \n",
        "                    cv= 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTO7VG99DTcp",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGGOb9ACbyEZ",
        "colab_type": "text"
      },
      "source": [
        "Guide: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/xgboost_caip.ipynb?hl=de\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5JGFSwCCMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install current tensorflow version\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JlJ_VWqARH_X",
        "colab": {}
      },
      "source": [
        "# Install the What-If Tool\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoYDH1TSBqO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Helper Functions for the What-If tool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import witwidget\n",
        "\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuZMXEnMdNe5",
        "colab_type": "text"
      },
      "source": [
        "**Specify feature and label columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbMGWyF0EcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the column in the dataset you wish for the model to predict\n",
        "label_column = 'Over-50K'\n",
        "\n",
        "# Set list of all columns from the dataset we will use for model input.\n",
        "input_features = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
        "                  'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
        "                  'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country']\n",
        "\n",
        "# Create a list containing all input features and the label column\n",
        "features_and_labels = input_features + [label_column]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQm_vccX1Y4V",
        "colab_type": "text"
      },
      "source": [
        "**Convert dataset to tf.Example protos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjPGPeI1asw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = df_to_examples(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBwOj9R3PZw",
        "colab_type": "text"
      },
      "source": [
        "**Create and train the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJH-6Tf2t5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 200  #@param {type: \"number\"}\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df_what_if, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHVwhOw3ORW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "num_datapoints = 2000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 1000  #@param {type: \"number\"}\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Load up the test dataset\n",
        "test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True,\n",
        "  skiprows=1)\n",
        "make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n",
        "test_examples = df_to_examples(test_df[0:num_datapoints])\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(test_examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec).set_label_vocab(['Under 50K', 'Over 50K'])\n",
        "a = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}