{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master_Thesis-Algorithmic_Bias_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qXEVy0agdEPj"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1xgO861smhhAVw-ALtedP9mnmWIFwVSvY",
      "authorship_tag": "ABX9TyMozr+J5MX7yTWWZkjRANl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilsBahr94/Master_Thesis_Algorithmic-Bias-in-AI/blob/Develop/Master_Thesis_Algorithmic_Bias_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LslOkLJNLSY-"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5GE201KpLiKY"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kWQTpKzC0D",
        "colab_type": "code",
        "outputId": "ab5c4c5d-51fa-492e-82a9-7c8893001e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0J4SEBlzMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install relevant libraries\n",
        "!pip install plotnine  \n",
        "!pip install pandas\n",
        "!pip install plotly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFZNjOwEsaEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0047770c-9b84-49c5-a2bb-b8a706a99137"
      },
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotnine import *\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Learning Curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkXO8VHI5CD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg1LaX71h40E",
        "colab_type": "text"
      },
      "source": [
        "**Fairness Tools**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LBYqEv4Omj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installation\n",
        "!pip install aif360\n",
        "# https://github.com/IBM/AIF360/blob/master/examples/README.md\n",
        "!pip install aequitas\n",
        "# https://github.com/dssg/aequitas\n",
        "!pip install audit-AI\n",
        "# https://github.com/pymetrics/audit-ai\n",
        "!pip install responsibly\n",
        "# https://docs.responsibly.ai/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBA6AQHbR0pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ89yoNTe1AE",
        "colab_type": "text"
      },
      "source": [
        "## Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KErf0aaNiQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label, cv):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.2],\n",
        "                \"n_estimators\": [50, 100, 150],\n",
        "                \"max_depth\": [3, 6, 9],\n",
        "                \"min_child_weight\": [1, 3, 5], \n",
        "                \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "  xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  print(grid_rf_class.best_params_)\n",
        "  print(grid_rf_class.best_estimator_) \n",
        "\n",
        "  return(grid_rf_class)\n",
        "\n",
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(\"7. Class Balance: The Class Balance looks as follows:\")\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  print(data.apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().sum()))\n",
        "  print(data.groupby(disc_feature).apply(lambda x: x.isna().mean()))\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  if second_disc_feature:\n",
        "    print(\"10. Cross-sectional sample size disparity\")\n",
        "    data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "# Define Data Preprocessing Function\n",
        "\n",
        "def fair_preprocess(data, label, neg_class, pos_class):\n",
        "\n",
        "  ''' Applies binary encoding on the values of the label feature.\n",
        "\n",
        "  Returns two datasets in the following order: data_input_features and data_label. '''\n",
        "\n",
        "  # Binary encoding\n",
        "  data[label] = data[label].replace({neg_class: 0, pos_class: 1})\n",
        "\n",
        "  # Create separate dataset version for input features and label\n",
        "  data_input_features = data.drop(columns=[label])\n",
        "  data_label = data[label]\n",
        "\n",
        "  return data_input_features, data_label\n",
        "\n",
        "\n",
        "# Function to create datasets with different minority group sizes\n",
        "\n",
        "def create_datasets(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "    datasets = []\n",
        "    for training_size in training_sizes:\n",
        "      if abs(training_size) <= len(min_data.index):\n",
        "        dataset_min_sample = min_data.sample(n=training_size)\n",
        "        dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "        datasets.append(dataset)\n",
        "      else:\n",
        "        break\n",
        "    print(len(datasets))\n",
        "    print([df.shape[0] for df in datasets])\n",
        "    return datasets\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  import numpy as np\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize lists for metrics \n",
        "  # COMPLETE\n",
        "  rows_compl_list = [] \n",
        "  f1_compl_list = []\n",
        "  f1_avg_train_score_list = []\n",
        "  tpr_compl_list = []\n",
        "  # MINORITY\n",
        "  rows_min_list = [] \n",
        "  f1_min_list = []\n",
        "  tpr_min_list = []\n",
        "  fpr_min_list = []\n",
        "  prob_y_1_min_list = []\n",
        "  # MAJORITY\n",
        "  rows_maj_list = []\n",
        "  f1_maj_list = []\n",
        "  tpr_maj_list = []\n",
        "  fpr_maj_list = []\n",
        "  prob_y_1_maj_list = []\n",
        "\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      # NUMBER OF ROWS\n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)   \n",
        "      # F1 test scores\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "      # F1 training scores\n",
        "      f1 = make_scorer(f1_score)\n",
        "      cross_val_results = cross_validate(estimator = model, \n",
        "                                         X = df_train_input, \n",
        "                                         y = df_train_label, \n",
        "                                         cv = cv, \n",
        "                                         scoring = f1, \n",
        "                                         return_train_score=True)\n",
        "      f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n",
        "      f1_avg_train_score_list.append(f1_avg_train_score)\n",
        "      # Recall\n",
        "      tpr_compl = recall_score(df_train_label, y_train_pred)\n",
        "      tpr_compl_list.append(tpr_compl)\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = fp_min / (fp_min+tn_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "      # Cond. Prob. P(y_min=1|minority)\n",
        "      filter_race_black_y_1 = dataset_var[discr_feature].isin([min_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_black = dataset_var[discr_feature].isin([min_value])\n",
        "      prob_y_1_min = len(dataset_var[filter_race_black_y_1].index) / len(dataset_var[filter_race_black].index)\n",
        "      prob_y_1_min_list.append(prob_y_1_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) \n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = fp_maj / (fp_maj+tn_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "      # Cond. Prob. P(y_maj=1|majority)\n",
        "      filter_race_white_y_1 = dataset_var[discr_feature].isin([maj_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      prob_race_white_y_1 = len(dataset_var[filter_race_white_y_1].index) / len(dataset_var[filter_race_white].index)\n",
        "      prob_y_1_maj_list.append(prob_race_white_y_1)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                             \"rows_minority\": rows_min_list,\n",
        "                             \"rows_majority\": rows_maj_list, \n",
        "                             'f1_complete': f1_compl_list,\n",
        "                             \"f1_complete_train\": f1_avg_train_score_list,\n",
        "                             'f1_minority': f1_min_list,\n",
        "                             'f1_majority': f1_maj_list,\n",
        "                             \"tpr_complete\": tpr_compl_list,\n",
        "                             'tpr_minority': tpr_min_list,\n",
        "                             \"tpr_majority\": tpr_maj_list,\n",
        "                             \"fpr_minority\": fpr_min_list,\n",
        "                             \"fpr_majority\": fpr_maj_list,\n",
        "                             \"prob_yhat_1_minority\": prob_y_1_min_list,\n",
        "                             \"prob_yhat_1_majority\": prob_y_1_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"])\n",
        "  # FAIRNESS METRICS\n",
        "  # Average Absolute Odds Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"aver_abs_odds_diff\"] = 0.5*(abs(results_df[\"fpr_minority\"] - results_df[\"fpr_majority\"])+abs(results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"])) \n",
        "  # Statistical Parity Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"stat_parity_diff\"] =  results_df[\"prob_yhat_1_minority\"] - results_df[\"prob_yhat_1_majority\"]\n",
        "  # Equal Opportunity Distance -> The closer to 0, the fairer.\n",
        "  results_df[\"equal_opport_dist\"] = results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"]  \n",
        "  # Disparate Impact -> The closer to 1, the fairer.\n",
        "  results_df[\"disparate_impact\"] =  results_df[\"prob_yhat_1_minority\"] / results_df[\"prob_yhat_1_majority\"]\n",
        "\n",
        "  return(results_df)\n",
        "\n",
        "# Learning Curve https://www.kaggle.com/grfiv4/learning-curves-1\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, \n",
        "                        ylim=None, cv=None, \n",
        "                        scoring=None, obj_line=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "              A string (see model evaluation documentation)\n",
        "              or a scorer callable object / function with signature scorer(estimator, X, y)\n",
        "              For Python 3.5 the documentation is here:\n",
        "              http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "              For example, Log Loss is specified as 'neg_log_loss'\n",
        "              \n",
        "    obj_line : numeric or None (default: None)\n",
        "               draw a horizontal line \n",
        "               \n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "        \n",
        "        \n",
        "    Citation\n",
        "    --------\n",
        "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "        \n",
        "    Usage\n",
        "    -----\n",
        "        plot_learning_curve(estimator = best_estimator, \n",
        "                            title     = best_estimator_title, \n",
        "                            X         = X_train, \n",
        "                            y         = y_train, \n",
        "                            ylim      = (-1.1, 0.1), # neg_log_loss is negative\n",
        "                            cv        = StatifiedCV, # CV generator\n",
        "                            scoring   = scoring,     # eg., 'neg_log_loss'\n",
        "                            obj_line  = obj_line,    # horizontal line\n",
        "                            n_jobs    = n_jobs)      # how many CPUs\n",
        "\n",
        "         plt.show()\n",
        "    \"\"\"\n",
        "    \n",
        "    from sklearn.model_selection import learning_curve\n",
        "    import numpy as np\n",
        "    from matplotlib import pyplot as plt\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std  = np.std(train_scores, axis=1)\n",
        "    test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "    test_scores_std   = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.style.use('seaborn')\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    if obj_line:\n",
        "        plt.axhline(y=obj_line, color='blue')\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "# Minority Line Chart - Function Definition\n",
        "\n",
        "def min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "  \n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "  # Fairness Metrics\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"aver_abs_odds_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Avg. Abs. Odds Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"stat_parity_diff\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Statistical Parity Difference'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"equal_opport_dist\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Equal Opportunity Distance'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_minority\"], y=metric_df[\"disparate_impact\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='Disparate Impact'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Minority',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "# Majority <-> Minority Line Chart - Function Definition\n",
        "\n",
        "def maj_min_metrics_line_chart(metric_df, title):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z0MC-9BGLofv"
      },
      "source": [
        "# Case Studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66N6XRfQf1Pb",
        "colab_type": "text"
      },
      "source": [
        "## 0) Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xIamwLuVcSP",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZJiBYwBVqOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "best_classifier_adult = hyperparameter_tuning(df_3_adult_train_input, df_3_adult_train_label, cv = 5)\n",
        "\n",
        "# Resulting Model\n",
        "\n",
        "# {'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'reg_lambda': 1}\n",
        "# XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "#               learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "#               nthread=None, objective='binary:logistic', random_state=0,\n",
        "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "#               silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZJ5ojlvVm59",
        "colab_type": "code",
        "outputId": "b7d1943e-8480-4f3f-f53b-5501733fcb66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-947c25bc7585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_compas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_compas_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_compas_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_compas_train_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BvyZkFRVn8H",
        "colab_type": "code",
        "outputId": "3ceaffda-7696-40ee-fd81-4771dddac835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TerminatedWorkerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-24e7ab38772a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_classifier_homicide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_homicide_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_homicide_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ca97542c01a2>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m(df_train_input, df_train_label, cv)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_input_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeGjO1NFVyDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_credit = hyperparameter_tuning(df_credit_train_input, df_credit_train_label, cv = 5)\n",
        "\n",
        "{'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 50, 'reg_lambda': 1.2}\n",
        "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DTSOwjLd9IU",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Curves: Compare diff. algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKTdADBDbp14",
        "colab_type": "text"
      },
      "source": [
        "Selection of classification method:\n",
        "\n",
        "- **Linear** machine learning algorithms often have a high bias but a low variance.\n",
        "    1. Logistic Regression\n",
        "    2. Linear Discriminant Analysis\n",
        "    3. Partial Least Squares Discriminant Analysis\n",
        "- **Nonlinear**machine learning algorithms often have a low bias but a high variance.\n",
        "    1. Nonlinear Discriminant Analysis\n",
        "    2. Neural Networks\n",
        "    3. Flexible Discriminant Analysis\n",
        "    4. Support Vector Machines \n",
        "    5. K-Nearest Neighbors\n",
        "    6. Naive Bayes\n",
        "- Others\n",
        "    1. Basic Classification Trees\n",
        "    2. Random Forest\n",
        "    3. Boosted Trees - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHM2sG6QNt0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define different classification algorithms\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "log_reg = LogisticRegression()\n",
        "svm = SVC(C = 1.0, kernel = \"rbf\")\n",
        "\n",
        "# Store different models in a list\n",
        "models = [random_forest, log_reg, svm]\n",
        "\n",
        "# Define other input arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Plot learning curve for different classifcation algorithms\n",
        "for model in models:\n",
        "  plt.subplot\n",
        "  plot_learning_curve(estimator = model, \n",
        "                      title = f\"{model} Learning Curve\", \n",
        "                      X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                      cv = 10, \n",
        "                      scoring = f1, \n",
        "                      ylim = (0, 1), \n",
        "                      train_sizes = sizes)\n",
        "\n",
        "# Plot different subplots \n",
        "\n",
        "# for model, i in [(RandomForestClassifier(), 1), (KNeighborsClassifier(),2)]:\n",
        "#     plt.subplot(1,2,i)\n",
        "#     learning_curves(estimator = model, \n",
        "#                     data = df_3_adult_dummies, \n",
        "#                     features = df_3_adult_train_input.columns, \n",
        "#                     target= \"Over-50K\", \n",
        "#                     train_sizes = train_sizes, \n",
        "#                     cv= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhsVjAHMqceC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def data_preprocess_fair(full_dataset, \n",
        "#                          outcome_feature, \n",
        "#                          privileged_outcome_label, \n",
        "#                          discriminatory_feature, \n",
        "#                          majority_label,\n",
        "#                          minority_label)\n",
        "\n",
        "\n",
        "## 1) Convert the labels of the target variable into a suitable  format (binary integer)\n",
        "# Encoding Binary \n",
        "df_3_adult[\"Over-50KVariables\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" \n",
        "                                                               else 0)\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "## 2. Prepare data for training (input features and label).\n",
        "# Features of complete dataset\n",
        "# print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "# print(df_3_adult_train_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]\n",
        "# print(df_3_adult_train_label)\n",
        "\n",
        "## 3) Prepare data as such that machine learning classification algorithm can handle that properly (e.g. standardization, normalization, feature scaling, dummy encoding)\n",
        "# Dummy \n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "## 4) Filter dataset based on certain feature values which identify subpopulations and create different dataset versions based on that\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "# print(df_3_adult.shape)\n",
        "# print(df_3_adult_black.shape)\n",
        "# print(df_3_adult_white.shape)\n",
        "# print(df_3_adult_female.shape)\n",
        "# print(df_3_adult_male.shape)\n",
        "\n",
        "\n",
        "  # \"\"\"\n",
        "  #   Convert the target label in an binary number format, create two different datasets sets \n",
        "\n",
        "  #   Parameters\n",
        "  #   ----------\n",
        "  #   estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "  #       An object of that type which is cloned for each validation.\n",
        "\n",
        "  #   title : string\n",
        "  #       Title for the chart.\n",
        "\n",
        "  #   X : array-like, shape (n_samples, n_features)\n",
        "  #       Training vector, where n_samples is the number of samples and\n",
        "  #       n_features is the number of features.\n",
        "\n",
        "  #   y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "  #       Target relative to X for classification or regression;\n",
        "  #       None for unsupervised learning.\n",
        "\n",
        "  #   ylim : tuple, shape (ymin, ymax), optional\n",
        "  #       Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "  # \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxrRXcX700-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "model = xgboost.XGBClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdqX0VzxMCuy"
      },
      "source": [
        "## 1) US Adult Income Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpPHz6cXJhv",
        "colab_type": "text"
      },
      "source": [
        "UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on peoples annual incomes, or various other studies [6]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1N6Co27mxY",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNF_2o9_ALP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "               \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_3_adult = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q76uhuRlwiAs",
        "colab_type": "code",
        "outputId": "10f6f64e-95cf-480a-cf6c-54a64e08e3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Over-50K</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Over-50K</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;=50K</th>\n",
              "      <td>24720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;50K</th>\n",
              "      <td>7841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Over-50K\n",
              "Over-50K          \n",
              "<=50K        24720\n",
              ">50K          7841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxt_BWxnmvKs",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWRpmZKLmy2M",
        "colab_type": "text"
      },
      "source": [
        "Not necessary here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl76RtG5KyC4",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR2CNkgLlA-f",
        "colab_type": "text"
      },
      "source": [
        "1. Sensitive Feature \t\n",
        "  - Features are included which could lead to discrimination (e.g. Race, Sexual Orientation, Color, National Origin, Citizenship, Familiar Status, Pregnancy, Disability Status) and on the basis of which groups in the data can be assigned to the majority or minority group.  \n",
        "\n",
        "2. Binary Target Feature Y  {0, 1}: \n",
        "  - The target variable should reflect allocative harms (= allocation of limited opportunities, resources, or information).\n",
        "\n",
        "3. Total Number of Predictor Features\t\n",
        "\n",
        "  - Number of predictor features on the basis of which a mapping to a target variable is carried out. Partly representative of the complexity of the prediction problem at hand.\n",
        "\n",
        "4. Total Number of Training Examples \t\n",
        "  - Total number of training examples in the dataset.\n",
        "\n",
        "5. Total Number of Training Examples in the Minority Group\n",
        "  - Number of training examples per value of a sensitive attribute.\n",
        "\n",
        "6. Sample Size Disparity\t\n",
        "  - Sample size disparity is represented by the difference between the relative share of the minority and the majority group.\n",
        "\n",
        "7. Class Balance\t\n",
        "  - Percentage of examples with 1 and 0 in the majority group, in the minority groupas well as in the complete dataset.\n",
        "\n",
        "8. Coarseness of Features\t\n",
        "  - Features might be less reliably collected for minority groups. \n",
        "  - Represented by percent of missing values for the subpopulations per feature for the examined training data interval \n",
        "\n",
        "9. Feature Importance\t\n",
        "  - The predictive power of the features represent how much contribution the features have brought in average in order to derive at the final predictions. \n",
        "  - The distribution of the predictive power of the features should be even for different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffuJd_WpxZcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.info)\n",
        "print(df_3_adult.describe())\n",
        "print(df_3_adult.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CierIp3nTNln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_3_adult, disc_feature= \"Race\", disc_min_value=\"Black\", label = \"Over-50K\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvxB8Eg_SFP",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzjd5IvaEJk-",
        "colab_type": "text"
      },
      "source": [
        "###### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HHPEI2vf-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_u077HEWs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY190l6ZkNN3",
        "colab_type": "text"
      },
      "source": [
        "###### 1) Define hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-9BqfcHMLd",
        "colab_type": "text"
      },
      "source": [
        "Get Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8er2vO90UkZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_rf_class_adult = hyperparameter_tuning(df_train_input = df_3_adult_train_input, df_train_label= df_3_adult_train_label)\n",
        "\n",
        "print(grid_rf_class_adult.best_params_)\n",
        "print(grid_rf_class_adult.best_estimator_) # print in order to check actual final hyperparameters\n",
        "\n",
        "cv_results_df = pd.DataFrame(grid_rf_class_adult.cv_results_)\n",
        "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "print(best_row)\n",
        "print(cv_results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AAaPCPTImax",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU8Qe9FuUCHf",
        "colab_type": "text"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzBhMm1Nx23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HYPERPARAMETER TUNING - XGBoost\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "# Important: Keys in the dictionary must be valid hyperparameters \n",
        "param_grid = {\"n_estimators\": [10, 25, 50, 75, 100, 200],\n",
        "              \"max_depth\": [2, 5, 8, 15, 25],        \n",
        "              \"min_samples_split\": [2, 5, 10, 15, 100],\n",
        "              \"min_samples_leaf\": [1, 2, 5, 10]}\n",
        "\n",
        "# Define classifier\n",
        "xgboost_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                        max_features = \"auto\")\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1, \n",
        "                                                     cv = 5,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "\n",
        "# Fit model\n",
        "grid_rf_class.fit(df_3_adult_train_input, df_3_adult_train_label) # To Do: specify dataset variable name\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_) # print in order to check actual final hyperparameters\n",
        "\n",
        "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
        "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "print(best_row)\n",
        "print(cv_results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kjFVSOtljy5",
        "colab_type": "code",
        "outputId": "424f3949-b14b-41e3-86a3-0372c575f9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "# HYPERPARAMETER TUNING - Random Forest\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "# Important: Keys in the dictionary must be valid hyperparameters \n",
        "param_grid = {\"n_estimators\": [10, 25, 50, 75, 100, 200],\n",
        "              \"max_depth\": [2, 5, 8, 15, 25],        \n",
        "              \"min_samples_split\": [2, 5, 10, 15, 100],\n",
        "              \"min_samples_leaf\": [1, 2, 5, 10]}\n",
        "\n",
        "# n_estimators = number of trees in the foreset\n",
        "# max_features = max number of features considered for splitting a node\n",
        "# max_depth = max number of levels in each decision tree\n",
        "# min_samples_split = min number of data points placed in a node before the node is split\n",
        "# min_samples_leaf = min number of data points allowed in a leaf node\n",
        "# bootstrap = method for sampling data points (with or without replacement)\n",
        "\n",
        "# Define classifier\n",
        "rf_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                        max_features = \"auto\")\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1,\n",
        "                                                     n_jobs = 2, \n",
        "                                                     cv = 5,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = True)\n",
        "\n",
        "\n",
        "# Fit model\n",
        "grid_rf_class.fit(df_3_adult_train_input, df_3_adult_train_label) # To Do: specify dataset variable name\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_) # print in order to check actual final hyperparameters\n",
        "\n",
        "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
        "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "print(best_row)\n",
        "print(cv_results_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning:\n",
            "\n",
            "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=25, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=5,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeCn37qQpS-",
        "colab_type": "text"
      },
      "source": [
        "###### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTkqDkW4QrWV",
        "colab_type": "text"
      },
      "source": [
        "First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGE2KGthLrrm",
        "colab_type": "code",
        "outputId": "2617740d-591a-4de4-cd50-1e0cc1a023bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Determine number of training examples for the examples that are at risk of being discriminated\n",
        "print(df_3_adult.groupby(['Race', \"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             Over-50K\n",
            "Race               Over-50K          \n",
            "Amer-Indian-Eskimo 0              275\n",
            "                   1               36\n",
            "Asian-Pac-Islander 0              763\n",
            "                   1              276\n",
            "Black              0             2737\n",
            "                   1              387\n",
            "Other              0              246\n",
            "                   1               25\n",
            "White              0            20699\n",
            "                   1             7117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-GcZxMQEaxl",
        "colab_type": "code",
        "outputId": "3ec88599-1a06-4616-ee22-c534f3789c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Define arguments\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "df_3_adult_black = df_3_adult[is_black]  # Minority\n",
        "df_3_adult_white = df_3_adult[is_white]  # Majority\n",
        "\n",
        "# Execute function\n",
        "list_dfs_adult = create_datasets(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "[27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyn0FoHBJmAm",
        "colab_type": "code",
        "outputId": "06f6e988-ba95-49c1-cba5-bea9938ae245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "df_test = list_dfs_adult[15]\n",
        "print(df_test.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Over-50K\n",
            "Over-50K          \n",
            "0            21046\n",
            "1             7170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkzaawcuCKZ",
        "colab_type": "text"
      },
      "source": [
        "###### 3) Create dataframe with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBSkMj5qSUX_",
        "colab_type": "code",
        "outputId": "10c5f20f-36c3-4a09-fa6a-50d6ebb38cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define arguments\n",
        "list_dfs = list_dfs_adult\n",
        "label = \"Over-50K\"\n",
        "# model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                               criterion='gini', max_depth=25, max_features='auto',\n",
        "#                               max_leaf_nodes=None, max_samples=None,\n",
        "#                               min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                               min_samples_leaf=1, min_samples_split=5,\n",
        "#                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "#                               n_jobs=None, oob_score=False, random_state=None,\n",
        "#                               verbose=0, warm_start=False)\n",
        "\n",
        "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "              nthread=None, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "cv = 10\n",
        "discr_feature = \"Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Execute function\n",
        "results_df_adult = metrics_to_df(list_dfs = list_dfs_adult, label = label, model = model, cv = cv, \n",
        "                           discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_adult"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27821</td>\n",
              "      <td>5</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682611</td>\n",
              "      <td>0.690950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.682611</td>\n",
              "      <td>0.599550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.599550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.193594</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.326781</td>\n",
              "      <td>-0.193594</td>\n",
              "      <td>-0.599550</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27826</td>\n",
              "      <td>10</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684240</td>\n",
              "      <td>0.691339</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684240</td>\n",
              "      <td>0.601799</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.601799</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054061</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.194205</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.327930</td>\n",
              "      <td>-0.194205</td>\n",
              "      <td>-0.601799</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27836</td>\n",
              "      <td>20</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682545</td>\n",
              "      <td>0.691281</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.682708</td>\n",
              "      <td>0.599719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.599972</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054206</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.193845</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.327089</td>\n",
              "      <td>-0.193845</td>\n",
              "      <td>-0.599972</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27846</td>\n",
              "      <td>30</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684362</td>\n",
              "      <td>0.690884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684581</td>\n",
              "      <td>0.601039</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.601377</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.193666</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.327429</td>\n",
              "      <td>-0.193666</td>\n",
              "      <td>-0.601377</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27856</td>\n",
              "      <td>40</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682123</td>\n",
              "      <td>0.690729</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.682396</td>\n",
              "      <td>0.598989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.599410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.193630</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.326759</td>\n",
              "      <td>-0.193630</td>\n",
              "      <td>-0.599410</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27866</td>\n",
              "      <td>50</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683581</td>\n",
              "      <td>0.690859</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.683912</td>\n",
              "      <td>0.601348</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.601799</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.194420</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.245018</td>\n",
              "      <td>-0.154420</td>\n",
              "      <td>-0.458941</td>\n",
              "      <td>0.205740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27891</td>\n",
              "      <td>75</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683530</td>\n",
              "      <td>0.691310</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.683819</td>\n",
              "      <td>0.600870</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.601236</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.054061</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.194061</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.130205</td>\n",
              "      <td>-0.114061</td>\n",
              "      <td>-0.237600</td>\n",
              "      <td>0.412242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>27916</td>\n",
              "      <td>100</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684169</td>\n",
              "      <td>0.690426</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.684627</td>\n",
              "      <td>0.601009</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.601658</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.053626</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.193845</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>0.159487</td>\n",
              "      <td>-0.123845</td>\n",
              "      <td>-0.289158</td>\n",
              "      <td>0.361113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27941</td>\n",
              "      <td>125</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683754</td>\n",
              "      <td>0.690442</td>\n",
              "      <td>0.516129</td>\n",
              "      <td>0.684169</td>\n",
              "      <td>0.601709</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.602361</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.054495</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.194672</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.128336</td>\n",
              "      <td>-0.114672</td>\n",
              "      <td>-0.221408</td>\n",
              "      <td>0.410947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>27966</td>\n",
              "      <td>150</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682760</td>\n",
              "      <td>0.691110</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.683281</td>\n",
              "      <td>0.600224</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.600955</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.073333</td>\n",
              "      <td>0.194205</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>0.134116</td>\n",
              "      <td>-0.120871</td>\n",
              "      <td>-0.237319</td>\n",
              "      <td>0.377608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>27991</td>\n",
              "      <td>175</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683748</td>\n",
              "      <td>0.690676</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.684328</td>\n",
              "      <td>0.599860</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>0.600674</td>\n",
              "      <td>0.019737</td>\n",
              "      <td>0.053239</td>\n",
              "      <td>0.062857</td>\n",
              "      <td>0.193306</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.143175</td>\n",
              "      <td>-0.130449</td>\n",
              "      <td>-0.252848</td>\n",
              "      <td>0.325169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>28016</td>\n",
              "      <td>200</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684517</td>\n",
              "      <td>0.690572</td>\n",
              "      <td>0.487805</td>\n",
              "      <td>0.685161</td>\n",
              "      <td>0.601204</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.602080</td>\n",
              "      <td>0.023121</td>\n",
              "      <td>0.053433</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.193809</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.131010</td>\n",
              "      <td>-0.123809</td>\n",
              "      <td>-0.231709</td>\n",
              "      <td>0.361180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>28066</td>\n",
              "      <td>250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683532</td>\n",
              "      <td>0.690008</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.684345</td>\n",
              "      <td>0.600755</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.601939</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.054061</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.194241</td>\n",
              "      <td>0.008988</td>\n",
              "      <td>0.142270</td>\n",
              "      <td>-0.130241</td>\n",
              "      <td>-0.248998</td>\n",
              "      <td>0.329488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>28116</td>\n",
              "      <td>300</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682143</td>\n",
              "      <td>0.690276</td>\n",
              "      <td>0.474576</td>\n",
              "      <td>0.683122</td>\n",
              "      <td>0.599329</td>\n",
              "      <td>0.341463</td>\n",
              "      <td>0.600815</td>\n",
              "      <td>0.015444</td>\n",
              "      <td>0.054399</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.194205</td>\n",
              "      <td>0.010785</td>\n",
              "      <td>0.149153</td>\n",
              "      <td>-0.134205</td>\n",
              "      <td>-0.259352</td>\n",
              "      <td>0.308952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>28166</td>\n",
              "      <td>350</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682215</td>\n",
              "      <td>0.690720</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.683262</td>\n",
              "      <td>0.600140</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.601658</td>\n",
              "      <td>0.019868</td>\n",
              "      <td>0.054834</td>\n",
              "      <td>0.068571</td>\n",
              "      <td>0.194744</td>\n",
              "      <td>0.012583</td>\n",
              "      <td>0.130812</td>\n",
              "      <td>-0.126173</td>\n",
              "      <td>-0.226658</td>\n",
              "      <td>0.352111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>28216</td>\n",
              "      <td>400</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683345</td>\n",
              "      <td>0.690909</td>\n",
              "      <td>0.506329</td>\n",
              "      <td>0.684463</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.601658</td>\n",
              "      <td>0.017291</td>\n",
              "      <td>0.053771</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.193953</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.130390</td>\n",
              "      <td>-0.128953</td>\n",
              "      <td>-0.224300</td>\n",
              "      <td>0.335133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28266</td>\n",
              "      <td>450</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684536</td>\n",
              "      <td>0.691230</td>\n",
              "      <td>0.534884</td>\n",
              "      <td>0.685564</td>\n",
              "      <td>0.601533</td>\n",
              "      <td>0.396552</td>\n",
              "      <td>0.603204</td>\n",
              "      <td>0.012755</td>\n",
              "      <td>0.053819</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.194385</td>\n",
              "      <td>0.016178</td>\n",
              "      <td>0.123858</td>\n",
              "      <td>-0.132162</td>\n",
              "      <td>-0.206652</td>\n",
              "      <td>0.320099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>28316</td>\n",
              "      <td>500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682679</td>\n",
              "      <td>0.690441</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.683710</td>\n",
              "      <td>0.599554</td>\n",
              "      <td>0.415385</td>\n",
              "      <td>0.601236</td>\n",
              "      <td>0.013793</td>\n",
              "      <td>0.054157</td>\n",
              "      <td>0.066000</td>\n",
              "      <td>0.194133</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>0.113108</td>\n",
              "      <td>-0.128133</td>\n",
              "      <td>-0.185852</td>\n",
              "      <td>0.339973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28416</td>\n",
              "      <td>600</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.681977</td>\n",
              "      <td>0.690600</td>\n",
              "      <td>0.593220</td>\n",
              "      <td>0.682814</td>\n",
              "      <td>0.598638</td>\n",
              "      <td>0.460526</td>\n",
              "      <td>0.600112</td>\n",
              "      <td>0.013359</td>\n",
              "      <td>0.054206</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.193881</td>\n",
              "      <td>0.021570</td>\n",
              "      <td>0.090216</td>\n",
              "      <td>-0.123881</td>\n",
              "      <td>-0.139586</td>\n",
              "      <td>0.361046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28516</td>\n",
              "      <td>700</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.682537</td>\n",
              "      <td>0.690560</td>\n",
              "      <td>0.609929</td>\n",
              "      <td>0.683356</td>\n",
              "      <td>0.598723</td>\n",
              "      <td>0.477778</td>\n",
              "      <td>0.600253</td>\n",
              "      <td>0.013115</td>\n",
              "      <td>0.053819</td>\n",
              "      <td>0.072857</td>\n",
              "      <td>0.193630</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>0.081590</td>\n",
              "      <td>-0.120772</td>\n",
              "      <td>-0.122475</td>\n",
              "      <td>0.376271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>28616</td>\n",
              "      <td>800</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683908</td>\n",
              "      <td>0.690566</td>\n",
              "      <td>0.601227</td>\n",
              "      <td>0.684984</td>\n",
              "      <td>0.600526</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.602501</td>\n",
              "      <td>0.012950</td>\n",
              "      <td>0.053867</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.194241</td>\n",
              "      <td>0.028760</td>\n",
              "      <td>0.088376</td>\n",
              "      <td>-0.121741</td>\n",
              "      <td>-0.135834</td>\n",
              "      <td>0.373248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>28716</td>\n",
              "      <td>900</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683381</td>\n",
              "      <td>0.689916</td>\n",
              "      <td>0.618785</td>\n",
              "      <td>0.684316</td>\n",
              "      <td>0.599198</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.601096</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>0.053529</td>\n",
              "      <td>0.072222</td>\n",
              "      <td>0.193630</td>\n",
              "      <td>0.032355</td>\n",
              "      <td>0.080193</td>\n",
              "      <td>-0.121407</td>\n",
              "      <td>-0.118337</td>\n",
              "      <td>0.372992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>28816</td>\n",
              "      <td>1000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.684244</td>\n",
              "      <td>0.690684</td>\n",
              "      <td>0.610837</td>\n",
              "      <td>0.685432</td>\n",
              "      <td>0.601462</td>\n",
              "      <td>0.469697</td>\n",
              "      <td>0.603906</td>\n",
              "      <td>0.010369</td>\n",
              "      <td>0.054399</td>\n",
              "      <td>0.071000</td>\n",
              "      <td>0.194996</td>\n",
              "      <td>0.035951</td>\n",
              "      <td>0.089120</td>\n",
              "      <td>-0.123996</td>\n",
              "      <td>-0.134209</td>\n",
              "      <td>0.364111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>29066</td>\n",
              "      <td>1250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.683404</td>\n",
              "      <td>0.689698</td>\n",
              "      <td>0.612403</td>\n",
              "      <td>0.684866</td>\n",
              "      <td>0.600137</td>\n",
              "      <td>0.484663</td>\n",
              "      <td>0.602782</td>\n",
              "      <td>0.014719</td>\n",
              "      <td>0.054157</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.194528</td>\n",
              "      <td>0.044938</td>\n",
              "      <td>0.078779</td>\n",
              "      <td>-0.118528</td>\n",
              "      <td>-0.118119</td>\n",
              "      <td>0.390689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>29316</td>\n",
              "      <td>1500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.681173</td>\n",
              "      <td>0.689392</td>\n",
              "      <td>0.616393</td>\n",
              "      <td>0.682751</td>\n",
              "      <td>0.597646</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.600393</td>\n",
              "      <td>0.016031</td>\n",
              "      <td>0.054447</td>\n",
              "      <td>0.076667</td>\n",
              "      <td>0.194133</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>0.072037</td>\n",
              "      <td>-0.117466</td>\n",
              "      <td>-0.105657</td>\n",
              "      <td>0.394919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>29566</td>\n",
              "      <td>1750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.681599</td>\n",
              "      <td>0.688525</td>\n",
              "      <td>0.603989</td>\n",
              "      <td>0.683777</td>\n",
              "      <td>0.597654</td>\n",
              "      <td>0.493023</td>\n",
              "      <td>0.600815</td>\n",
              "      <td>0.019544</td>\n",
              "      <td>0.053819</td>\n",
              "      <td>0.077714</td>\n",
              "      <td>0.193773</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.071033</td>\n",
              "      <td>-0.116059</td>\n",
              "      <td>-0.107792</td>\n",
              "      <td>0.401058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>29816</td>\n",
              "      <td>2000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.679913</td>\n",
              "      <td>0.688087</td>\n",
              "      <td>0.611529</td>\n",
              "      <td>0.682099</td>\n",
              "      <td>0.595190</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.598286</td>\n",
              "      <td>0.019909</td>\n",
              "      <td>0.053626</td>\n",
              "      <td>0.078500</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.071901</td>\n",
              "      <td>0.063935</td>\n",
              "      <td>-0.114482</td>\n",
              "      <td>-0.094154</td>\n",
              "      <td>0.406773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>30066</td>\n",
              "      <td>2250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.678307</td>\n",
              "      <td>0.686837</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.681491</td>\n",
              "      <td>0.593966</td>\n",
              "      <td>0.474453</td>\n",
              "      <td>0.598567</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.074667</td>\n",
              "      <td>0.193594</td>\n",
              "      <td>0.080889</td>\n",
              "      <td>0.079617</td>\n",
              "      <td>-0.118927</td>\n",
              "      <td>-0.124114</td>\n",
              "      <td>0.385688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30316</td>\n",
              "      <td>2500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.681874</td>\n",
              "      <td>0.686861</td>\n",
              "      <td>0.606786</td>\n",
              "      <td>0.684879</td>\n",
              "      <td>0.598167</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.602361</td>\n",
              "      <td>0.020492</td>\n",
              "      <td>0.053867</td>\n",
              "      <td>0.078800</td>\n",
              "      <td>0.194205</td>\n",
              "      <td>0.089876</td>\n",
              "      <td>0.067868</td>\n",
              "      <td>-0.115405</td>\n",
              "      <td>-0.102361</td>\n",
              "      <td>0.405757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30566</td>\n",
              "      <td>2750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.680254</td>\n",
              "      <td>0.686435</td>\n",
              "      <td>0.622540</td>\n",
              "      <td>0.682837</td>\n",
              "      <td>0.595439</td>\n",
              "      <td>0.514793</td>\n",
              "      <td>0.599269</td>\n",
              "      <td>0.019486</td>\n",
              "      <td>0.053626</td>\n",
              "      <td>0.080364</td>\n",
              "      <td>0.193234</td>\n",
              "      <td>0.098864</td>\n",
              "      <td>0.059308</td>\n",
              "      <td>-0.112870</td>\n",
              "      <td>-0.084476</td>\n",
              "      <td>0.415887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30816</td>\n",
              "      <td>3000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.675725</td>\n",
              "      <td>0.685701</td>\n",
              "      <td>0.476954</td>\n",
              "      <td>0.683643</td>\n",
              "      <td>0.587976</td>\n",
              "      <td>0.323370</td>\n",
              "      <td>0.601658</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>0.054495</td>\n",
              "      <td>0.043667</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.107852</td>\n",
              "      <td>0.164112</td>\n",
              "      <td>-0.150826</td>\n",
              "      <td>-0.278288</td>\n",
              "      <td>0.224516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  rows_majority  f1_complete  \\\n",
              "0           27821              5          27816     0.682611   \n",
              "1           27826             10          27816     0.684240   \n",
              "2           27836             20          27816     0.682545   \n",
              "3           27846             30          27816     0.684362   \n",
              "4           27856             40          27816     0.682123   \n",
              "5           27866             50          27816     0.683581   \n",
              "6           27891             75          27816     0.683530   \n",
              "7           27916            100          27816     0.684169   \n",
              "8           27941            125          27816     0.683754   \n",
              "9           27966            150          27816     0.682760   \n",
              "10          27991            175          27816     0.683748   \n",
              "11          28016            200          27816     0.684517   \n",
              "12          28066            250          27816     0.683532   \n",
              "13          28116            300          27816     0.682143   \n",
              "14          28166            350          27816     0.682215   \n",
              "15          28216            400          27816     0.683345   \n",
              "16          28266            450          27816     0.684536   \n",
              "17          28316            500          27816     0.682679   \n",
              "18          28416            600          27816     0.681977   \n",
              "19          28516            700          27816     0.682537   \n",
              "20          28616            800          27816     0.683908   \n",
              "21          28716            900          27816     0.683381   \n",
              "22          28816           1000          27816     0.684244   \n",
              "23          29066           1250          27816     0.683404   \n",
              "24          29316           1500          27816     0.681173   \n",
              "25          29566           1750          27816     0.681599   \n",
              "26          29816           2000          27816     0.679913   \n",
              "27          30066           2250          27816     0.678307   \n",
              "28          30316           2500          27816     0.681874   \n",
              "29          30566           2750          27816     0.680254   \n",
              "30          30816           3000          27816     0.675725   \n",
              "\n",
              "    f1_complete_train  f1_minority  f1_majority  tpr_complete  tpr_minority  \\\n",
              "0            0.690950     0.000000     0.682611      0.599550      0.000000   \n",
              "1            0.691339     0.000000     0.684240      0.601799      0.000000   \n",
              "2            0.691281     0.000000     0.682708      0.599719      0.000000   \n",
              "3            0.690884     0.000000     0.684581      0.601039      0.000000   \n",
              "4            0.690729     0.000000     0.682396      0.598989      0.000000   \n",
              "5            0.690859     0.222222     0.683912      0.601348      0.142857   \n",
              "6            0.691310     0.470588     0.683819      0.600870      0.363636   \n",
              "7            0.690426     0.434783     0.684627      0.601009      0.312500   \n",
              "8            0.690442     0.516129     0.684169      0.601709      0.380952   \n",
              "9            0.691110     0.484848     0.683281      0.600224      0.363636   \n",
              "10           0.690676     0.470588     0.684328      0.599860      0.347826   \n",
              "11           0.690572     0.487805     0.685161      0.601204      0.370370   \n",
              "12           0.690008     0.480000     0.684345      0.600755      0.352941   \n",
              "13           0.690276     0.474576     0.683122      0.599329      0.341463   \n",
              "14           0.690720     0.500000     0.683262      0.600140      0.375000   \n",
              "15           0.690909     0.506329     0.684463      0.600000      0.377358   \n",
              "16           0.691230     0.534884     0.685564      0.601533      0.396552   \n",
              "17           0.690441     0.551020     0.683710      0.599554      0.415385   \n",
              "18           0.690600     0.593220     0.682814      0.598638      0.460526   \n",
              "19           0.690560     0.609929     0.683356      0.598723      0.477778   \n",
              "20           0.690566     0.601227     0.684984      0.600526      0.466667   \n",
              "21           0.689916     0.618785     0.684316      0.599198      0.482759   \n",
              "22           0.690684     0.610837     0.685432      0.601462      0.469697   \n",
              "23           0.689698     0.612403     0.684866      0.600137      0.484663   \n",
              "24           0.689392     0.616393     0.682751      0.597646      0.494737   \n",
              "25           0.688525     0.603989     0.683777      0.597654      0.493023   \n",
              "26           0.688087     0.611529     0.682099      0.595190      0.504132   \n",
              "27           0.686837     0.588235     0.681491      0.593966      0.474453   \n",
              "28           0.686861     0.606786     0.684879      0.598167      0.500000   \n",
              "29           0.686435     0.622540     0.682837      0.595439      0.514793   \n",
              "30           0.685701     0.476954     0.683643      0.587976      0.323370   \n",
              "\n",
              "    tpr_majority  fpr_minority  fpr_majority  prob_yhat_1_minority  \\\n",
              "0       0.599550      0.000000      0.054012              0.000000   \n",
              "1       0.601799      0.000000      0.054061              0.000000   \n",
              "2       0.599972      0.000000      0.054206              0.000000   \n",
              "3       0.601377      0.000000      0.053481              0.000000   \n",
              "4       0.599410      0.000000      0.054109              0.000000   \n",
              "5       0.601799      0.023256      0.054350              0.040000   \n",
              "6       0.601236      0.031250      0.054061              0.080000   \n",
              "7       0.601658      0.023810      0.053626              0.070000   \n",
              "8       0.602361      0.019231      0.054495              0.080000   \n",
              "9       0.600955      0.023438      0.054350              0.073333   \n",
              "10      0.600674      0.019737      0.053239              0.062857   \n",
              "11      0.602080      0.023121      0.053433              0.070000   \n",
              "12      0.601939      0.018519      0.054061              0.064000   \n",
              "13      0.600815      0.015444      0.054399              0.060000   \n",
              "14      0.601658      0.019868      0.054834              0.068571   \n",
              "15      0.601658      0.017291      0.053771              0.065000   \n",
              "16      0.603204      0.012755      0.053819              0.062222   \n",
              "17      0.601236      0.013793      0.054157              0.066000   \n",
              "18      0.600112      0.013359      0.054206              0.070000   \n",
              "19      0.600253      0.013115      0.053819              0.072857   \n",
              "20      0.602501      0.012950      0.053867              0.072500   \n",
              "21      0.601096      0.011480      0.053529              0.072222   \n",
              "22      0.603906      0.010369      0.054399              0.071000   \n",
              "23      0.602782      0.014719      0.054157              0.076000   \n",
              "24      0.600393      0.016031      0.054447              0.076667   \n",
              "25      0.600815      0.019544      0.053819              0.077714   \n",
              "26      0.598286      0.019909      0.053626              0.078500   \n",
              "27      0.598567      0.019231      0.054350              0.074667   \n",
              "28      0.602361      0.020492      0.053867              0.078800   \n",
              "29      0.599269      0.019486      0.053626              0.080364   \n",
              "30      0.601658      0.004559      0.054495              0.043667   \n",
              "\n",
              "    prob_yhat_1_majority  rel_share_min_of_maj  aver_abs_odds_diff  \\\n",
              "0               0.193594              0.000180            0.326781   \n",
              "1               0.194205              0.000360            0.327930   \n",
              "2               0.193845              0.000719            0.327089   \n",
              "3               0.193666              0.001079            0.327429   \n",
              "4               0.193630              0.001438            0.326759   \n",
              "5               0.194420              0.001798            0.245018   \n",
              "6               0.194061              0.002696            0.130205   \n",
              "7               0.193845              0.003595            0.159487   \n",
              "8               0.194672              0.004494            0.128336   \n",
              "9               0.194205              0.005393            0.134116   \n",
              "10              0.193306              0.006291            0.143175   \n",
              "11              0.193809              0.007190            0.131010   \n",
              "12              0.194241              0.008988            0.142270   \n",
              "13              0.194205              0.010785            0.149153   \n",
              "14              0.194744              0.012583            0.130812   \n",
              "15              0.193953              0.014380            0.130390   \n",
              "16              0.194385              0.016178            0.123858   \n",
              "17              0.194133              0.017975            0.113108   \n",
              "18              0.193881              0.021570            0.090216   \n",
              "19              0.193630              0.025165            0.081590   \n",
              "20              0.194241              0.028760            0.088376   \n",
              "21              0.193630              0.032355            0.080193   \n",
              "22              0.194996              0.035951            0.089120   \n",
              "23              0.194528              0.044938            0.078779   \n",
              "24              0.194133              0.053926            0.072037   \n",
              "25              0.193773              0.062913            0.071033   \n",
              "26              0.192982              0.071901            0.063935   \n",
              "27              0.193594              0.080889            0.079617   \n",
              "28              0.194205              0.089876            0.067868   \n",
              "29              0.193234              0.098864            0.059308   \n",
              "30              0.194492              0.107852            0.164112   \n",
              "\n",
              "    stat_parity_diff  equal_opport_dist  disparate_impact  \n",
              "0          -0.193594          -0.599550          0.000000  \n",
              "1          -0.194205          -0.601799          0.000000  \n",
              "2          -0.193845          -0.599972          0.000000  \n",
              "3          -0.193666          -0.601377          0.000000  \n",
              "4          -0.193630          -0.599410          0.000000  \n",
              "5          -0.154420          -0.458941          0.205740  \n",
              "6          -0.114061          -0.237600          0.412242  \n",
              "7          -0.123845          -0.289158          0.361113  \n",
              "8          -0.114672          -0.221408          0.410947  \n",
              "9          -0.120871          -0.237319          0.377608  \n",
              "10         -0.130449          -0.252848          0.325169  \n",
              "11         -0.123809          -0.231709          0.361180  \n",
              "12         -0.130241          -0.248998          0.329488  \n",
              "13         -0.134205          -0.259352          0.308952  \n",
              "14         -0.126173          -0.226658          0.352111  \n",
              "15         -0.128953          -0.224300          0.335133  \n",
              "16         -0.132162          -0.206652          0.320099  \n",
              "17         -0.128133          -0.185852          0.339973  \n",
              "18         -0.123881          -0.139586          0.361046  \n",
              "19         -0.120772          -0.122475          0.376271  \n",
              "20         -0.121741          -0.135834          0.373248  \n",
              "21         -0.121407          -0.118337          0.372992  \n",
              "22         -0.123996          -0.134209          0.364111  \n",
              "23         -0.118528          -0.118119          0.390689  \n",
              "24         -0.117466          -0.105657          0.394919  \n",
              "25         -0.116059          -0.107792          0.401058  \n",
              "26         -0.114482          -0.094154          0.406773  \n",
              "27         -0.118927          -0.124114          0.385688  \n",
              "28         -0.115405          -0.102361          0.405757  \n",
              "29         -0.112870          -0.084476          0.415887  \n",
              "30         -0.150826          -0.278288          0.224516  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbV0p2PNhG4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "\n",
        "results_df_adult.to_csv(\"df_adult_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_adult_metrics.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbXQvAkgi8vN",
        "colab_type": "text"
      },
      "source": [
        "###### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk_dZBxdzJro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L8M_z7m_5EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u0QSdisyQ9O",
        "colab_type": "text"
      },
      "source": [
        "Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b85h5SQhwdv",
        "colab_type": "code",
        "outputId": "f87c7733-3eee-4f0f-8b0f-5c6a416e1a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "min_metrics_line_chart(metric_df = results_df_adult, title=\"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b699132b-7783-4b9d-ae5e-1a2ca500da84\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b699132b-7783-4b9d-ae5e-1a2ca500da84\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b699132b-7783-4b9d-ae5e-1a2ca500da84',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.3267813217360542, 0.32792954662262236, 0.3270887077232432, 0.3274289145848995, 0.3267593789280469, 0.24501800275520716, 0.1302053475218777, 0.15948712859036918, 0.1283363906204608, 0.13411602341849663, 0.14317539966921974, 0.13101015160498267, 0.1422699534487097, 0.14915314163327353, 0.13081200956861225, 0.130389579339852, 0.12385789795807225, 0.11310798143624734, 0.09021641483617, 0.0815897043759216, 0.0883760417233408, 0.08019345543673981, 0.08911963508073041, 0.07877864442440707, 0.07203656123854496, 0.0710333727188929, 0.06393517735438485, 0.07961696979813886, 0.06786803924057411, 0.05930816543296586, 0.1641122762417779]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.19359361518550475, -0.1942047742306586, -0.19384526890997986, -0.19366551624964048, -0.19362956571757262, -0.15442047742306586, -0.11406097210238712, -0.12384526890997985, -0.11467213114754098, -0.12087144089732527, -0.13044886807181888, -0.12380931837791198, -0.1302407247627265, -0.1342047742306586, -0.12617260364024818, -0.1289531205061835, -0.13216230466877577, -0.12813287316652286, -0.12388121944204775, -0.12077242286042976, -0.1217407247627265, -0.1214073434953504, -0.12399568593615186, -0.11852832901926948, -0.1174662064998562, -0.11605908213155841, -0.11448245614035087, -0.11892694851883807, -0.1154047742306586, -0.11287047350118964, -0.15082571182053495]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [-0.5995503723478994, -0.6017985106084024, -0.5999718982717437, -0.6013769846845581, -0.599409863706618, -0.45894136775125954, -0.23760011240691303, -0.28915800196712094, -0.22140816422114729, -0.23731909512435012, -0.2528483545216291, -0.23170915752059495, -0.2489978427790956, -0.25935153548528594, -0.22665800196712094, -0.22429951140108317, -0.2066518728832858, -0.18585186065866127, -0.13958609112355147, -0.1224751377765288, -0.1358343871481429, -0.11833734671234003, -0.1342091705306543, -0.11811949441025593, -0.10565658209032491, -0.1077916943054788, -0.09415356317140777, -0.12411425711440383, -0.10236054517352822, -0.08447645565705253, -0.27828843674972964]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.2057396449704142, 0.4122415709522045, 0.361112759643917, 0.4109473684210527, 0.3776082932247316, 0.3251691065118627, 0.3611797440178075, 0.3294880621876735, 0.308952239911144, 0.35211055143859277, 0.33513253012048194, 0.3200986375685839, 0.33997333333333335, 0.36104580011125537, 0.3762707548671158, 0.37324819544697385, 0.3729917068944176, 0.3641106194690265, 0.3906885973017927, 0.3949185185185185, 0.4010576199310893, 0.4067727272727273, 0.3856876508820799, 0.4057572750833025, 0.41588742494714587, 0.22451608133086876]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \" Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b699132b-7783-4b9d-ae5e-1a2ca500da84');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrPVof-byOTQ",
        "colab_type": "text"
      },
      "source": [
        "Majority <-> Minority Line Chart "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridl2ll4YxAP",
        "colab_type": "code",
        "outputId": "8d8b8949-7c9d-4617-b82a-a9057b2f828a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "maj_min_metrics_line_chart(metric_df=results_df_adult, title=\"Majority and Minority Metrics for the Adult Dataset with XGBoost\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"28fd47ae-efdd-4002-9250-83c9500b1bd5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"28fd47ae-efdd-4002-9250-83c9500b1bd5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '28fd47ae-efdd-4002-9250-83c9500b1bd5',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.6826107822748361, 0.6842399552679926, 0.6827084499160604, 0.6845809341010877, 0.6823962249060226, 0.6839121756487027, 0.68381941669996, 0.6846270685106723, 0.6841685285668688, 0.6832814122533748, 0.6843284776692812, 0.685161496642149, 0.6843450479233226, 0.6831216550842718, 0.6832615286420933, 0.684462915601023, 0.6855637176620888, 0.6837101541903012, 0.6828137490007994, 0.6833559945613052, 0.6849840255591054, 0.684315764216588, 0.6854317837493024, 0.6848659003831418, 0.6827514580170968, 0.6837770848324938, 0.6820985182218663, 0.6814909614461686, 0.6848789839444045, 0.6828370156900416, 0.6836433304063224]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4705882352941177, 0.43478260869565216, 0.5161290322580645, 0.4848484848484849, 0.4705882352941176, 0.4878048780487805, 0.48, 0.4745762711864407, 0.5, 0.5063291139240507, 0.5348837209302326, 0.5510204081632654, 0.5932203389830509, 0.6099290780141844, 0.6012269938650308, 0.6187845303867403, 0.6108374384236452, 0.6124031007751938, 0.6163934426229508, 0.6039886039886041, 0.6115288220551378, 0.588235294117647, 0.6067864271457085, 0.6225402504472272, 0.4769539078156313]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.5995503723478994, 0.6017985106084024, 0.5999718982717437, 0.6013769846845581, 0.599409863706618, 0.6017985106084024, 0.6012364760432767, 0.6016580019671209, 0.6023605451735282, 0.6009554587607138, 0.6006744414781509, 0.6020795278909653, 0.6019390192496838, 0.6008149501194323, 0.6016580019671209, 0.6016580019671209, 0.6032035970212168, 0.6012364760432767, 0.6001124069130251, 0.6002529155543066, 0.6025010538148096, 0.6010959674019952, 0.603906140227624, 0.6027820710973725, 0.600393424195588, 0.6008149501194323, 0.5982857945763664, 0.5985668118589293, 0.6023605451735282, 0.5992693550653365, 0.6016580019671209]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.36363636363636365, 0.3125, 0.38095238095238093, 0.36363636363636365, 0.34782608695652173, 0.37037037037037035, 0.35294117647058826, 0.34146341463414637, 0.375, 0.37735849056603776, 0.39655172413793105, 0.4153846153846154, 0.4605263157894737, 0.4777777777777778, 0.4666666666666667, 0.4827586206896552, 0.4696969696969697, 0.48466257668711654, 0.49473684210526314, 0.4930232558139535, 0.5041322314049587, 0.4744525547445255, 0.5, 0.514792899408284, 0.3233695652173913]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0540122711242089, 0.05406058263684236, 0.05420551717474274, 0.05348084448524083, 0.05410889414947582, 0.054350451712643126, 0.05406058263684236, 0.053625779023141217, 0.0544953862505435, 0.054350451712643126, 0.05323928692207353, 0.053432532972607374, 0.05406058263684236, 0.054398763225276585, 0.05483356683897773, 0.05377071356104159, 0.05381902507367506, 0.054157205662109284, 0.05420551717474274, 0.05381902507367506, 0.05386733658630852, 0.05352915599787429, 0.054398763225276585, 0.054157205662109284, 0.054447074737910044, 0.05381902507367506, 0.053625779023141217, 0.054350451712643126, 0.05386733658630852, 0.053625779023141217, 0.0544953862505435]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.03125, 0.023809523809523808, 0.019230769230769232, 0.0234375, 0.019736842105263157, 0.023121387283236993, 0.018518518518518517, 0.015444015444015444, 0.019867549668874173, 0.01729106628242075, 0.012755102040816327, 0.013793103448275862, 0.013358778625954198, 0.013114754098360656, 0.012949640287769784, 0.011479591836734694, 0.010368663594470046, 0.014719411223551058, 0.01603053435114504, 0.019543973941368076, 0.019908987485779295, 0.019230769230769232, 0.020491803278688523, 0.019485903814262025, 0.004559270516717325]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('28fd47ae-efdd-4002-9250-83c9500b1bd5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssH5Y-F0Lsqt"
      },
      "source": [
        "## 2) COMPAS Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-gROLSu6ag",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ7OVi5Y-MvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_compas = \"/content/drive/My Drive/Master Thesis/Data/compas-scores-two-years_dataset.csv\"\n",
        "compas_column_names = ['id', 'name', 'first', 'last', 'compas screening date', 'sex', 'dob',\n",
        "                       'age', 'age cat', 'race', 'juv fel count', 'decile score',\n",
        "                       'juv misd count', 'juv other count', 'priors count',\n",
        "                       'days b screening arrest', 'c jail in', 'c jail out', 'c case number',\n",
        "                       'c offense date', 'c arrest date', 'c days from compas',\n",
        "                       'c charge degree', 'c charge desc', 'is recid', 'r case number',\n",
        "                       'r charge degree', 'r days from arrest', 'r offense date',\n",
        "                       'r charge desc', 'r jail in', 'r jail out', 'violent recid',\n",
        "                       'is violent recid', 'vr case number', 'vr charge degree',\n",
        "                       'vr offense date', 'vr charge desc', 'type of assessment',\n",
        "                       'decile score.1', 'score text', 'screening date',\n",
        "                       'v type of assessment', 'v decile score', 'v score text',\n",
        "                       'v screening date', 'in custody', 'out custody', 'priors count.1',\n",
        "                       'start', 'end', 'event', 'two year recid']\n",
        "df_compas = pd.read_csv(path_compas, low_memory=False, names = compas_column_names, header = 0, sep = \";\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6DV7Qcm1yV",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h69rOkMDjAan",
        "colab_type": "code",
        "outputId": "0501fd42-64ef-46f5-df02-97b7d5634ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Rename columns\n",
        "df_compas = df_compas.rename(index=str, columns={\"decile score.1\":\"decile_score a\",\n",
        "                                                 \"priors count.1\":\"priors_count a\"})\n",
        "\n",
        "# Replace empty strings with NAs\n",
        "df_compas = df_compas.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_compas = df_compas.drop(['id', 'name', 'age cat', \"is recid\", \"event\", \"start\", \"end\"], axis=1)\n",
        "\n",
        "# Drop NaN from label column \n",
        "df_compas = df_compas[df_compas['two year recid'].notna()]\n",
        "print(df_compas.shape)\n",
        "\n",
        "# Convert columns to datetime format\n",
        "# Columns with yyyy-mm-dd format:\n",
        "# date_columns = [\"dob\", \"c_offense_date\", \"c_arrest_date\", \"r_offense_date\", \"r_jail_out\", \"vr_offense_date\", \"screening_date\", \"v_screening_date\", \"v_screening_date\", \"in_custody\", \"out_custody\"]\n",
        "# for date_time in date_columns:\n",
        "#   df_compas[date_time]= pd.to_datetime(df_compas[date_time])\n",
        "\n",
        "# To Do: Columns with yyyy-mm-dd and hh:mm:ss -> c_jail_in, c_jail_out"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6873, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_xvYT2xu8L8",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbt9k0dsxkfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check column names\n",
        "print(df_compas.columns)\n",
        "# Check datatypes of columns\n",
        "print(df_compas.dtypes)\n",
        "df_compas.head(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duNufuy52C28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_compas.columns)\n",
        "print(df_compas[\"race\"].unique())\n",
        "print(df_compas.groupby([\"race\"]).agg({\"race\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twN3wQQh2LHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_compas, disc_feature = \"race\", disc_min_value=\"African-American\", \n",
        "                label = \"two_year_recid\", second_disc_feature=\"sex\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YznLf--cDdd",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgBktRpZcHdP",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hqczOKFBeeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_compas_train_input, df_compas_train_label = fair_preprocess(data = df_compas, \n",
        "                                                               label = \"two year recid\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SbCWg77cIEG",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AQ5ZvcFm5ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "# Important: Keys in the dictionary must be valid hyperparameters \n",
        "param_grid = {\"learning_rate\": [0.2],\n",
        "                \"n_estimators\": [50, 100, 150],\n",
        "                \"max_depth\": [3, 6, 9], \n",
        "                \"min_child_weight\": [1, 3, 5],       \n",
        "                \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=5\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1, \n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_compas_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFHoRCNcIes",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqNYcdQxExno",
        "colab_type": "code",
        "outputId": "b6ed52f4-3376-459b-9cd9-bfd3d8b4d398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_priv = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_unpriv = df_compas[is_unpriv]\n",
        "df_compas_priv = df_compas[is_priv]\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_unpriv, maj_data = df_compas_priv, training_sizes=training_sizes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n",
            "[2336, 2341, 2351, 2361, 2371, 2381, 2406, 2431, 2456, 2481, 2506, 2531, 2581, 2631, 2681, 2731, 2781, 2831, 2931, 3031, 3131, 3231, 3331, 3581, 3831, 4081, 4331, 4581, 4831, 5081, 5331, 5581, 5831]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYaTGG5cI-P",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTBnMllH4RI",
        "colab_type": "code",
        "outputId": "e0d179b8-eef8-4792-a1eb-50ebf973e01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define arguments\n",
        "label = \"two year recid\"\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                      colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                      learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "                      min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "                      nthread=4, objective='binary:logistic', random_state=0,\n",
        "                      reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                      silent=None, subsample=1, verbosity=1)\n",
        "cv = 5 \n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "# Run function\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = compas_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "\n",
        "results_df_compas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_complete_train</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_complete</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2336</td>\n",
              "      <td>5</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992432</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997826</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.005337</td>\n",
              "      <td>-0.198541</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.501830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2341</td>\n",
              "      <td>10</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992449</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997831</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.004290</td>\n",
              "      <td>0.005337</td>\n",
              "      <td>-0.098541</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.752745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2351</td>\n",
              "      <td>20</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.993029</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992962</td>\n",
              "      <td>0.997845</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.008580</td>\n",
              "      <td>0.004983</td>\n",
              "      <td>0.051888</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>1.130334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2361</td>\n",
              "      <td>30</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991462</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.991870</td>\n",
              "      <td>0.995713</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.012870</td>\n",
              "      <td>0.029531</td>\n",
              "      <td>0.102746</td>\n",
              "      <td>0.004353</td>\n",
              "      <td>1.258639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2371</td>\n",
              "      <td>40</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991480</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.995722</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.017160</td>\n",
              "      <td>0.040302</td>\n",
              "      <td>0.053175</td>\n",
              "      <td>0.004353</td>\n",
              "      <td>1.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2381</td>\n",
              "      <td>50</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991543</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.996812</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.021450</td>\n",
              "      <td>0.033451</td>\n",
              "      <td>0.082317</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.206990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2406</td>\n",
              "      <td>75</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992716</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.993492</td>\n",
              "      <td>0.996865</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.054054</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.032175</td>\n",
              "      <td>0.025472</td>\n",
              "      <td>0.136508</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2431</td>\n",
              "      <td>100</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991795</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.980769</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.996907</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.042900</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>0.132317</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.332718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2456</td>\n",
              "      <td>125</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.992397</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.993492</td>\n",
              "      <td>0.996945</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.053625</td>\n",
              "      <td>0.022639</td>\n",
              "      <td>0.131175</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.330560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2481</td>\n",
              "      <td>150</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991555</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982036</td>\n",
              "      <td>0.992416</td>\n",
              "      <td>0.997003</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.044118</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.064350</td>\n",
              "      <td>0.019796</td>\n",
              "      <td>0.168983</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>1.424919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2506</td>\n",
              "      <td>175</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990673</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.978947</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.996051</td>\n",
              "      <td>0.989362</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.548571</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.075075</td>\n",
              "      <td>0.017956</td>\n",
              "      <td>0.150459</td>\n",
              "      <td>-0.007374</td>\n",
              "      <td>1.377931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2531</td>\n",
              "      <td>200</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991304</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.981982</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.997085</td>\n",
              "      <td>0.990909</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>0.015875</td>\n",
              "      <td>0.161459</td>\n",
              "      <td>-0.006915</td>\n",
              "      <td>1.405124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2581</td>\n",
              "      <td>250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991042</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985401</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.996209</td>\n",
              "      <td>0.992647</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.552000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.107250</td>\n",
              "      <td>0.010953</td>\n",
              "      <td>0.153888</td>\n",
              "      <td>-0.004089</td>\n",
              "      <td>1.386543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2631</td>\n",
              "      <td>300</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990809</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.981707</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.995383</td>\n",
              "      <td>0.981707</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.022059</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.546667</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.128700</td>\n",
              "      <td>0.014838</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>-0.016116</td>\n",
              "      <td>1.371668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2681</td>\n",
              "      <td>350</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990942</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.980716</td>\n",
              "      <td>0.992954</td>\n",
              "      <td>0.994545</td>\n",
              "      <td>0.983425</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.023669</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.150150</td>\n",
              "      <td>0.014948</td>\n",
              "      <td>0.122746</td>\n",
              "      <td>-0.013310</td>\n",
              "      <td>1.308985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2731</td>\n",
              "      <td>400</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990308</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.983452</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.994690</td>\n",
              "      <td>0.985782</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.021164</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.171600</td>\n",
              "      <td>0.011810</td>\n",
              "      <td>0.131888</td>\n",
              "      <td>-0.010954</td>\n",
              "      <td>1.331282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2781</td>\n",
              "      <td>450</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990501</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982906</td>\n",
              "      <td>0.992424</td>\n",
              "      <td>0.994796</td>\n",
              "      <td>0.982906</td>\n",
              "      <td>0.997824</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.398541</td>\n",
              "      <td>0.193050</td>\n",
              "      <td>0.012469</td>\n",
              "      <td>0.121459</td>\n",
              "      <td>-0.014918</td>\n",
              "      <td>1.304758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2831</td>\n",
              "      <td>500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990733</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986717</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.994924</td>\n",
              "      <td>0.988593</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.016878</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>0.129888</td>\n",
              "      <td>-0.008142</td>\n",
              "      <td>1.326259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2931</td>\n",
              "      <td>600</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990700</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.987220</td>\n",
              "      <td>0.991879</td>\n",
              "      <td>0.995126</td>\n",
              "      <td>0.990385</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.017361</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.523333</td>\n",
              "      <td>0.398112</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>0.007607</td>\n",
              "      <td>0.125221</td>\n",
              "      <td>-0.006351</td>\n",
              "      <td>1.314537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3031</td>\n",
              "      <td>700</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991096</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990502</td>\n",
              "      <td>0.991333</td>\n",
              "      <td>0.996109</td>\n",
              "      <td>0.997268</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.397683</td>\n",
              "      <td>0.300300</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>0.132317</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>1.332718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3131</td>\n",
              "      <td>800</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991376</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990268</td>\n",
              "      <td>0.991870</td>\n",
              "      <td>0.995482</td>\n",
              "      <td>0.995110</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.015345</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.516250</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>0.343200</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.118996</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>1.299545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3231</td>\n",
              "      <td>900</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991664</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990164</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.995633</td>\n",
              "      <td>0.995604</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.015730</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.386100</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>1.288000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3331</td>\n",
              "      <td>1000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991573</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.989055</td>\n",
              "      <td>0.992946</td>\n",
              "      <td>0.995769</td>\n",
              "      <td>0.995992</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.506000</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.429000</td>\n",
              "      <td>0.005967</td>\n",
              "      <td>0.109604</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>1.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3581</td>\n",
              "      <td>1250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990335</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.988906</td>\n",
              "      <td>0.991314</td>\n",
              "      <td>0.994179</td>\n",
              "      <td>0.995215</td>\n",
              "      <td>0.993471</td>\n",
              "      <td>0.017657</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.508000</td>\n",
              "      <td>0.395967</td>\n",
              "      <td>0.536251</td>\n",
              "      <td>0.006159</td>\n",
              "      <td>0.112033</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>1.282934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3831</td>\n",
              "      <td>1500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991682</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991464</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.995823</td>\n",
              "      <td>0.997358</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.014805</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.510667</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.643501</td>\n",
              "      <td>0.005261</td>\n",
              "      <td>0.114270</td>\n",
              "      <td>0.002799</td>\n",
              "      <td>1.288273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4081</td>\n",
              "      <td>1750</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990519</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.989099</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.993844</td>\n",
              "      <td>0.993088</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.750751</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.103604</td>\n",
              "      <td>-0.001472</td>\n",
              "      <td>1.261364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4331</td>\n",
              "      <td>2000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.991187</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990571</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.995315</td>\n",
              "      <td>0.996008</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.015030</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.506500</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>0.858001</td>\n",
              "      <td>0.004698</td>\n",
              "      <td>0.110104</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>1.277761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4581</td>\n",
              "      <td>2250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.989022</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986253</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.992654</td>\n",
              "      <td>0.990205</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.017746</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.503111</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.965251</td>\n",
              "      <td>0.008053</td>\n",
              "      <td>0.106286</td>\n",
              "      <td>-0.005443</td>\n",
              "      <td>1.267840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4831</td>\n",
              "      <td>2500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.988451</td>\n",
              "      <td>0.992400</td>\n",
              "      <td>0.994462</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.017572</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.505200</td>\n",
              "      <td>0.395967</td>\n",
              "      <td>1.072501</td>\n",
              "      <td>0.005683</td>\n",
              "      <td>0.109233</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>1.275863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5081</td>\n",
              "      <td>2750</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.989660</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.987848</td>\n",
              "      <td>0.992408</td>\n",
              "      <td>0.993512</td>\n",
              "      <td>0.992103</td>\n",
              "      <td>0.995647</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.510909</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>1.179751</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>0.114084</td>\n",
              "      <td>-0.003544</td>\n",
              "      <td>1.287491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5331</td>\n",
              "      <td>3000</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.988158</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.986248</td>\n",
              "      <td>0.991323</td>\n",
              "      <td>0.992210</td>\n",
              "      <td>0.990789</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.018919</td>\n",
              "      <td>0.007790</td>\n",
              "      <td>0.511333</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>1.287001</td>\n",
              "      <td>0.007449</td>\n",
              "      <td>0.114508</td>\n",
              "      <td>-0.003770</td>\n",
              "      <td>1.288560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5581</td>\n",
              "      <td>3250</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.987327</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.984784</td>\n",
              "      <td>0.991861</td>\n",
              "      <td>0.992163</td>\n",
              "      <td>0.990814</td>\n",
              "      <td>0.994559</td>\n",
              "      <td>0.021645</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.508615</td>\n",
              "      <td>0.396396</td>\n",
              "      <td>1.394251</td>\n",
              "      <td>0.009154</td>\n",
              "      <td>0.112219</td>\n",
              "      <td>-0.003745</td>\n",
              "      <td>1.283098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5831</td>\n",
              "      <td>3500</td>\n",
              "      <td>2331</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985233</td>\n",
              "      <td>0.992954</td>\n",
              "      <td>0.994074</td>\n",
              "      <td>0.992701</td>\n",
              "      <td>0.996736</td>\n",
              "      <td>0.023269</td>\n",
              "      <td>0.007082</td>\n",
              "      <td>0.516571</td>\n",
              "      <td>0.397254</td>\n",
              "      <td>1.501502</td>\n",
              "      <td>0.010111</td>\n",
              "      <td>0.119317</td>\n",
              "      <td>-0.004035</td>\n",
              "      <td>1.300354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  ...  equal_opport_dist  disparate_impact\n",
              "0            2336              5  ...           0.002176          0.501830\n",
              "1            2341             10  ...           0.002176          0.752745\n",
              "2            2351             20  ...           0.002176          1.130334\n",
              "3            2361             30  ...           0.004353          1.258639\n",
              "4            2371             40  ...           0.004353          1.134000\n",
              "5            2381             50  ...           0.003264          1.206990\n",
              "6            2406             75  ...           0.003264          1.344000\n",
              "7            2431            100  ...           0.003264          1.332718\n",
              "8            2456            125  ...           0.003264          1.330560\n",
              "9            2481            150  ...           0.003264          1.424919\n",
              "10           2506            175  ...          -0.007374          1.377931\n",
              "11           2531            200  ...          -0.006915          1.405124\n",
              "12           2581            250  ...          -0.004089          1.386543\n",
              "13           2631            300  ...          -0.016116          1.371668\n",
              "14           2681            350  ...          -0.013310          1.308985\n",
              "15           2731            400  ...          -0.010954          1.331282\n",
              "16           2781            450  ...          -0.014918          1.304758\n",
              "17           2831            500  ...          -0.008142          1.326259\n",
              "18           2931            600  ...          -0.006351          1.314537\n",
              "19           3031            700  ...           0.001620          1.332718\n",
              "20           3131            800  ...          -0.000537          1.299545\n",
              "21           3231            900  ...          -0.000043          1.288000\n",
              "22           3331           1000  ...           0.000345          1.276500\n",
              "23           3581           1250  ...           0.001744          1.282934\n",
              "24           3831           1500  ...           0.002799          1.288273\n",
              "25           4081           1750  ...          -0.001472          1.261364\n",
              "26           4331           2000  ...           0.001449          1.277761\n",
              "27           4581           2250  ...          -0.005443          1.267840\n",
              "28           4831           2500  ...          -0.000168          1.275863\n",
              "29           5081           2750  ...          -0.003544          1.287491\n",
              "30           5331           3000  ...          -0.003770          1.288560\n",
              "31           5581           3250  ...          -0.003745          1.283098\n",
              "32           5831           3500  ...          -0.004035          1.300354\n",
              "\n",
              "[33 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtyIDAwfGNO7",
        "colab_type": "code",
        "outputId": "b8323768-850d-450b-fca8-a72c86dedef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# Save metrics csv\n",
        "\n",
        "results_df_compas.to_csv(\"df_compas_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_compas_metrics.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-abf481bc5df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save metrics csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults_df_compas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_compas_metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_compas_metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df_compas' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1230qeIkcp-a",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2DKNaYH3X9j",
        "colab_type": "code",
        "outputId": "aab5ef9f-9d7c-4a32-d5a6-214f3364f964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(df_compas[\"two year recid\"].unique())\n",
        "print(df_compas.shape)\n",
        "df_compas[\"two year recid\"].isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1. nan]\n",
            "(6874, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVvVG_BxzFMF",
        "colab_type": "code",
        "outputId": "b79d847e-e6bf-48c6-f6a4-7549cbeb5548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "compas_model.fit(df_compas_train_input, df_compas_train_label)\n",
        "plot_importance(compas_model)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-748cb64c90d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               silent=None, subsample=1, verbosity=1)\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcompas_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_compas_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_compas_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompas_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 726\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields first, last, compas screening date, sex, dob, race, c jail in, c jail out, c case number, c offense date, c arrest date, c charge degree, c charge desc, r case number, r charge degree, r offense date, r charge desc, r jail in, r jail out, vr case number, vr charge degree, vr offense date, vr charge desc, type of assessment, score text, screening date, v type of assessment, v score text, v screening date, in custody, out custody"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeup5l7RXeoe",
        "colab_type": "code",
        "outputId": "0e7aba24-a3de-4bb3-e8f0-4b3483bd9d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "compas_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_compas_train_input = pd.get_dummies(df_compas_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = compas_model, \n",
        "                    title = \"COMPAS Dataset - XGBoost Learning Curve\", \n",
        "                    X = df_compas_train_input, y = df_compas_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sdh1b0FYzDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwnExnGiYzmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYLBIMnHMHQy"
      },
      "source": [
        "## 3) Homicide Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbdFqBmv7QI",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tdRkreU8pcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_homicide = \"/content/drive/My Drive/Master Thesis/Data/Others/homicide_dataset_reduced.csv\"\n",
        "df_homicide_full = pd.read_csv(path_homicide, low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otsY0WTLCVdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_homicide = \"/content/drive/My Drive/Master Thesis/Data/homicide_dataset.csv\"\n",
        "df_homicide_full = pd.read_csv(path_homicide, low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8zayFMxm3vA",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx41WDcL4eAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "df_homicide_full[\"Perpetrator\"] = df_homicide_full[\"Perpetrator Count\"].apply(lambda val: False if val >= 1 else val == 0)\n",
        "\n",
        "# Replace False by 1 and True by 0\n",
        "df_homicide_full[\"Perpetrator\"] = df_homicide_full[\"Perpetrator\"].replace({False: 0, True: 1})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_homicide_full = df_homicide_full.drop(['Record ID', \"Perpetrator Count\", \"Victim Count\"], axis=1)\n",
        "\n",
        "# Replace values by NaNs\n",
        "df_homicide_full.replace('Unknown', np.nan)\n",
        "\n",
        "# Filter dataset based on year to reduce file size\n",
        "df_homicide = df_homicide_full[df_homicide_full[\"Year\"] > 2010]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_72jile7c6Dq",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYfLDcqBU81J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpbuBrUd3aOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.groupby([\"Perpetrator\"]).agg({\"Perpetrator\": 'count'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzCYLwOCEzZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_homicide.info)\n",
        "print(df_homicide.describe)\n",
        "print(df_homicide.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M963RlbU9eWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_homicide.head(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSHfWmP4k1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(data = df_homicide, disc_feature=\"Perpetrator Race\", disc_min_value=\"Black\", label = \"Perpetrator\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C01BFbz3daQn",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcHFLoqdh5L",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSOE-LXCFGOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_homicide_train_input, df_homicide_train_label = fair_preprocess(data = df_homicide,\n",
        "                                                                   label = \"Perpetrator\",\n",
        "                                                                   neg_class = 0,\n",
        "                                                                   pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_homicide.groupby([\"Perpetrator\"]).agg({\"Perpetrator\": 'count'}))\n",
        "print(df_homicide_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBuKJ_7udqW6",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4WE7mKdF1UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_homicide = hyperparameter_tuning(df_homicide_train_input, df_homicide_train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vknxj1bdnisU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.2],\n",
        "                \"n_estimators\": [50, 100, 150],\n",
        "                \"max_depth\": [3, 6, 9], \n",
        "                \"min_child_weight\": [1, 3, 5],       \n",
        "                \"reg_lambda\": [1, 1.2]}\n",
        "\n",
        "  xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "  cv=5\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      cv = cv,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = False)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_homicide_train_label)\n",
        "\n",
        "  print(grid_rf_class.best_params_)\n",
        "  print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN65JoIHdxiv",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2V-5rfYVdjz",
        "colab_type": "code",
        "outputId": "d6d812ed-c54d-4982-817e-d6cbac7726a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# training_sizes_homicide = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "#                           700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "#                           4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# training_sizes_homicide = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_homicide[\"Perpetrator Race\"].isin([\"Black\"])\n",
        "is_white = df_homicide[\"Perpetrator Race\"].isin([\"White\"])\n",
        "df_homicide_black = df_homicide[is_black]  # Minority\n",
        "df_homicide_white = df_homicide[is_white]  # Majority\n",
        "\n",
        "list_dfs_homicide = create_datasets(min_data = df_homicide_black, maj_data = df_homicide_white, training_sizes=training_sizes_homicide)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4d08c5a73908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_homicide_white\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_homicide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_white\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Majority\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlist_dfs_homicide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_homicide_black\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaj_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_homicide_white\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_sizes_homicide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_sizes_homicide' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uIyMWH8d7ma",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbsp9jPVVeHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define arguments\n",
        "label = \"Perpetrator\"\n",
        "homicide_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                      colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                      learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
        "                      min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "                      nthread=4, objective='binary:logistic', random_state=0,\n",
        "                      reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                      silent=None, subsample=1, verbosity=1)\n",
        "cv = 5 \n",
        "discr_feature = \"Perpetrator Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Run function\n",
        "results_df_homicide = metrics_to_df(list_dfs=list_dfs_homicide, label = label, model = homicide_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "results_df_homicide"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW0vsFyyp6ZR",
        "colab_type": "code",
        "outputId": "55a375fc-f704-481b-8a29-429e09d83401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "results_df_homicide"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f2684fe65dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df_homicide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df_homicide' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcd3Bqes4YMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2nd iteration due to RAM constraints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2QR3CpeGXOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "\n",
        "results_df_homicide.to_csv(\"df_homicide_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_homicide_metrics.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFknrG64eQwJ",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5k8nM-fzHxK",
        "colab_type": "code",
        "outputId": "390d24fc-6d67-4a6f-fb1c-6b7e50099324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "model = homicide_model\n",
        "model.fit(df_homicide_train_input, df_homicide_train_label)\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f8ab47024ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomicide_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_homicide_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_homicide_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 726\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields Agency Code, Agency Name, Agency Type, City, State, Month, Crime Type, Crime Solved, Victim Sex, Victim Race, Victim Ethnicity, Perpetrator Sex, Perpetrator Age, Perpetrator Race, Perpetrator Ethnicity, Relationship, Weapon, Record Source"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2nN17MLRMGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_homicide_train_input = pd.get_dummies(df_homicide_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_homicide_train_input, y = df_homicide_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkVU1LaVe4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_homicide)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jG0KRhjWmM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_homicide) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5PQKjKwpdT",
        "colab_type": "text"
      },
      "source": [
        "## 4) German Credit Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwsnRs9XPxz",
        "colab_type": "text"
      },
      "source": [
        "The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [42].\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9tpy66--lw",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsfLldE3OLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Long dataset version\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# path_credit_uci = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "path_credit_uci = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "column_names_credit = [\"chk_acct\", \"duration\", \"credit_his\", \"purpose\", \"amount\", \"saving_acct\", \n",
        "                       \"present_emp\", \"installment_rate\", \"sex\", \"other_debtor\", \"present_resid\", \n",
        "                       \"property\", \"age\", \"other_install\", \"housing\", \"n_credits\", \"job\", \"n_people\", \n",
        "                       \"telephone\", \"foreign\", \"response\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_credit_long = pd.read_csv(path_credit_uci, delim_whitespace=True, names = column_names_credit)\n",
        "df_credit_long"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGCylXv_vSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT - Short dataset version\n",
        "path_credit = \"/content/drive/My Drive/Master Thesis/Data/german_credit_dataset.csv\"\n",
        "df_credit_short = pd.read_csv(path_credit, header = 0, sep = \";\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmMJQzTeRjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join Risk and Sex column of the short dataset to rest of columns of the long dataset\n",
        "df_credit_long['id'] = range(0, len(df_credit_long))\n",
        "\n",
        "# Join based on \"id\" column - add risk and Sex column from short dataset to long dataset\n",
        "df_credit_short_reduced_cols = df_credit_short.loc[:, [\"id\", \"Sex\", \"Risk\"]]\n",
        "df_credit = pd.merge(df_credit_long, df_credit_short_reduced_cols, how = \"left\", on = \"id\")\n",
        "df_credit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9tFALHm5sc",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv9rHRxsM-ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop unnecessary columns\n",
        "df_credit = df_credit.drop(['id', \"response\"], axis=1)\n",
        "\n",
        "df_credit[\"Risk\"] = df_credit[\"Risk\"].replace({'good': 1, 'bad': 0})\n",
        "\n",
        "df_credit.rename(columns={'sex':'sex_specific'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqrA4Pbc_as",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiICbX-HE02N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit.info)\n",
        "print(df_credit.describe())\n",
        "print(df_credit.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x9J2edEZ1Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_credit)\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enom-Sm6gUsV",
        "colab_type": "text"
      },
      "source": [
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njYG0xUTEouo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats(df_credit, disc_feature=\"Sex\", disc_min_value=\"female\", label =\"Risk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEIZJoydbve",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxL3Q3hdjgM",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvAybUs5FHo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_credit_train_input, df_credit_train_label = fair_preprocess(data = df_credit, \n",
        "                                                               label = \"Risk\",\n",
        "                                                               neg_class = 0,\n",
        "                                                               pos_class = 1)\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_credit.groupby([\"Risk\"]).agg({\"Risk\": 'count'}))\n",
        "print(df_credit_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buM8oHaXdrdm",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7mw8mSvF2lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the hyperparameter grid\n",
        "param_grid = {\"learning_rate\": [0.2],\n",
        "              \"n_estimators\": [50, 100, 150],\n",
        "              \"max_depth\": [3, 6, 9], \n",
        "              \"min_child_weight\": [1, 3, 5],   \n",
        "              \"reg_lambda\": [1, 1.2]}\n",
        "xgb_grid_search = XGBClassifier(objective= 'binary:logistic', nthread=4)\n",
        "cv=5\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Dummy Coding\n",
        "df_train_input_dummy = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = xgb_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1,\n",
        "                                                     cv = cv,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = False)\n",
        "  \n",
        "# Fit model\n",
        "grid_rf_class.fit(df_train_input_dummy, df_credit_train_label)\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ0lfdI0dzit",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOicTRmoQ8qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_unpriv = df_credit[\"Sex\"].isin([\"female\"])\n",
        "is_priv = df_credit[\"Sex\"].isin([\"male\"])\n",
        "df_credit_unpriv = df_credit[is_unpriv]  # Minority\n",
        "df_credit_priv = df_credit[is_priv]  # Majority\n",
        "\n",
        "list_dfs_credit = create_datasets(min_data = df_credit_unpriv, maj_data = df_credit_priv, training_sizes=training_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArInhVkxd9N0",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4-M2nsFd9FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_dfs = list_dfs_credit\n",
        "label = \"Risk\"\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "# credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "#               learning_rate=0.3, max_delta_step=0, max_depth=3,\n",
        "#               min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
        "#               nthread=4, objective='binary:logistic', random_state=0,\n",
        "#               reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "#               silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "cv = 5\n",
        "discr_feature = \"Sex\"\n",
        "min_value = \"female\"\n",
        "maj_value = \"male\"\n",
        "\n",
        "results_df_credit = metrics_to_df(list_dfs=list_dfs_credit, label = label, model = credit_model, \n",
        "                                  cv = cv, discr_feature = discr_feature, min_value = min_value,\n",
        "                                  maj_value = maj_value)\n",
        "\n",
        "results_df_credit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjsOE_ANGdLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save metrics csv\n",
        "\n",
        "results_df_credit.to_csv(\"df_credit_metrics.csv\") \n",
        "from google.colab import files\n",
        "files.download(\"df_credit_metrics.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLhop7beSsL",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDUGL3Nrq0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Feature Importance\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "credit_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.3, max_delta_step=0, max_depth=1,\n",
        "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
        "              nthread=4, objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1.2, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1)\n",
        "credit_model.fit(df_credit_train_input, df_credit_train_label)\n",
        "plot_importance(credit_model)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0krkR9GRNrK",
        "colab_type": "code",
        "outputId": "8e8abdf1-6faa-42ba-b06e-b12387bd0f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_credit_train_input = pd.get_dummies(df_credit_train_input)\n",
        "\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = credit_model, \n",
        "                    title = \"Credit Dataset - XGBoost Learning Curve\", \n",
        "                    X = df_credit_train_input, y = df_credit_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRUHBMJRTiX",
        "colab_type": "code",
        "outputId": "0470f150-b236-4785-ef44-c5c2a6068366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "min_metrics_line_chart(results_df_credit, title = \"Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f4690177-5cac-47a8-9f14-f6e1f1631ad3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f4690177-5cac-47a8-9f14-f6e1f1631ad3',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}, {\"mode\": \"lines+markers\", \"name\": \"Avg. Abs. Odds Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.03157624148821203, 0.03157624148821203, 0.029572233472179965, 0.028570229464163932, 0.03319203852731645, 0.03319203852731645, 0.04705746571677388, 0.041433652645605334, 0.025417448802389886, 0.003245936395879445, 0.39914604224008365, 0.5509887854708386, 0.030733453168714786, 0.03777228250180936]}, {\"mode\": \"lines+markers\", \"name\": \"Statistical Parity Difference\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.024637681159420333, 0.024637681159420333, 0.021739130434782594, 0.020289855072463725, 0.024637681159420333, 0.024637681159420333, 0.03768115942028982, 0.03188405797101446, 0.02168115942028981, -0.0008695652173913437, -0.38252587991718423, -0.5544202898550724, 0.017275362318840526, 0.02681159420289847]}, {\"mode\": \"lines+markers\", \"name\": \"Equal Opportunity Distance\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [0.01603206412825653, 0.01603206412825653, 0.012024048096192397, 0.01002004008016033, 0.014028056112224463, 0.014028056112224463, 0.02605210420841686, 0.02004008016032066, 0.024048096192384794, 0.003429539491353828, -0.32233558025141185, -0.5453764672201546, 0.0038136879820246383, 0.015951094107406805]}, {\"mode\": \"lines+markers\", \"name\": \"Disparate Impact\", \"type\": \"scatter\", \"x\": [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300], \"y\": [1.0252600297176822, 1.0252600297176822, 1.0222222222222221, 1.0207100591715976, 1.0252600297176822, 1.0252600297176822, 1.0391566265060241, 1.0329341317365268, 1.0225301204819277, 0.9990950226244344, 0.6083933870284018, 0.42212990936555894, 1.018170731707317, 1.0280303030303028]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Minority\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f4690177-5cac-47a8-9f14-f6e1f1631ad3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxycTP8pRYzV",
        "colab_type": "code",
        "outputId": "a81e47e1-fab2-48a4-9642-167f27a78b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_credit, title = \"Majority and Minority Metrics for the Adult Dataset with XGBoost\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d5e52a13-47e6-4c9f-8dee-1a5e6363abb8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd5e52a13-47e6-4c9f-8dee-1a5e6363abb8',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"F1 Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.8378839590443685, 0.8378839590443685, 0.8398637137989777, 0.8408510638297872, 0.8395904436860069, 0.8395904436860069, 0.8357695614789338, 0.8380462724935732, 0.8374892519346518, 0.8382099827882961, 0.8388746803069054, 0.8372093023255814, 0.8398268398268398, 0.8386540120793787]}, {\"mode\": \"lines+markers\", \"name\": \"F1 Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.7499999999999999, 0.7499999999999999, 0.787878787878788, 0.8, 0.8059701492537313, 0.8095238095238095, 0.8095238095238095, 0.7804878048780487, 0.8, 0.7883817427385892, 0.6822429906542056, 0.5217391304347826, 0.7911547911547911, 0.795131845841785]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9839679358717435, 0.9839679358717435, 0.9879759519038076, 0.9899799599198397, 0.9859719438877755, 0.9859719438877755, 0.9739478957915831, 0.9799599198396793, 0.9759519038076152, 0.9759519038076152, 0.9859719438877755, 0.9739478957915831, 0.9719438877755511, 0.9739478957915831]}, {\"mode\": \"lines+markers\", \"name\": \"TPR/Recall Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.979381443298969, 0.6636363636363637, 0.42857142857142855, 0.9757575757575757, 0.98989898989899]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Majority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9528795811518325, 0.9476439790575916, 0.9476439790575916, 0.9319371727748691, 0.93717277486911, 0.9267015706806283, 0.9214659685863874, 0.9528795811518325, 0.9214659685863874, 0.8952879581151832, 0.9109947643979057]}, {\"mode\": \"lines+markers\", \"name\": \"FPR Minority\", \"type\": \"scatter\", \"x\": [695, 700, 710, 720, 730, 740, 765, 790, 815, 840, 865, 890, 940, 990], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883720930233, 0.9245283018867925, 0.47692307692307695, 0.36486486486486486, 0.9529411764705882, 0.9705882352941176]}],\n",
              "                        {\"font\": {\"size\": 14}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Majority and Minority Metrics for the Adult Dataset with XGBoost\", \"x\": 0.5, \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Rows Complete\"}}, \"yaxis\": {\"title\": {\"text\": \"Metric Score\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d5e52a13-47e6-4c9f-8dee-1a5e6363abb8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i07ZtFVpOwcv"
      },
      "source": [
        "# Generalizable Rule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wse3LOyiH_2",
        "colab_type": "text"
      },
      "source": [
        "**Approach**:\n",
        "1. Rename columns of each metrics_df to a format in which the name of the original dataset is stored. \n",
        "2. Merge data frames with metrics (column bind) on minority row index via left join. Start with the dataframe with the most rows_minority and then merge step by step with the df with second most, etc.\n",
        "3. Determine slop change per row in comparison to previous row. \n",
        "4. Then plot only relevant fairness metrics. \n",
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/).\n",
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSgVqfuZiHRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Rename columns of each results_df\n",
        "\n",
        "# results_df_adult\n",
        "results_df_adult.columns = [str(col) + '_adult' for col in results_df_adult.columns]\n",
        "# Rename rows_minority_adult column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_adult\":\"rows_minority\"})\n",
        "\n",
        "# results_df_compas\n",
        "results_df_compas.columns = [str(col) + '_compas' for col in results_df_compas.columns]\n",
        "# Rename rows_minority_compas column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_compas\":\"rows_minority\"})\n",
        "\n",
        "# results_df_homicide\n",
        "results_df_homicide.columns = [str(col) + '_homicide' for col in results_df_homicide.columns]\n",
        "# Rename rows_minority_homicide column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_homicide\":\"rows_minority\"})\n",
        "\n",
        "# results_df_credit\n",
        "results_df_credit.columns = [str(col) + '_credit' for col in results_df_credit.columns]\n",
        "# Rename rows_minority_credit column into standard format\n",
        "results_df_adult = results_df_adult.rename(index=str, \n",
        "                                           columns={\"rows_minority_credit\":\"rows_minority\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUm1gEXwMjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Left join all result dfs into one\n",
        "merge_1 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "merge_2 = pd.merge(data1, data2, \n",
        "                   how='left', on='X1')\n",
        "\n",
        "complete_results_df = pd.merge(data1, data2, \n",
        "                               how='left', on='X1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6vRFnW3ebk",
        "colab_type": "text"
      },
      "source": [
        "Determine slop change per row in comparison to previous row: \n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
        "\n",
        "\n",
        "**Ideas:**\n",
        "- Get diff for rows_minority and the metric of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sLcz2OqzLcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Determine slope change \n",
        "\n",
        "# Calculate slope (rise(y-axis change)/run(x-axis change))\n",
        "\n",
        "# Run\n",
        "complete_results_df[\"rows_minority_diff\"] = complete_results_df[\"rows_minority\"].diff()\n",
        "\n",
        "# Rise\n",
        "# Calculate slopes for aver_abs_odds_diff\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_diff\"] = complete_results_df[\"aver_abs_odds_diff_adult\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_adult_slope\"] = complete_results_df[\"aver_abs_odds_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_diff\"] = complete_results_df[\"aver_abs_odds_diff_compas\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_compas_slope\"] = complete_results_df[\"aver_abs_odds_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] = complete_results_df[\"aver_abs_odds_diff_homicide\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_homicide_slope\"] = complete_results_df[\"aver_abs_odds_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_diff\"] = complete_results_df[\"aver_abs_odds_diff_credit\"].diff()\n",
        "complete_results_df[\"aver_abs_odds_diff_credit_slope\"] = complete_results_df[\"aver_abs_odds_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for stat_parity_diff\n",
        "complete_results_df[\"stat_parity_diff_adult_diff\"] = complete_results_df[\"stat_parity_diff_adult\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_adult_slope\"] = complete_results_df[\"stat_parity_diff_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_compas_diff\"] = complete_results_df[\"stat_parity_diff_compas\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_compas_slope\"] = complete_results_df[\"stat_parity_diff_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_homicide_diff\"] = complete_results_df[\"stat_parity_diff_homicide\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_homicide_slope\"] = complete_results_df[\"stat_parity_diff_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"stat_parity_diff_credit_diff\"] = complete_results_df[\"stat_parity_diff_credit\"].diff()\n",
        "complete_results_df[\"stat_parity_diff_credit_slope\"] = complete_results_df[\"stat_parity_diff_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for equal_opport_dist\n",
        "complete_results_df[\"equal_opport_dist_adult_diff\"] = complete_results_df[\"equal_opport_dist_adult\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_adult_slope\"] = complete_results_df[\"equal_opport_dist_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_compas_diff\"] = complete_results_df[\"equal_opport_dist_compas\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_compas_slope\"] = complete_results_df[\"equal_opport_dist_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_homicide_diff\"] = complete_results_df[\"equal_opport_dist_homicide\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_homicide_slope\"] = complete_results_df[\"equal_opport_dist_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"equal_opport_dist_credit_diff\"] = complete_results_df[\"equal_opport_dist_credit\"].diff()\n",
        "complete_results_df[\"equal_opport_dist_credit_slope\"] = complete_results_df[\"equal_opport_dist_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "\n",
        "# Calculate slopes for disparate_impact\n",
        "complete_results_df[\"disparate_impact_adult_diff\"] = complete_results_df[\"disparate_impact_adult\"].diff()\n",
        "complete_results_df[\"disparate_impact_adult_slope\"] = complete_results_df[\"disparate_impact_adult_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_compas_diff\"] = complete_results_df[\"disparate_impact_compas\"].diff()\n",
        "complete_results_df[\"disparate_impact_compas_slope\"] = complete_results_df[\"disparate_impact_compas_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_homicide_diff\"] = complete_results_df[\"disparate_impact_homicide\"].diff()\n",
        "complete_results_df[\"disparate_impact_homicide_slope\"] = complete_results_df[\"disparate_impact_homicide_diff\"] / complete_results_df[\"rows_minority_diff\"]\n",
        "complete_results_df[\"disparate_impact_credit_diff\"] = complete_results_df[\"disparate_impact_credit\"].diff()\n",
        "complete_results_df[\"disparate_impact_credit_slope\"] = complete_results_df[\"disparate_impact_credit_diff\"] / complete_results_df[\"rows_minority_diff\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CB0JtaoKaJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_results_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZDFjNB0FJTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. Plot all fairness metrics \n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"f1_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  # TPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"tpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "  # FPR\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_majority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Majority'))\n",
        "  fig.add_trace(go.Scatter(x=complete_results_df[\"rows_complete\"], y=complete_results_df[\"fpr_minority\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='FPR Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': title,\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           # 'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJOmyc6VFswT",
        "colab_type": "text"
      },
      "source": [
        "5. For each inflection point (slope change < threshold) for each dataset and metric of the chosen subset, plot a line that vertically touches then the x-axis (-> https://plotly.com/python/shapes/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0jr3FtCFfRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add shapes\n",
        "fig.add_shape(\n",
        "        # Line Vertical\n",
        "        dict(\n",
        "            type=\"line\",\n",
        "            x0=1,\n",
        "            y0=0,\n",
        "            x1=1,\n",
        "            y1=2,\n",
        "            line=dict(\n",
        "                color=\"RoyalBlue\",\n",
        "                width=3, \n",
        "                dash=\"dot\",\n",
        "                opacity=0.7\n",
        "            )\n",
        "\n",
        "fig.add_shape(\n",
        "        # Line reference to the axes\n",
        "            type=\"line\",\n",
        "            xref=\"x\",\n",
        "            yref=\"y\",\n",
        "            x0=4,\n",
        "            y0=0,\n",
        "            x1=8,\n",
        "            y1=1,\n",
        "            line=dict(\n",
        "                color=\"LightSeaGreen\",\n",
        "                width=3,\n",
        "            ),\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxH0zA8eGNB8",
        "colab_type": "text"
      },
      "source": [
        "6. Then, create corridor shape (dotted, medium alpha) that covers inflection point with the minimum number of training examples and the maximum and everything in between (-> https://plotly.com/python/shapes/ - *Highlighting Time Series Regions with Rectangle Shapes*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM-X6rYBGQZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "# Add shape regions\n",
        "fig.update_layout(\n",
        "    shapes=[\n",
        "        # 1st highlight during Feb 4 - Feb 6\n",
        "        dict(\n",
        "            type=\"rect\",\n",
        "            # x-reference is assigned to the x-values\n",
        "            xref=\"x\",\n",
        "            # y-reference is assigned to the plot paper [0,1]\n",
        "            yref=\"paper\",\n",
        "            x0=\"2015-02-04\", # First value of corridor\n",
        "            y0=0,\n",
        "            x1=\"2015-02-06\", # Second value of corridor \n",
        "            y1=1,\n",
        "            fillcolor=\"LightSalmon\",\n",
        "            opacity=0.5,\n",
        "            layer=\"below\",\n",
        "            line_width=0,\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC806v-lnXrg",
        "colab_type": "text"
      },
      "source": [
        "# Other Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgjHGsyBv5q",
        "colab_type": "text"
      },
      "source": [
        "## Juvenile Justice dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JokphHiwjPhH",
        "colab_type": "text"
      },
      "source": [
        "Recidivism in Juvenile Justice\n",
        "\n",
        "http://cejfe.gencat.cat/en/recerca/opendata/jjuvenil/reincidencia-justicia-menors/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14pe6cBdFMR",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpIKmP03oKYp",
        "colab_type": "code",
        "outputId": "aaa1c140-059e-4187-a896-e67994bbc3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "path = \"/content/drive/My Drive/Master Thesis/Data/heloc_dataset.csv\"\n",
        "df_heloc = pd.read_csv(path)\n",
        "df_heloc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RiskPerformance</th>\n",
              "      <th>ExternalRiskEstimate</th>\n",
              "      <th>MSinceOldestTradeOpen</th>\n",
              "      <th>MSinceMostRecentTradeOpen</th>\n",
              "      <th>AverageMInFile</th>\n",
              "      <th>NumSatisfactoryTrades</th>\n",
              "      <th>NumTrades60Ever2DerogPubRec</th>\n",
              "      <th>NumTrades90Ever2DerogPubRec</th>\n",
              "      <th>PercentTradesNeverDelq</th>\n",
              "      <th>MSinceMostRecentDelq</th>\n",
              "      <th>MaxDelq2PublicRecLast12M</th>\n",
              "      <th>MaxDelqEver</th>\n",
              "      <th>NumTotalTrades</th>\n",
              "      <th>NumTradesOpeninLast12M</th>\n",
              "      <th>PercentInstallTrades</th>\n",
              "      <th>MSinceMostRecentInqexcl7days</th>\n",
              "      <th>NumInqLast6M</th>\n",
              "      <th>NumInqLast6Mexcl7days</th>\n",
              "      <th>NetFractionRevolvingBurden</th>\n",
              "      <th>NetFractionInstallBurden</th>\n",
              "      <th>NumRevolvingTradesWBalance</th>\n",
              "      <th>NumInstallTradesWBalance</th>\n",
              "      <th>NumBank2NatlTradesWHighUtilization</th>\n",
              "      <th>PercentTradesWBalance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bad</td>\n",
              "      <td>55</td>\n",
              "      <td>144</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>83</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>-8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bad</td>\n",
              "      <td>61</td>\n",
              "      <td>58</td>\n",
              "      <td>15</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>-7</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>-8</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bad</td>\n",
              "      <td>67</td>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>-7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>53</td>\n",
              "      <td>66</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bad</td>\n",
              "      <td>66</td>\n",
              "      <td>169</td>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>76</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>72</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bad</td>\n",
              "      <td>81</td>\n",
              "      <td>333</td>\n",
              "      <td>27</td>\n",
              "      <td>132</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>-7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>89</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10454</th>\n",
              "      <td>Good</td>\n",
              "      <td>73</td>\n",
              "      <td>131</td>\n",
              "      <td>5</td>\n",
              "      <td>57</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>-8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10455</th>\n",
              "      <td>Bad</td>\n",
              "      <td>65</td>\n",
              "      <td>147</td>\n",
              "      <td>39</td>\n",
              "      <td>68</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>92</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>86</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10456</th>\n",
              "      <td>Bad</td>\n",
              "      <td>74</td>\n",
              "      <td>129</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>-7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>5</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10457</th>\n",
              "      <td>Bad</td>\n",
              "      <td>72</td>\n",
              "      <td>234</td>\n",
              "      <td>12</td>\n",
              "      <td>113</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>96</td>\n",
              "      <td>35</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>-8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10458</th>\n",
              "      <td>Bad</td>\n",
              "      <td>66</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>-7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>67</td>\n",
              "      <td>-8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10459 rows  24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RiskPerformance  ExternalRiskEstimate  MSinceOldestTradeOpen  \\\n",
              "0                 Bad                    55                    144   \n",
              "1                 Bad                    61                     58   \n",
              "2                 Bad                    67                     66   \n",
              "3                 Bad                    66                    169   \n",
              "4                 Bad                    81                    333   \n",
              "...               ...                   ...                    ...   \n",
              "10454            Good                    73                    131   \n",
              "10455             Bad                    65                    147   \n",
              "10456             Bad                    74                    129   \n",
              "10457             Bad                    72                    234   \n",
              "10458             Bad                    66                     28   \n",
              "\n",
              "       MSinceMostRecentTradeOpen  AverageMInFile  NumSatisfactoryTrades  \\\n",
              "0                              4              84                     20   \n",
              "1                             15              41                      2   \n",
              "2                              5              24                      9   \n",
              "3                              1              73                     28   \n",
              "4                             27             132                     12   \n",
              "...                          ...             ...                    ...   \n",
              "10454                          5              57                     21   \n",
              "10455                         39              68                     11   \n",
              "10456                          6              64                     18   \n",
              "10457                         12             113                     42   \n",
              "10458                          1              17                      4   \n",
              "\n",
              "       NumTrades60Ever2DerogPubRec  NumTrades90Ever2DerogPubRec  \\\n",
              "0                                3                            0   \n",
              "1                                4                            4   \n",
              "2                                0                            0   \n",
              "3                                1                            1   \n",
              "4                                0                            0   \n",
              "...                            ...                          ...   \n",
              "10454                            0                            0   \n",
              "10455                            0                            0   \n",
              "10456                            1                            1   \n",
              "10457                            2                            2   \n",
              "10458                            0                            0   \n",
              "\n",
              "       PercentTradesNeverDelq  MSinceMostRecentDelq  MaxDelq2PublicRecLast12M  \\\n",
              "0                          83                     2                         3   \n",
              "1                         100                    -7                         0   \n",
              "2                         100                    -7                         7   \n",
              "3                          93                    76                         6   \n",
              "4                         100                    -7                         7   \n",
              "...                       ...                   ...                       ...   \n",
              "10454                      95                    80                         6   \n",
              "10455                      92                    28                         6   \n",
              "10456                     100                    -7                         6   \n",
              "10457                      96                    35                         6   \n",
              "10458                     100                    -7                         7   \n",
              "\n",
              "       MaxDelqEver  NumTotalTrades  NumTradesOpeninLast12M  \\\n",
              "0                5              23                       1   \n",
              "1                8               7                       0   \n",
              "2                8               9                       4   \n",
              "3                6              30                       3   \n",
              "4                8              12                       0   \n",
              "...            ...             ...                     ...   \n",
              "10454            6              21                       5   \n",
              "10455            6              12                       0   \n",
              "10456            8              18                       1   \n",
              "10457            2              45                       0   \n",
              "10458            8               5                       2   \n",
              "\n",
              "       PercentInstallTrades  MSinceMostRecentInqexcl7days  NumInqLast6M  \\\n",
              "0                        43                             0             0   \n",
              "1                        67                             0             0   \n",
              "2                        44                             0             4   \n",
              "3                        57                             0             5   \n",
              "4                        25                             0             1   \n",
              "...                     ...                           ...           ...   \n",
              "10454                    19                             7             0   \n",
              "10455                    42                             1             1   \n",
              "10456                    33                             3             4   \n",
              "10457                    20                             6             0   \n",
              "10458                    60                             3             3   \n",
              "\n",
              "       NumInqLast6Mexcl7days  NetFractionRevolvingBurden  \\\n",
              "0                          0                          33   \n",
              "1                          0                           0   \n",
              "2                          4                          53   \n",
              "3                          4                          72   \n",
              "4                          1                          51   \n",
              "...                      ...                         ...   \n",
              "10454                      0                          26   \n",
              "10455                      1                          86   \n",
              "10456                      4                           6   \n",
              "10457                      0                          19   \n",
              "10458                      2                          67   \n",
              "\n",
              "       NetFractionInstallBurden  NumRevolvingTradesWBalance  \\\n",
              "0                            -8                           8   \n",
              "1                            -8                           0   \n",
              "2                            66                           4   \n",
              "3                            83                           6   \n",
              "4                            89                           3   \n",
              "...                         ...                         ...   \n",
              "10454                        -8                           5   \n",
              "10455                        53                           2   \n",
              "10456                        -8                           5   \n",
              "10457                        -8                           4   \n",
              "10458                        -8                           2   \n",
              "\n",
              "       NumInstallTradesWBalance  NumBank2NatlTradesWHighUtilization  \\\n",
              "0                             1                                   1   \n",
              "1                            -8                                  -8   \n",
              "2                             2                                   1   \n",
              "3                             4                                   3   \n",
              "4                             1                                   0   \n",
              "...                         ...                                 ...   \n",
              "10454                         2                                   0   \n",
              "10455                         2                                   1   \n",
              "10456                        -8                                   0   \n",
              "10457                         1                                   0   \n",
              "10458                         1                                   0   \n",
              "\n",
              "       PercentTradesWBalance  \n",
              "0                         69  \n",
              "1                          0  \n",
              "2                         86  \n",
              "3                         91  \n",
              "4                         80  \n",
              "...                      ...  \n",
              "10454                    100  \n",
              "10455                     80  \n",
              "10456                     56  \n",
              "10457                     38  \n",
              "10458                    100  \n",
              "\n",
              "[10459 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMAaNMSWo6Z3",
        "colab_type": "code",
        "outputId": "3f037918-8f72-4964-de5e-bf417684a0e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "df_heloc.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['RiskPerformance', 'ExternalRiskEstimate', 'MSinceOldestTradeOpen',\n",
              "       'MSinceMostRecentTradeOpen', 'AverageMInFile', 'NumSatisfactoryTrades',\n",
              "       'NumTrades60Ever2DerogPubRec', 'NumTrades90Ever2DerogPubRec',\n",
              "       'PercentTradesNeverDelq', 'MSinceMostRecentDelq',\n",
              "       'MaxDelq2PublicRecLast12M', 'MaxDelqEver', 'NumTotalTrades',\n",
              "       'NumTradesOpeninLast12M', 'PercentInstallTrades',\n",
              "       'MSinceMostRecentInqexcl7days', 'NumInqLast6M', 'NumInqLast6Mexcl7days',\n",
              "       'NetFractionRevolvingBurden', 'NetFractionInstallBurden',\n",
              "       'NumRevolvingTradesWBalance', 'NumInstallTradesWBalance',\n",
              "       'NumBank2NatlTradesWHighUtilization', 'PercentTradesWBalance'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yL5zLpHm977",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEVy0agdEPj",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wh0GXBzEsM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda_descr_stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frEeeyOcdeia",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbfAc6dAdmp5",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjSg39N2FKik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIaa-Iwodmg9",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsCtMQYF5LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxo5ErlDd1sE",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFW3Y7yiQ_1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_white = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_black = df_compas[is_black]  # Minority\n",
        "df_compas_white = df_compas[is_white]  # Majority\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_black, maj_data = df_compas_white, training_sizes=training_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogKyufJNeArd",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77EkDXXQRG2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_dfs = list_dfs_compas\n",
        "label = \"two_year_recid\"\n",
        "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                              criterion='gini', max_depth=25, max_features='auto',\n",
        "                              max_leaf_nodes=None, max_samples=None,\n",
        "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                              min_samples_leaf=1, min_samples_split=5,\n",
        "                              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                              n_jobs=None, oob_score=False, random_state=None,\n",
        "                              verbose=0, warm_start=False)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "cv = 5 # To Do: For final calculations, change to cv = 10\n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = model, \n",
        "                                  cv = cv, discr_feature = \"race\", min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsugkMieWV6",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6BW1hQRQXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Adult Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C2H-qopRWLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp3ZnNGmRbK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SAZPLhwuUD",
        "colab_type": "text"
      },
      "source": [
        "## Communities and Crime Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRiwaHW2XE43",
        "colab_type": "text"
      },
      "source": [
        "The Communities and Crime dataset gathers information from different communities in the United States related to several factors that can highly influence some common crimes such as robberies, murders or rapes. The data includes crime data obtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains socio-economic data from the 1990 US Census."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v2ws3jO_65w",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RypNWRj_D8",
        "colab_type": "code",
        "outputId": "37e0d43d-2e6d-4d41-95ec-458884c994c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "path_communities = \"/content/drive/My Drive/Master Thesis/Data/communities_and_crime_dataset.csv\"\n",
        "df_communities = pd.read_csv(path_communities, encoding = \"utf-8\")\n",
        "df_communities"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>communityname</th>\n",
              "      <th>state</th>\n",
              "      <th>countyCode</th>\n",
              "      <th>communityCode</th>\n",
              "      <th>fold</th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>PctOccupManu</th>\n",
              "      <th>PctOccupMgmtProf</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>FemalePctDiv</th>\n",
              "      <th>TotalPctDiv</th>\n",
              "      <th>PersPerFam</th>\n",
              "      <th>PctFam2Par</th>\n",
              "      <th>PctKids2Par</th>\n",
              "      <th>PctYoungKids2Par</th>\n",
              "      <th>PctTeen2Par</th>\n",
              "      <th>PctWorkMomYoungKids</th>\n",
              "      <th>PctWorkMom</th>\n",
              "      <th>NumKidsBornNeverMar</th>\n",
              "      <th>PctKidsBornNeverMar</th>\n",
              "      <th>NumImmig</th>\n",
              "      <th>PctImmigRecent</th>\n",
              "      <th>PctImmigRec5</th>\n",
              "      <th>PctImmigRec8</th>\n",
              "      <th>PctImmigRec10</th>\n",
              "      <th>PctRecentImmig</th>\n",
              "      <th>PctRecImmig5</th>\n",
              "      <th>PctRecImmig8</th>\n",
              "      <th>PctRecImmig10</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>OwnOccQrange</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>RentQrange</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LemasSwornFT</th>\n",
              "      <th>LemasSwFTPerPop</th>\n",
              "      <th>LemasSwFTFieldOps</th>\n",
              "      <th>LemasSwFTFieldPerPop</th>\n",
              "      <th>LemasTotalReq</th>\n",
              "      <th>LemasTotReqPerPop</th>\n",
              "      <th>PolicReqPerOffic</th>\n",
              "      <th>PolicPerPop</th>\n",
              "      <th>RacialMatchCommPol</th>\n",
              "      <th>PctPolicWhite</th>\n",
              "      <th>PctPolicBlack</th>\n",
              "      <th>PctPolicHisp</th>\n",
              "      <th>PctPolicAsian</th>\n",
              "      <th>PctPolicMinor</th>\n",
              "      <th>OfficAssgnDrugUnits</th>\n",
              "      <th>NumKindsDrugsSeiz</th>\n",
              "      <th>PolicAveOTWorked</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "      <th>PolicCars</th>\n",
              "      <th>PolicOperBudg</th>\n",
              "      <th>LemasPctPolicOnPatr</th>\n",
              "      <th>LemasGangUnitDeploy</th>\n",
              "      <th>LemasPctOfficDrugUn</th>\n",
              "      <th>PolicBudgPerPop</th>\n",
              "      <th>murders</th>\n",
              "      <th>murdPerPop</th>\n",
              "      <th>rapes</th>\n",
              "      <th>rapesPerPop</th>\n",
              "      <th>robberies</th>\n",
              "      <th>robbbPerPop</th>\n",
              "      <th>assaults</th>\n",
              "      <th>assaultPerPop</th>\n",
              "      <th>burglaries</th>\n",
              "      <th>burglPerPop</th>\n",
              "      <th>larcenies</th>\n",
              "      <th>larcPerPop</th>\n",
              "      <th>autoTheft</th>\n",
              "      <th>autoTheftPerPop</th>\n",
              "      <th>arsons</th>\n",
              "      <th>arsonsPerPop</th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "      <th>nonViolPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BerkeleyHeightstownship</td>\n",
              "      <td>NJ</td>\n",
              "      <td>39</td>\n",
              "      <td>5320</td>\n",
              "      <td>1</td>\n",
              "      <td>11980</td>\n",
              "      <td>3.10</td>\n",
              "      <td>1.37</td>\n",
              "      <td>91.78</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1.88</td>\n",
              "      <td>12.47</td>\n",
              "      <td>21.44</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.33</td>\n",
              "      <td>11980</td>\n",
              "      <td>100.00</td>\n",
              "      <td>75122</td>\n",
              "      <td>89.24</td>\n",
              "      <td>1.55</td>\n",
              "      <td>70.20</td>\n",
              "      <td>23.62</td>\n",
              "      <td>1.03</td>\n",
              "      <td>18.39</td>\n",
              "      <td>79584</td>\n",
              "      <td>29711</td>\n",
              "      <td>30233</td>\n",
              "      <td>13600</td>\n",
              "      <td>5725</td>\n",
              "      <td>27101</td>\n",
              "      <td>5115</td>\n",
              "      <td>22838</td>\n",
              "      <td>227</td>\n",
              "      <td>1.96</td>\n",
              "      <td>5.81</td>\n",
              "      <td>9.90</td>\n",
              "      <td>48.18</td>\n",
              "      <td>2.70</td>\n",
              "      <td>64.55</td>\n",
              "      <td>14.65</td>\n",
              "      <td>28.82</td>\n",
              "      <td>5.49</td>\n",
              "      <td>50.73</td>\n",
              "      <td>3.67</td>\n",
              "      <td>26.38</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.47</td>\n",
              "      <td>3.22</td>\n",
              "      <td>91.43</td>\n",
              "      <td>90.17</td>\n",
              "      <td>95.78</td>\n",
              "      <td>95.81</td>\n",
              "      <td>44.56</td>\n",
              "      <td>58.88</td>\n",
              "      <td>31</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1277</td>\n",
              "      <td>8.69</td>\n",
              "      <td>13.00</td>\n",
              "      <td>20.99</td>\n",
              "      <td>30.93</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.24</td>\n",
              "      <td>3.30</td>\n",
              "      <td>85.68</td>\n",
              "      <td>1.37</td>\n",
              "      <td>4.81</td>\n",
              "      <td>4.17</td>\n",
              "      <td>2.99</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.84</td>\n",
              "      <td>91.46</td>\n",
              "      <td>0.39</td>\n",
              "      <td>11.06</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>98.37</td>\n",
              "      <td>91.01</td>\n",
              "      <td>3.12</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1959</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.28</td>\n",
              "      <td>215900</td>\n",
              "      <td>262600</td>\n",
              "      <td>326900</td>\n",
              "      <td>111000</td>\n",
              "      <td>685</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>316</td>\n",
              "      <td>1001</td>\n",
              "      <td>23.8</td>\n",
              "      <td>21.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>10.66</td>\n",
              "      <td>53.72</td>\n",
              "      <td>65.29</td>\n",
              "      <td>78.09</td>\n",
              "      <td>89.14</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.5</td>\n",
              "      <td>1845.9</td>\n",
              "      <td>9.63</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>4</td>\n",
              "      <td>32.81</td>\n",
              "      <td>14</td>\n",
              "      <td>114.85</td>\n",
              "      <td>138</td>\n",
              "      <td>1132.08</td>\n",
              "      <td>16</td>\n",
              "      <td>131.26</td>\n",
              "      <td>2</td>\n",
              "      <td>16.41</td>\n",
              "      <td>41.02</td>\n",
              "      <td>1394.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Marpletownship</td>\n",
              "      <td>PA</td>\n",
              "      <td>45</td>\n",
              "      <td>47616</td>\n",
              "      <td>1</td>\n",
              "      <td>23123</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.80</td>\n",
              "      <td>95.57</td>\n",
              "      <td>3.44</td>\n",
              "      <td>0.85</td>\n",
              "      <td>11.01</td>\n",
              "      <td>21.30</td>\n",
              "      <td>10.48</td>\n",
              "      <td>17.18</td>\n",
              "      <td>23123</td>\n",
              "      <td>100.00</td>\n",
              "      <td>47917</td>\n",
              "      <td>78.99</td>\n",
              "      <td>1.11</td>\n",
              "      <td>64.11</td>\n",
              "      <td>35.50</td>\n",
              "      <td>2.75</td>\n",
              "      <td>22.85</td>\n",
              "      <td>55323</td>\n",
              "      <td>20148</td>\n",
              "      <td>20191</td>\n",
              "      <td>18137</td>\n",
              "      <td>0</td>\n",
              "      <td>20074</td>\n",
              "      <td>5250</td>\n",
              "      <td>12222</td>\n",
              "      <td>885</td>\n",
              "      <td>3.98</td>\n",
              "      <td>5.61</td>\n",
              "      <td>13.72</td>\n",
              "      <td>29.89</td>\n",
              "      <td>2.43</td>\n",
              "      <td>61.96</td>\n",
              "      <td>12.26</td>\n",
              "      <td>29.28</td>\n",
              "      <td>6.39</td>\n",
              "      <td>37.64</td>\n",
              "      <td>4.23</td>\n",
              "      <td>27.99</td>\n",
              "      <td>6.45</td>\n",
              "      <td>5.42</td>\n",
              "      <td>3.11</td>\n",
              "      <td>86.91</td>\n",
              "      <td>85.33</td>\n",
              "      <td>96.82</td>\n",
              "      <td>86.46</td>\n",
              "      <td>51.14</td>\n",
              "      <td>62.43</td>\n",
              "      <td>43</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1920</td>\n",
              "      <td>5.21</td>\n",
              "      <td>8.65</td>\n",
              "      <td>13.33</td>\n",
              "      <td>22.50</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.87</td>\n",
              "      <td>87.79</td>\n",
              "      <td>1.81</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.34</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.83</td>\n",
              "      <td>1.96</td>\n",
              "      <td>89.03</td>\n",
              "      <td>1.01</td>\n",
              "      <td>23.60</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>97.15</td>\n",
              "      <td>84.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>18.33</td>\n",
              "      <td>1958</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.14</td>\n",
              "      <td>136300</td>\n",
              "      <td>164200</td>\n",
              "      <td>199900</td>\n",
              "      <td>63600</td>\n",
              "      <td>467</td>\n",
              "      <td>560</td>\n",
              "      <td>672</td>\n",
              "      <td>205</td>\n",
              "      <td>627</td>\n",
              "      <td>27.6</td>\n",
              "      <td>20.7</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.30</td>\n",
              "      <td>77.17</td>\n",
              "      <td>71.27</td>\n",
              "      <td>90.22</td>\n",
              "      <td>96.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2186.7</td>\n",
              "      <td>3.84</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>5</td>\n",
              "      <td>21.26</td>\n",
              "      <td>24</td>\n",
              "      <td>102.05</td>\n",
              "      <td>57</td>\n",
              "      <td>242.37</td>\n",
              "      <td>376</td>\n",
              "      <td>1598.78</td>\n",
              "      <td>26</td>\n",
              "      <td>110.55</td>\n",
              "      <td>1</td>\n",
              "      <td>4.25</td>\n",
              "      <td>127.56</td>\n",
              "      <td>1955.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tigardcity</td>\n",
              "      <td>OR</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>29344</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.74</td>\n",
              "      <td>94.33</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.35</td>\n",
              "      <td>11.36</td>\n",
              "      <td>25.88</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.28</td>\n",
              "      <td>29344</td>\n",
              "      <td>100.00</td>\n",
              "      <td>35669</td>\n",
              "      <td>82.00</td>\n",
              "      <td>1.15</td>\n",
              "      <td>55.73</td>\n",
              "      <td>22.25</td>\n",
              "      <td>2.94</td>\n",
              "      <td>14.56</td>\n",
              "      <td>42112</td>\n",
              "      <td>16946</td>\n",
              "      <td>17103</td>\n",
              "      <td>16644</td>\n",
              "      <td>21606</td>\n",
              "      <td>15528</td>\n",
              "      <td>5954</td>\n",
              "      <td>8405</td>\n",
              "      <td>1389</td>\n",
              "      <td>4.75</td>\n",
              "      <td>2.80</td>\n",
              "      <td>9.09</td>\n",
              "      <td>30.13</td>\n",
              "      <td>4.01</td>\n",
              "      <td>69.80</td>\n",
              "      <td>15.95</td>\n",
              "      <td>21.52</td>\n",
              "      <td>8.79</td>\n",
              "      <td>32.48</td>\n",
              "      <td>10.10</td>\n",
              "      <td>25.78</td>\n",
              "      <td>14.76</td>\n",
              "      <td>12.55</td>\n",
              "      <td>2.95</td>\n",
              "      <td>78.54</td>\n",
              "      <td>78.85</td>\n",
              "      <td>92.37</td>\n",
              "      <td>75.72</td>\n",
              "      <td>66.08</td>\n",
              "      <td>74.19</td>\n",
              "      <td>164</td>\n",
              "      <td>0.88</td>\n",
              "      <td>1468</td>\n",
              "      <td>16.42</td>\n",
              "      <td>23.98</td>\n",
              "      <td>32.08</td>\n",
              "      <td>35.63</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.78</td>\n",
              "      <td>93.11</td>\n",
              "      <td>1.14</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.42</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.06</td>\n",
              "      <td>64.18</td>\n",
              "      <td>2.03</td>\n",
              "      <td>47.46</td>\n",
              "      <td>3</td>\n",
              "      <td>544</td>\n",
              "      <td>95.68</td>\n",
              "      <td>57.79</td>\n",
              "      <td>0.92</td>\n",
              "      <td>7.54</td>\n",
              "      <td>1976</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.12</td>\n",
              "      <td>74700</td>\n",
              "      <td>90400</td>\n",
              "      <td>112000</td>\n",
              "      <td>37300</td>\n",
              "      <td>370</td>\n",
              "      <td>428</td>\n",
              "      <td>520</td>\n",
              "      <td>150</td>\n",
              "      <td>484</td>\n",
              "      <td>24.1</td>\n",
              "      <td>21.7</td>\n",
              "      <td>11.6</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>44.77</td>\n",
              "      <td>36.60</td>\n",
              "      <td>61.26</td>\n",
              "      <td>82.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10.6</td>\n",
              "      <td>2780.9</td>\n",
              "      <td>4.37</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>3</td>\n",
              "      <td>8.30</td>\n",
              "      <td>6</td>\n",
              "      <td>16.6</td>\n",
              "      <td>56</td>\n",
              "      <td>154.95</td>\n",
              "      <td>14</td>\n",
              "      <td>38.74</td>\n",
              "      <td>274</td>\n",
              "      <td>758.14</td>\n",
              "      <td>1797</td>\n",
              "      <td>4972.19</td>\n",
              "      <td>136</td>\n",
              "      <td>376.3</td>\n",
              "      <td>22</td>\n",
              "      <td>60.87</td>\n",
              "      <td>218.59</td>\n",
              "      <td>6167.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gloversvillecity</td>\n",
              "      <td>NY</td>\n",
              "      <td>35</td>\n",
              "      <td>29443</td>\n",
              "      <td>1</td>\n",
              "      <td>16656</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1.70</td>\n",
              "      <td>97.35</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.70</td>\n",
              "      <td>12.55</td>\n",
              "      <td>25.20</td>\n",
              "      <td>12.19</td>\n",
              "      <td>17.57</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20580</td>\n",
              "      <td>68.15</td>\n",
              "      <td>0.24</td>\n",
              "      <td>38.95</td>\n",
              "      <td>39.48</td>\n",
              "      <td>11.71</td>\n",
              "      <td>18.33</td>\n",
              "      <td>26501</td>\n",
              "      <td>10810</td>\n",
              "      <td>10909</td>\n",
              "      <td>9984</td>\n",
              "      <td>4941</td>\n",
              "      <td>3541</td>\n",
              "      <td>2451</td>\n",
              "      <td>4391</td>\n",
              "      <td>2831</td>\n",
              "      <td>17.23</td>\n",
              "      <td>11.05</td>\n",
              "      <td>33.68</td>\n",
              "      <td>10.81</td>\n",
              "      <td>9.86</td>\n",
              "      <td>54.74</td>\n",
              "      <td>31.22</td>\n",
              "      <td>27.43</td>\n",
              "      <td>26.76</td>\n",
              "      <td>22.71</td>\n",
              "      <td>10.98</td>\n",
              "      <td>28.15</td>\n",
              "      <td>14.47</td>\n",
              "      <td>12.91</td>\n",
              "      <td>2.98</td>\n",
              "      <td>64.02</td>\n",
              "      <td>62.36</td>\n",
              "      <td>65.38</td>\n",
              "      <td>67.43</td>\n",
              "      <td>59.59</td>\n",
              "      <td>70.27</td>\n",
              "      <td>561</td>\n",
              "      <td>3.84</td>\n",
              "      <td>339</td>\n",
              "      <td>13.86</td>\n",
              "      <td>13.86</td>\n",
              "      <td>15.34</td>\n",
              "      <td>15.34</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>94.98</td>\n",
              "      <td>0.56</td>\n",
              "      <td>3.93</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.51</td>\n",
              "      <td>2.20</td>\n",
              "      <td>58.18</td>\n",
              "      <td>1.21</td>\n",
              "      <td>45.66</td>\n",
              "      <td>3</td>\n",
              "      <td>669</td>\n",
              "      <td>91.19</td>\n",
              "      <td>54.89</td>\n",
              "      <td>2.54</td>\n",
              "      <td>57.85</td>\n",
              "      <td>1939</td>\n",
              "      <td>7.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>36400</td>\n",
              "      <td>49600</td>\n",
              "      <td>66500</td>\n",
              "      <td>30100</td>\n",
              "      <td>195</td>\n",
              "      <td>250</td>\n",
              "      <td>309</td>\n",
              "      <td>114</td>\n",
              "      <td>333</td>\n",
              "      <td>28.7</td>\n",
              "      <td>20.6</td>\n",
              "      <td>14.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.04</td>\n",
              "      <td>88.71</td>\n",
              "      <td>56.70</td>\n",
              "      <td>90.17</td>\n",
              "      <td>96.24</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>5.2</td>\n",
              "      <td>3217.7</td>\n",
              "      <td>3.31</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>10</td>\n",
              "      <td>57.86</td>\n",
              "      <td>33</td>\n",
              "      <td>190.93</td>\n",
              "      <td>225</td>\n",
              "      <td>1301.78</td>\n",
              "      <td>716</td>\n",
              "      <td>4142.56</td>\n",
              "      <td>47</td>\n",
              "      <td>271.93</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>306.64</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bemidjicity</td>\n",
              "      <td>MN</td>\n",
              "      <td>7</td>\n",
              "      <td>5068</td>\n",
              "      <td>1</td>\n",
              "      <td>11245</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.53</td>\n",
              "      <td>89.16</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.52</td>\n",
              "      <td>24.46</td>\n",
              "      <td>40.53</td>\n",
              "      <td>28.69</td>\n",
              "      <td>12.65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>17390</td>\n",
              "      <td>69.33</td>\n",
              "      <td>0.55</td>\n",
              "      <td>42.82</td>\n",
              "      <td>32.16</td>\n",
              "      <td>11.21</td>\n",
              "      <td>14.43</td>\n",
              "      <td>24018</td>\n",
              "      <td>8483</td>\n",
              "      <td>9009</td>\n",
              "      <td>887</td>\n",
              "      <td>4425</td>\n",
              "      <td>3352</td>\n",
              "      <td>3000</td>\n",
              "      <td>1328</td>\n",
              "      <td>2855</td>\n",
              "      <td>29.99</td>\n",
              "      <td>12.15</td>\n",
              "      <td>23.06</td>\n",
              "      <td>25.28</td>\n",
              "      <td>9.08</td>\n",
              "      <td>52.44</td>\n",
              "      <td>6.89</td>\n",
              "      <td>36.54</td>\n",
              "      <td>10.94</td>\n",
              "      <td>27.80</td>\n",
              "      <td>7.51</td>\n",
              "      <td>50.66</td>\n",
              "      <td>11.64</td>\n",
              "      <td>9.73</td>\n",
              "      <td>2.98</td>\n",
              "      <td>58.59</td>\n",
              "      <td>55.20</td>\n",
              "      <td>66.51</td>\n",
              "      <td>79.17</td>\n",
              "      <td>61.22</td>\n",
              "      <td>68.94</td>\n",
              "      <td>402</td>\n",
              "      <td>4.70</td>\n",
              "      <td>196</td>\n",
              "      <td>46.94</td>\n",
              "      <td>56.12</td>\n",
              "      <td>67.86</td>\n",
              "      <td>69.90</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.22</td>\n",
              "      <td>94.64</td>\n",
              "      <td>0.39</td>\n",
              "      <td>5.23</td>\n",
              "      <td>3.11</td>\n",
              "      <td>2.35</td>\n",
              "      <td>2.55</td>\n",
              "      <td>2.12</td>\n",
              "      <td>58.13</td>\n",
              "      <td>2.94</td>\n",
              "      <td>55.64</td>\n",
              "      <td>2</td>\n",
              "      <td>333</td>\n",
              "      <td>92.45</td>\n",
              "      <td>53.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>42.64</td>\n",
              "      <td>1958</td>\n",
              "      <td>7.45</td>\n",
              "      <td>0.82</td>\n",
              "      <td>30600</td>\n",
              "      <td>43200</td>\n",
              "      <td>59500</td>\n",
              "      <td>28900</td>\n",
              "      <td>202</td>\n",
              "      <td>283</td>\n",
              "      <td>362</td>\n",
              "      <td>160</td>\n",
              "      <td>332</td>\n",
              "      <td>32.2</td>\n",
              "      <td>23.2</td>\n",
              "      <td>12.9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.74</td>\n",
              "      <td>73.75</td>\n",
              "      <td>42.22</td>\n",
              "      <td>60.34</td>\n",
              "      <td>89.02</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>11.5</td>\n",
              "      <td>974.2</td>\n",
              "      <td>0.38</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>4</td>\n",
              "      <td>32.04</td>\n",
              "      <td>14</td>\n",
              "      <td>112.14</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>1060</td>\n",
              "      <td>8490.87</td>\n",
              "      <td>91</td>\n",
              "      <td>728.93</td>\n",
              "      <td>5</td>\n",
              "      <td>40.05</td>\n",
              "      <td>?</td>\n",
              "      <td>9988.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2210</th>\n",
              "      <td>Mercedcity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>56216</td>\n",
              "      <td>3.07</td>\n",
              "      <td>6.87</td>\n",
              "      <td>61.68</td>\n",
              "      <td>15.23</td>\n",
              "      <td>29.86</td>\n",
              "      <td>15.46</td>\n",
              "      <td>30.16</td>\n",
              "      <td>14.34</td>\n",
              "      <td>8.08</td>\n",
              "      <td>56216</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24727</td>\n",
              "      <td>75.05</td>\n",
              "      <td>1.12</td>\n",
              "      <td>31.42</td>\n",
              "      <td>21.45</td>\n",
              "      <td>19.98</td>\n",
              "      <td>14.41</td>\n",
              "      <td>27388</td>\n",
              "      <td>10237</td>\n",
              "      <td>13041</td>\n",
              "      <td>8344</td>\n",
              "      <td>8590</td>\n",
              "      <td>3399</td>\n",
              "      <td>6470</td>\n",
              "      <td>6644</td>\n",
              "      <td>13804</td>\n",
              "      <td>25.06</td>\n",
              "      <td>17.12</td>\n",
              "      <td>30.87</td>\n",
              "      <td>15.79</td>\n",
              "      <td>9.99</td>\n",
              "      <td>55.53</td>\n",
              "      <td>13.47</td>\n",
              "      <td>27.18</td>\n",
              "      <td>16.38</td>\n",
              "      <td>25.02</td>\n",
              "      <td>10.22</td>\n",
              "      <td>31.91</td>\n",
              "      <td>16.28</td>\n",
              "      <td>13.34</td>\n",
              "      <td>3.56</td>\n",
              "      <td>67.04</td>\n",
              "      <td>64.81</td>\n",
              "      <td>76.19</td>\n",
              "      <td>72.78</td>\n",
              "      <td>47.24</td>\n",
              "      <td>55.38</td>\n",
              "      <td>1960</td>\n",
              "      <td>4.49</td>\n",
              "      <td>10623</td>\n",
              "      <td>22.97</td>\n",
              "      <td>35.12</td>\n",
              "      <td>42.10</td>\n",
              "      <td>60.31</td>\n",
              "      <td>4.34</td>\n",
              "      <td>6.64</td>\n",
              "      <td>7.96</td>\n",
              "      <td>11.40</td>\n",
              "      <td>65.33</td>\n",
              "      <td>11.87</td>\n",
              "      <td>13.49</td>\n",
              "      <td>9.91</td>\n",
              "      <td>3.03</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.19</td>\n",
              "      <td>41.69</td>\n",
              "      <td>16.89</td>\n",
              "      <td>57.23</td>\n",
              "      <td>2</td>\n",
              "      <td>683</td>\n",
              "      <td>96.40</td>\n",
              "      <td>44.63</td>\n",
              "      <td>1.46</td>\n",
              "      <td>13.18</td>\n",
              "      <td>1973</td>\n",
              "      <td>4.91</td>\n",
              "      <td>0.55</td>\n",
              "      <td>71200</td>\n",
              "      <td>91100</td>\n",
              "      <td>118900</td>\n",
              "      <td>47700</td>\n",
              "      <td>298</td>\n",
              "      <td>374</td>\n",
              "      <td>455</td>\n",
              "      <td>157</td>\n",
              "      <td>438</td>\n",
              "      <td>29.8</td>\n",
              "      <td>22.6</td>\n",
              "      <td>11.7</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>18.90</td>\n",
              "      <td>52.67</td>\n",
              "      <td>39.19</td>\n",
              "      <td>74.58</td>\n",
              "      <td>85.88</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>16.7</td>\n",
              "      <td>3365.4</td>\n",
              "      <td>0.59</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>16.49</td>\n",
              "      <td>30</td>\n",
              "      <td>49.46</td>\n",
              "      <td>121</td>\n",
              "      <td>199.5</td>\n",
              "      <td>170</td>\n",
              "      <td>280.29</td>\n",
              "      <td>1376</td>\n",
              "      <td>2268.72</td>\n",
              "      <td>2563</td>\n",
              "      <td>4225.82</td>\n",
              "      <td>489</td>\n",
              "      <td>806.25</td>\n",
              "      <td>34</td>\n",
              "      <td>56.06</td>\n",
              "      <td>545.75</td>\n",
              "      <td>7356.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2211</th>\n",
              "      <td>Pinevillecity</td>\n",
              "      <td>LA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>12251</td>\n",
              "      <td>2.68</td>\n",
              "      <td>21.18</td>\n",
              "      <td>76.65</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.29</td>\n",
              "      <td>17.36</td>\n",
              "      <td>31.23</td>\n",
              "      <td>16.97</td>\n",
              "      <td>12.57</td>\n",
              "      <td>12251</td>\n",
              "      <td>100.00</td>\n",
              "      <td>20321</td>\n",
              "      <td>75.06</td>\n",
              "      <td>0.47</td>\n",
              "      <td>33.25</td>\n",
              "      <td>27.63</td>\n",
              "      <td>8.85</td>\n",
              "      <td>18.23</td>\n",
              "      <td>25000</td>\n",
              "      <td>9995</td>\n",
              "      <td>11353</td>\n",
              "      <td>5768</td>\n",
              "      <td>10910</td>\n",
              "      <td>1718</td>\n",
              "      <td>11471</td>\n",
              "      <td>4883</td>\n",
              "      <td>2364</td>\n",
              "      <td>20.79</td>\n",
              "      <td>12.51</td>\n",
              "      <td>27.71</td>\n",
              "      <td>19.28</td>\n",
              "      <td>7.90</td>\n",
              "      <td>54.64</td>\n",
              "      <td>7.81</td>\n",
              "      <td>37.06</td>\n",
              "      <td>10.37</td>\n",
              "      <td>28.73</td>\n",
              "      <td>10.86</td>\n",
              "      <td>29.51</td>\n",
              "      <td>16.12</td>\n",
              "      <td>13.77</td>\n",
              "      <td>3.12</td>\n",
              "      <td>68.57</td>\n",
              "      <td>63.66</td>\n",
              "      <td>80.29</td>\n",
              "      <td>73.68</td>\n",
              "      <td>64.20</td>\n",
              "      <td>66.67</td>\n",
              "      <td>277</td>\n",
              "      <td>2.98</td>\n",
              "      <td>275</td>\n",
              "      <td>2.91</td>\n",
              "      <td>45.09</td>\n",
              "      <td>65.45</td>\n",
              "      <td>75.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.70</td>\n",
              "      <td>92.78</td>\n",
              "      <td>0.86</td>\n",
              "      <td>5.03</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.49</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.39</td>\n",
              "      <td>56.06</td>\n",
              "      <td>3.99</td>\n",
              "      <td>54.48</td>\n",
              "      <td>2</td>\n",
              "      <td>523</td>\n",
              "      <td>89.72</td>\n",
              "      <td>54.24</td>\n",
              "      <td>4.59</td>\n",
              "      <td>46.08</td>\n",
              "      <td>1966</td>\n",
              "      <td>7.56</td>\n",
              "      <td>0.12</td>\n",
              "      <td>33600</td>\n",
              "      <td>52000</td>\n",
              "      <td>72700</td>\n",
              "      <td>39100</td>\n",
              "      <td>176</td>\n",
              "      <td>248</td>\n",
              "      <td>297</td>\n",
              "      <td>121</td>\n",
              "      <td>330</td>\n",
              "      <td>23.8</td>\n",
              "      <td>17.3</td>\n",
              "      <td>14.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.24</td>\n",
              "      <td>75.16</td>\n",
              "      <td>49.12</td>\n",
              "      <td>78.79</td>\n",
              "      <td>92.85</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>7.3</td>\n",
              "      <td>1682.8</td>\n",
              "      <td>1.15</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4</td>\n",
              "      <td>33.09</td>\n",
              "      <td>1</td>\n",
              "      <td>8.27</td>\n",
              "      <td>10</td>\n",
              "      <td>82.73</td>\n",
              "      <td>104</td>\n",
              "      <td>860.43</td>\n",
              "      <td>574</td>\n",
              "      <td>4748.9</td>\n",
              "      <td>24</td>\n",
              "      <td>198.56</td>\n",
              "      <td>2</td>\n",
              "      <td>16.55</td>\n",
              "      <td>124.1</td>\n",
              "      <td>5824.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2212</th>\n",
              "      <td>Yucaipacity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>32824</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>92.62</td>\n",
              "      <td>0.98</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.81</td>\n",
              "      <td>20.96</td>\n",
              "      <td>9.53</td>\n",
              "      <td>20.73</td>\n",
              "      <td>32824</td>\n",
              "      <td>100.00</td>\n",
              "      <td>27182</td>\n",
              "      <td>59.79</td>\n",
              "      <td>0.51</td>\n",
              "      <td>44.72</td>\n",
              "      <td>43.40</td>\n",
              "      <td>9.01</td>\n",
              "      <td>23.56</td>\n",
              "      <td>34973</td>\n",
              "      <td>14131</td>\n",
              "      <td>14416</td>\n",
              "      <td>13630</td>\n",
              "      <td>13197</td>\n",
              "      <td>17313</td>\n",
              "      <td>8532</td>\n",
              "      <td>9398</td>\n",
              "      <td>2460</td>\n",
              "      <td>7.56</td>\n",
              "      <td>7.82</td>\n",
              "      <td>26.14</td>\n",
              "      <td>12.42</td>\n",
              "      <td>5.18</td>\n",
              "      <td>50.54</td>\n",
              "      <td>9.34</td>\n",
              "      <td>23.36</td>\n",
              "      <td>13.53</td>\n",
              "      <td>23.54</td>\n",
              "      <td>9.89</td>\n",
              "      <td>20.56</td>\n",
              "      <td>12.38</td>\n",
              "      <td>11.23</td>\n",
              "      <td>2.99</td>\n",
              "      <td>76.77</td>\n",
              "      <td>74.20</td>\n",
              "      <td>76.92</td>\n",
              "      <td>82.42</td>\n",
              "      <td>55.73</td>\n",
              "      <td>62.62</td>\n",
              "      <td>434</td>\n",
              "      <td>1.60</td>\n",
              "      <td>2414</td>\n",
              "      <td>6.63</td>\n",
              "      <td>9.03</td>\n",
              "      <td>17.15</td>\n",
              "      <td>26.72</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.66</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.97</td>\n",
              "      <td>88.95</td>\n",
              "      <td>1.70</td>\n",
              "      <td>5.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.67</td>\n",
              "      <td>74.61</td>\n",
              "      <td>4.39</td>\n",
              "      <td>61.03</td>\n",
              "      <td>2</td>\n",
              "      <td>957</td>\n",
              "      <td>93.30</td>\n",
              "      <td>76.81</td>\n",
              "      <td>0.84</td>\n",
              "      <td>29.47</td>\n",
              "      <td>1967</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.27</td>\n",
              "      <td>91700</td>\n",
              "      <td>123900</td>\n",
              "      <td>164000</td>\n",
              "      <td>72300</td>\n",
              "      <td>347</td>\n",
              "      <td>451</td>\n",
              "      <td>551</td>\n",
              "      <td>204</td>\n",
              "      <td>514</td>\n",
              "      <td>30.5</td>\n",
              "      <td>23.9</td>\n",
              "      <td>13.1</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>7.35</td>\n",
              "      <td>48.66</td>\n",
              "      <td>46.73</td>\n",
              "      <td>75.54</td>\n",
              "      <td>92.30</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>27.5</td>\n",
              "      <td>1195.2</td>\n",
              "      <td>0.12</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>5</td>\n",
              "      <td>13.61</td>\n",
              "      <td>24</td>\n",
              "      <td>65.32</td>\n",
              "      <td>96</td>\n",
              "      <td>261.29</td>\n",
              "      <td>628</td>\n",
              "      <td>1709.26</td>\n",
              "      <td>895</td>\n",
              "      <td>2435.97</td>\n",
              "      <td>179</td>\n",
              "      <td>487.19</td>\n",
              "      <td>8</td>\n",
              "      <td>21.77</td>\n",
              "      <td>353.83</td>\n",
              "      <td>4654.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>Beevillecity</td>\n",
              "      <td>TX</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>13547</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.37</td>\n",
              "      <td>69.91</td>\n",
              "      <td>0.90</td>\n",
              "      <td>62.11</td>\n",
              "      <td>17.16</td>\n",
              "      <td>30.01</td>\n",
              "      <td>14.73</td>\n",
              "      <td>10.42</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19899</td>\n",
              "      <td>71.67</td>\n",
              "      <td>1.70</td>\n",
              "      <td>21.94</td>\n",
              "      <td>26.44</td>\n",
              "      <td>13.05</td>\n",
              "      <td>10.29</td>\n",
              "      <td>22103</td>\n",
              "      <td>8100</td>\n",
              "      <td>9555</td>\n",
              "      <td>6437</td>\n",
              "      <td>8271</td>\n",
              "      <td>171</td>\n",
              "      <td>4436</td>\n",
              "      <td>5338</td>\n",
              "      <td>4021</td>\n",
              "      <td>30.32</td>\n",
              "      <td>24.37</td>\n",
              "      <td>39.63</td>\n",
              "      <td>12.40</td>\n",
              "      <td>12.12</td>\n",
              "      <td>52.53</td>\n",
              "      <td>8.22</td>\n",
              "      <td>25.16</td>\n",
              "      <td>10.63</td>\n",
              "      <td>20.87</td>\n",
              "      <td>10.35</td>\n",
              "      <td>29.18</td>\n",
              "      <td>14.36</td>\n",
              "      <td>12.48</td>\n",
              "      <td>3.46</td>\n",
              "      <td>67.76</td>\n",
              "      <td>63.45</td>\n",
              "      <td>87.82</td>\n",
              "      <td>74.12</td>\n",
              "      <td>50.57</td>\n",
              "      <td>60.14</td>\n",
              "      <td>279</td>\n",
              "      <td>2.35</td>\n",
              "      <td>309</td>\n",
              "      <td>4.85</td>\n",
              "      <td>8.09</td>\n",
              "      <td>11.00</td>\n",
              "      <td>20.71</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.47</td>\n",
              "      <td>48.92</td>\n",
              "      <td>6.66</td>\n",
              "      <td>9.83</td>\n",
              "      <td>7.10</td>\n",
              "      <td>2.84</td>\n",
              "      <td>2.93</td>\n",
              "      <td>2.73</td>\n",
              "      <td>60.11</td>\n",
              "      <td>9.64</td>\n",
              "      <td>50.28</td>\n",
              "      <td>2</td>\n",
              "      <td>802</td>\n",
              "      <td>85.39</td>\n",
              "      <td>58.39</td>\n",
              "      <td>5.61</td>\n",
              "      <td>67.21</td>\n",
              "      <td>1964</td>\n",
              "      <td>15.91</td>\n",
              "      <td>0.87</td>\n",
              "      <td>26000</td>\n",
              "      <td>37800</td>\n",
              "      <td>52100</td>\n",
              "      <td>26100</td>\n",
              "      <td>135</td>\n",
              "      <td>227</td>\n",
              "      <td>317</td>\n",
              "      <td>182</td>\n",
              "      <td>316</td>\n",
              "      <td>26.2</td>\n",
              "      <td>23.3</td>\n",
              "      <td>14.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.28</td>\n",
              "      <td>82.26</td>\n",
              "      <td>54.05</td>\n",
              "      <td>79.72</td>\n",
              "      <td>94.06</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2142.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>15.71</td>\n",
              "      <td>7</td>\n",
              "      <td>54.98</td>\n",
              "      <td>79</td>\n",
              "      <td>620.48</td>\n",
              "      <td>192</td>\n",
              "      <td>1508.01</td>\n",
              "      <td>474</td>\n",
              "      <td>3722.9</td>\n",
              "      <td>13</td>\n",
              "      <td>102.1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.85</td>\n",
              "      <td>691.17</td>\n",
              "      <td>5340.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>WestSacramentocity</td>\n",
              "      <td>CA</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>28898</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.39</td>\n",
              "      <td>71.27</td>\n",
              "      <td>9.09</td>\n",
              "      <td>24.43</td>\n",
              "      <td>12.99</td>\n",
              "      <td>25.21</td>\n",
              "      <td>11.63</td>\n",
              "      <td>12.12</td>\n",
              "      <td>28664</td>\n",
              "      <td>99.19</td>\n",
              "      <td>23287</td>\n",
              "      <td>68.89</td>\n",
              "      <td>1.20</td>\n",
              "      <td>27.54</td>\n",
              "      <td>28.62</td>\n",
              "      <td>19.05</td>\n",
              "      <td>17.29</td>\n",
              "      <td>27897</td>\n",
              "      <td>11510</td>\n",
              "      <td>13074</td>\n",
              "      <td>8163</td>\n",
              "      <td>9874</td>\n",
              "      <td>6827</td>\n",
              "      <td>7540</td>\n",
              "      <td>8275</td>\n",
              "      <td>5287</td>\n",
              "      <td>18.50</td>\n",
              "      <td>13.93</td>\n",
              "      <td>33.68</td>\n",
              "      <td>8.86</td>\n",
              "      <td>9.27</td>\n",
              "      <td>53.35</td>\n",
              "      <td>9.44</td>\n",
              "      <td>17.75</td>\n",
              "      <td>19.61</td>\n",
              "      <td>17.44</td>\n",
              "      <td>15.77</td>\n",
              "      <td>29.72</td>\n",
              "      <td>18.84</td>\n",
              "      <td>17.31</td>\n",
              "      <td>3.20</td>\n",
              "      <td>62.64</td>\n",
              "      <td>60.23</td>\n",
              "      <td>72.43</td>\n",
              "      <td>66.15</td>\n",
              "      <td>50.44</td>\n",
              "      <td>57.32</td>\n",
              "      <td>1152</td>\n",
              "      <td>4.85</td>\n",
              "      <td>4765</td>\n",
              "      <td>31.54</td>\n",
              "      <td>39.50</td>\n",
              "      <td>46.32</td>\n",
              "      <td>55.97</td>\n",
              "      <td>5.20</td>\n",
              "      <td>6.51</td>\n",
              "      <td>7.64</td>\n",
              "      <td>9.23</td>\n",
              "      <td>74.98</td>\n",
              "      <td>7.76</td>\n",
              "      <td>8.58</td>\n",
              "      <td>5.70</td>\n",
              "      <td>2.58</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.76</td>\n",
              "      <td>52.72</td>\n",
              "      <td>11.31</td>\n",
              "      <td>61.63</td>\n",
              "      <td>2</td>\n",
              "      <td>600</td>\n",
              "      <td>94.85</td>\n",
              "      <td>55.68</td>\n",
              "      <td>3.67</td>\n",
              "      <td>28.67</td>\n",
              "      <td>1960</td>\n",
              "      <td>7.37</td>\n",
              "      <td>1.43</td>\n",
              "      <td>68400</td>\n",
              "      <td>87600</td>\n",
              "      <td>114900</td>\n",
              "      <td>46500</td>\n",
              "      <td>272</td>\n",
              "      <td>369</td>\n",
              "      <td>449</td>\n",
              "      <td>177</td>\n",
              "      <td>426</td>\n",
              "      <td>30.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>11.6</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>16.49</td>\n",
              "      <td>55.29</td>\n",
              "      <td>48.74</td>\n",
              "      <td>66.20</td>\n",
              "      <td>89.08</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>21.7</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>1.39</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>?</td>\n",
              "      <td>5</td>\n",
              "      <td>16.53</td>\n",
              "      <td>19</td>\n",
              "      <td>62.8</td>\n",
              "      <td>102</td>\n",
              "      <td>337.15</td>\n",
              "      <td>152</td>\n",
              "      <td>502.41</td>\n",
              "      <td>791</td>\n",
              "      <td>2614.53</td>\n",
              "      <td>1458</td>\n",
              "      <td>4819.2</td>\n",
              "      <td>405</td>\n",
              "      <td>1338.67</td>\n",
              "      <td>20</td>\n",
              "      <td>66.11</td>\n",
              "      <td>918.89</td>\n",
              "      <td>8838.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2215 rows  147 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                communityname state countyCode communityCode  fold  \\\n",
              "0     BerkeleyHeightstownship    NJ         39          5320     1   \n",
              "1              Marpletownship    PA         45         47616     1   \n",
              "2                  Tigardcity    OR          ?             ?     1   \n",
              "3            Gloversvillecity    NY         35         29443     1   \n",
              "4                 Bemidjicity    MN          7          5068     1   \n",
              "...                       ...   ...        ...           ...   ...   \n",
              "2210               Mercedcity    CA          ?             ?    10   \n",
              "2211            Pinevillecity    LA          ?             ?    10   \n",
              "2212              Yucaipacity    CA          ?             ?    10   \n",
              "2213             Beevillecity    TX          ?             ?    10   \n",
              "2214       WestSacramentocity    CA          ?             ?    10   \n",
              "\n",
              "      population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
              "0          11980           3.10          1.37         91.78          6.50   \n",
              "1          23123           2.82          0.80         95.57          3.44   \n",
              "2          29344           2.43          0.74         94.33          3.43   \n",
              "3          16656           2.40          1.70         97.35          0.50   \n",
              "4          11245           2.76          0.53         89.16          1.17   \n",
              "...          ...            ...           ...           ...           ...   \n",
              "2210       56216           3.07          6.87         61.68         15.23   \n",
              "2211       12251           2.68         21.18         76.65          1.52   \n",
              "2212       32824           2.46          0.52         92.62          0.98   \n",
              "2213       13547           2.89          3.37         69.91          0.90   \n",
              "2214       28898           2.61          2.39         71.27          9.09   \n",
              "\n",
              "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
              "0            1.88        12.47        21.44        10.93       11.33   \n",
              "1            0.85        11.01        21.30        10.48       17.18   \n",
              "2            2.35        11.36        25.88        11.01       10.28   \n",
              "3            0.70        12.55        25.20        12.19       17.57   \n",
              "4            0.52        24.46        40.53        28.69       12.65   \n",
              "...           ...          ...          ...          ...         ...   \n",
              "2210        29.86        15.46        30.16        14.34        8.08   \n",
              "2211         1.29        17.36        31.23        16.97       12.57   \n",
              "2212        11.00        11.81        20.96         9.53       20.73   \n",
              "2213        62.11        17.16        30.01        14.73       10.42   \n",
              "2214        24.43        12.99        25.21        11.63       12.12   \n",
              "\n",
              "      numbUrban  pctUrban  medIncome  pctWWage  pctWFarmSelf  pctWInvInc  \\\n",
              "0         11980    100.00      75122     89.24          1.55       70.20   \n",
              "1         23123    100.00      47917     78.99          1.11       64.11   \n",
              "2         29344    100.00      35669     82.00          1.15       55.73   \n",
              "3             0      0.00      20580     68.15          0.24       38.95   \n",
              "4             0      0.00      17390     69.33          0.55       42.82   \n",
              "...         ...       ...        ...       ...           ...         ...   \n",
              "2210      56216    100.00      24727     75.05          1.12       31.42   \n",
              "2211      12251    100.00      20321     75.06          0.47       33.25   \n",
              "2212      32824    100.00      27182     59.79          0.51       44.72   \n",
              "2213          0      0.00      19899     71.67          1.70       21.94   \n",
              "2214      28664     99.19      23287     68.89          1.20       27.54   \n",
              "\n",
              "      pctWSocSec  pctWPubAsst  pctWRetire  medFamInc  perCapInc  whitePerCap  \\\n",
              "0          23.62         1.03       18.39      79584      29711        30233   \n",
              "1          35.50         2.75       22.85      55323      20148        20191   \n",
              "2          22.25         2.94       14.56      42112      16946        17103   \n",
              "3          39.48        11.71       18.33      26501      10810        10909   \n",
              "4          32.16        11.21       14.43      24018       8483         9009   \n",
              "...          ...          ...         ...        ...        ...          ...   \n",
              "2210       21.45        19.98       14.41      27388      10237        13041   \n",
              "2211       27.63         8.85       18.23      25000       9995        11353   \n",
              "2212       43.40         9.01       23.56      34973      14131        14416   \n",
              "2213       26.44        13.05       10.29      22103       8100         9555   \n",
              "2214       28.62        19.05       17.29      27897      11510        13074   \n",
              "\n",
              "      blackPerCap  indianPerCap  AsianPerCap OtherPerCap  HispPerCap  \\\n",
              "0           13600          5725        27101        5115       22838   \n",
              "1           18137             0        20074        5250       12222   \n",
              "2           16644         21606        15528        5954        8405   \n",
              "3            9984          4941         3541        2451        4391   \n",
              "4             887          4425         3352        3000        1328   \n",
              "...           ...           ...          ...         ...         ...   \n",
              "2210         8344          8590         3399        6470        6644   \n",
              "2211         5768         10910         1718       11471        4883   \n",
              "2212        13630         13197        17313        8532        9398   \n",
              "2213         6437          8271          171        4436        5338   \n",
              "2214         8163          9874         6827        7540        8275   \n",
              "\n",
              "      NumUnderPov  PctPopUnderPov  PctLess9thGrade  PctNotHSGrad  PctBSorMore  \\\n",
              "0             227            1.96             5.81          9.90        48.18   \n",
              "1             885            3.98             5.61         13.72        29.89   \n",
              "2            1389            4.75             2.80          9.09        30.13   \n",
              "3            2831           17.23            11.05         33.68        10.81   \n",
              "4            2855           29.99            12.15         23.06        25.28   \n",
              "...           ...             ...              ...           ...          ...   \n",
              "2210        13804           25.06            17.12         30.87        15.79   \n",
              "2211         2364           20.79            12.51         27.71        19.28   \n",
              "2212         2460            7.56             7.82         26.14        12.42   \n",
              "2213         4021           30.32            24.37         39.63        12.40   \n",
              "2214         5287           18.50            13.93         33.68         8.86   \n",
              "\n",
              "      PctUnemployed  PctEmploy  PctEmplManu  PctEmplProfServ  PctOccupManu  \\\n",
              "0              2.70      64.55        14.65            28.82          5.49   \n",
              "1              2.43      61.96        12.26            29.28          6.39   \n",
              "2              4.01      69.80        15.95            21.52          8.79   \n",
              "3              9.86      54.74        31.22            27.43         26.76   \n",
              "4              9.08      52.44         6.89            36.54         10.94   \n",
              "...             ...        ...          ...              ...           ...   \n",
              "2210           9.99      55.53        13.47            27.18         16.38   \n",
              "2211           7.90      54.64         7.81            37.06         10.37   \n",
              "2212           5.18      50.54         9.34            23.36         13.53   \n",
              "2213          12.12      52.53         8.22            25.16         10.63   \n",
              "2214           9.27      53.35         9.44            17.75         19.61   \n",
              "\n",
              "      PctOccupMgmtProf  MalePctDivorce  MalePctNevMarr  FemalePctDiv  \\\n",
              "0                50.73            3.67           26.38          5.22   \n",
              "1                37.64            4.23           27.99          6.45   \n",
              "2                32.48           10.10           25.78         14.76   \n",
              "3                22.71           10.98           28.15         14.47   \n",
              "4                27.80            7.51           50.66         11.64   \n",
              "...                ...             ...             ...           ...   \n",
              "2210             25.02           10.22           31.91         16.28   \n",
              "2211             28.73           10.86           29.51         16.12   \n",
              "2212             23.54            9.89           20.56         12.38   \n",
              "2213             20.87           10.35           29.18         14.36   \n",
              "2214             17.44           15.77           29.72         18.84   \n",
              "\n",
              "      TotalPctDiv  PersPerFam  PctFam2Par  PctKids2Par  PctYoungKids2Par  \\\n",
              "0            4.47        3.22       91.43        90.17             95.78   \n",
              "1            5.42        3.11       86.91        85.33             96.82   \n",
              "2           12.55        2.95       78.54        78.85             92.37   \n",
              "3           12.91        2.98       64.02        62.36             65.38   \n",
              "4            9.73        2.98       58.59        55.20             66.51   \n",
              "...           ...         ...         ...          ...               ...   \n",
              "2210        13.34        3.56       67.04        64.81             76.19   \n",
              "2211        13.77        3.12       68.57        63.66             80.29   \n",
              "2212        11.23        2.99       76.77        74.20             76.92   \n",
              "2213        12.48        3.46       67.76        63.45             87.82   \n",
              "2214        17.31        3.20       62.64        60.23             72.43   \n",
              "\n",
              "      PctTeen2Par  PctWorkMomYoungKids  PctWorkMom  NumKidsBornNeverMar  \\\n",
              "0           95.81                44.56       58.88                   31   \n",
              "1           86.46                51.14       62.43                   43   \n",
              "2           75.72                66.08       74.19                  164   \n",
              "3           67.43                59.59       70.27                  561   \n",
              "4           79.17                61.22       68.94                  402   \n",
              "...           ...                  ...         ...                  ...   \n",
              "2210        72.78                47.24       55.38                 1960   \n",
              "2211        73.68                64.20       66.67                  277   \n",
              "2212        82.42                55.73       62.62                  434   \n",
              "2213        74.12                50.57       60.14                  279   \n",
              "2214        66.15                50.44       57.32                 1152   \n",
              "\n",
              "      PctKidsBornNeverMar  NumImmig  PctImmigRecent  PctImmigRec5  \\\n",
              "0                    0.36      1277            8.69         13.00   \n",
              "1                    0.24      1920            5.21          8.65   \n",
              "2                    0.88      1468           16.42         23.98   \n",
              "3                    3.84       339           13.86         13.86   \n",
              "4                    4.70       196           46.94         56.12   \n",
              "...                   ...       ...             ...           ...   \n",
              "2210                 4.49     10623           22.97         35.12   \n",
              "2211                 2.98       275            2.91         45.09   \n",
              "2212                 1.60      2414            6.63          9.03   \n",
              "2213                 2.35       309            4.85          8.09   \n",
              "2214                 4.85      4765           31.54         39.50   \n",
              "\n",
              "      PctImmigRec8  PctImmigRec10  PctRecentImmig  PctRecImmig5  PctRecImmig8  \\\n",
              "0            20.99          30.93            0.93          1.39          2.24   \n",
              "1            13.33          22.50            0.43          0.72          1.11   \n",
              "2            32.08          35.63            0.82          1.20          1.61   \n",
              "3            15.34          15.34            0.28          0.28          0.31   \n",
              "4            67.86          69.90            0.82          0.98          1.18   \n",
              "...            ...            ...             ...           ...           ...   \n",
              "2210         42.10          60.31            4.34          6.64          7.96   \n",
              "2211         65.45          75.64            0.07          1.01          1.47   \n",
              "2212         17.15          26.72            0.49          0.66          1.26   \n",
              "2213         11.00          20.71            0.11          0.18          0.25   \n",
              "2214         46.32          55.97            5.20          6.51          7.64   \n",
              "\n",
              "      PctRecImmig10  PctSpeakEnglOnly  PctNotSpeakEnglWell  PctLargHouseFam  \\\n",
              "0              3.30             85.68                 1.37             4.81   \n",
              "1              1.87             87.79                 1.81             4.25   \n",
              "2              1.78             93.11                 1.14             2.97   \n",
              "3              0.31             94.98                 0.56             3.93   \n",
              "4              1.22             94.64                 0.39             5.23   \n",
              "...             ...               ...                  ...              ...   \n",
              "2210          11.40             65.33                11.87            13.49   \n",
              "2211           1.70             92.78                 0.86             5.03   \n",
              "2212           1.97             88.95                 1.70             5.10   \n",
              "2213           0.47             48.92                 6.66             9.83   \n",
              "2214           9.23             74.98                 7.76             8.58   \n",
              "\n",
              "      PctLargHouseOccup  PersPerOccupHous  PersPerOwnOccHous  \\\n",
              "0                  4.17              2.99               3.00   \n",
              "1                  3.34              2.70               2.83   \n",
              "2                  2.05              2.42               2.69   \n",
              "3                  2.56              2.37               2.51   \n",
              "4                  3.11              2.35               2.55   \n",
              "...                 ...               ...                ...   \n",
              "2210               9.91              3.03               2.83   \n",
              "2211               3.37              2.49               2.58   \n",
              "2212               3.50              2.44               2.37   \n",
              "2213               7.10              2.84               2.93   \n",
              "2214               5.70              2.58               2.45   \n",
              "\n",
              "      PersPerRentOccHous  PctPersOwnOccup  PctPersDenseHous  PctHousLess3BR  \\\n",
              "0                   2.84            91.46              0.39           11.06   \n",
              "1                   1.96            89.03              1.01           23.60   \n",
              "2                   2.06            64.18              2.03           47.46   \n",
              "3                   2.20            58.18              1.21           45.66   \n",
              "4                   2.12            58.13              2.94           55.64   \n",
              "...                  ...              ...               ...             ...   \n",
              "2210                3.19            41.69             16.89           57.23   \n",
              "2211                2.39            56.06              3.99           54.48   \n",
              "2212                2.67            74.61              4.39           61.03   \n",
              "2213                2.73            60.11              9.64           50.28   \n",
              "2214                2.76            52.72             11.31           61.63   \n",
              "\n",
              "      MedNumBR  HousVacant  PctHousOccup  PctHousOwnOcc  PctVacantBoarded  \\\n",
              "0            3          64         98.37          91.01              3.12   \n",
              "1            3         240         97.15          84.88              0.00   \n",
              "2            3         544         95.68          57.79              0.92   \n",
              "3            3         669         91.19          54.89              2.54   \n",
              "4            2         333         92.45          53.57              3.90   \n",
              "...        ...         ...           ...            ...               ...   \n",
              "2210         2         683         96.40          44.63              1.46   \n",
              "2211         2         523         89.72          54.24              4.59   \n",
              "2212         2         957         93.30          76.81              0.84   \n",
              "2213         2         802         85.39          58.39              5.61   \n",
              "2214         2         600         94.85          55.68              3.67   \n",
              "\n",
              "      PctVacMore6Mos  MedYrHousBuilt  PctHousNoPhone  PctWOFullPlumb  \\\n",
              "0              37.50            1959            0.00            0.28   \n",
              "1              18.33            1958            0.31            0.14   \n",
              "2               7.54            1976            1.55            0.12   \n",
              "3              57.85            1939            7.00            0.87   \n",
              "4              42.64            1958            7.45            0.82   \n",
              "...              ...             ...             ...             ...   \n",
              "2210           13.18            1973            4.91            0.55   \n",
              "2211           46.08            1966            7.56            0.12   \n",
              "2212           29.47            1967            2.51            0.27   \n",
              "2213           67.21            1964           15.91            0.87   \n",
              "2214           28.67            1960            7.37            1.43   \n",
              "\n",
              "      OwnOccLowQuart  OwnOccMedVal  OwnOccHiQuart  OwnOccQrange  RentLowQ  \\\n",
              "0             215900        262600         326900        111000       685   \n",
              "1             136300        164200         199900         63600       467   \n",
              "2              74700         90400         112000         37300       370   \n",
              "3              36400         49600          66500         30100       195   \n",
              "4              30600         43200          59500         28900       202   \n",
              "...              ...           ...            ...           ...       ...   \n",
              "2210           71200         91100         118900         47700       298   \n",
              "2211           33600         52000          72700         39100       176   \n",
              "2212           91700        123900         164000         72300       347   \n",
              "2213           26000         37800          52100         26100       135   \n",
              "2214           68400         87600         114900         46500       272   \n",
              "\n",
              "      RentMedian  RentHighQ  RentQrange  MedRent  MedRentPctHousInc  \\\n",
              "0           1001       1001         316     1001               23.8   \n",
              "1            560        672         205      627               27.6   \n",
              "2            428        520         150      484               24.1   \n",
              "3            250        309         114      333               28.7   \n",
              "4            283        362         160      332               32.2   \n",
              "...          ...        ...         ...      ...                ...   \n",
              "2210         374        455         157      438               29.8   \n",
              "2211         248        297         121      330               23.8   \n",
              "2212         451        551         204      514               30.5   \n",
              "2213         227        317         182      316               26.2   \n",
              "2214         369        449         177      426               30.9   \n",
              "\n",
              "      MedOwnCostPctInc  MedOwnCostPctIncNoMtg  NumInShelters  NumStreet  \\\n",
              "0                 21.1                   14.0             11          0   \n",
              "1                 20.7                   12.5              0          0   \n",
              "2                 21.7                   11.6             16          0   \n",
              "3                 20.6                   14.5              0          0   \n",
              "4                 23.2                   12.9              2          0   \n",
              "...                ...                    ...            ...        ...   \n",
              "2210              22.6                   11.7             64          0   \n",
              "2211              17.3                   14.4              0          0   \n",
              "2212              23.9                   13.1             44          0   \n",
              "2213              23.3                   14.1              0          0   \n",
              "2214              21.2                   11.6             10          2   \n",
              "\n",
              "      PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
              "0              10.66             53.72           65.29          78.09   \n",
              "1               8.30             77.17           71.27          90.22   \n",
              "2               5.00             44.77           36.60          61.26   \n",
              "3               2.04             88.71           56.70          90.17   \n",
              "4               1.74             73.75           42.22          60.34   \n",
              "...              ...               ...             ...            ...   \n",
              "2210           18.90             52.67           39.19          74.58   \n",
              "2211            2.24             75.16           49.12          78.79   \n",
              "2212            7.35             48.66           46.73          75.54   \n",
              "2213            2.28             82.26           54.05          79.72   \n",
              "2214           16.49             55.29           48.74          66.20   \n",
              "\n",
              "      PctSameState85 LemasSwornFT LemasSwFTPerPop LemasSwFTFieldOps  \\\n",
              "0              89.14            ?               ?                 ?   \n",
              "1              96.12            ?               ?                 ?   \n",
              "2              82.85            ?               ?                 ?   \n",
              "3              96.24            ?               ?                 ?   \n",
              "4              89.02            ?               ?                 ?   \n",
              "...              ...          ...             ...               ...   \n",
              "2210           85.88            ?               ?                 ?   \n",
              "2211           92.85            ?               ?                 ?   \n",
              "2212           92.30            ?               ?                 ?   \n",
              "2213           94.06            ?               ?                 ?   \n",
              "2214           89.08            ?               ?                 ?   \n",
              "\n",
              "     LemasSwFTFieldPerPop LemasTotalReq LemasTotReqPerPop PolicReqPerOffic  \\\n",
              "0                       ?             ?                 ?                ?   \n",
              "1                       ?             ?                 ?                ?   \n",
              "2                       ?             ?                 ?                ?   \n",
              "3                       ?             ?                 ?                ?   \n",
              "4                       ?             ?                 ?                ?   \n",
              "...                   ...           ...               ...              ...   \n",
              "2210                    ?             ?                 ?                ?   \n",
              "2211                    ?             ?                 ?                ?   \n",
              "2212                    ?             ?                 ?                ?   \n",
              "2213                    ?             ?                 ?                ?   \n",
              "2214                    ?             ?                 ?                ?   \n",
              "\n",
              "     PolicPerPop RacialMatchCommPol PctPolicWhite PctPolicBlack PctPolicHisp  \\\n",
              "0              ?                  ?             ?             ?            ?   \n",
              "1              ?                  ?             ?             ?            ?   \n",
              "2              ?                  ?             ?             ?            ?   \n",
              "3              ?                  ?             ?             ?            ?   \n",
              "4              ?                  ?             ?             ?            ?   \n",
              "...          ...                ...           ...           ...          ...   \n",
              "2210           ?                  ?             ?             ?            ?   \n",
              "2211           ?                  ?             ?             ?            ?   \n",
              "2212           ?                  ?             ?             ?            ?   \n",
              "2213           ?                  ?             ?             ?            ?   \n",
              "2214           ?                  ?             ?             ?            ?   \n",
              "\n",
              "     PctPolicAsian PctPolicMinor OfficAssgnDrugUnits NumKindsDrugsSeiz  \\\n",
              "0                ?             ?                   ?                 ?   \n",
              "1                ?             ?                   ?                 ?   \n",
              "2                ?             ?                   ?                 ?   \n",
              "3                ?             ?                   ?                 ?   \n",
              "4                ?             ?                   ?                 ?   \n",
              "...            ...           ...                 ...               ...   \n",
              "2210             ?             ?                   ?                 ?   \n",
              "2211             ?             ?                   ?                 ?   \n",
              "2212             ?             ?                   ?                 ?   \n",
              "2213             ?             ?                   ?                 ?   \n",
              "2214             ?             ?                   ?                 ?   \n",
              "\n",
              "     PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans PolicCars  \\\n",
              "0                   ?       6.5   1845.9            9.63         ?   \n",
              "1                   ?      10.6   2186.7            3.84         ?   \n",
              "2                   ?      10.6   2780.9            4.37         ?   \n",
              "3                   ?       5.2   3217.7            3.31         ?   \n",
              "4                   ?      11.5    974.2            0.38         ?   \n",
              "...               ...       ...      ...             ...       ...   \n",
              "2210                ?      16.7   3365.4            0.59         ?   \n",
              "2211                ?       7.3   1682.8            1.15         ?   \n",
              "2212                ?      27.5   1195.2            0.12         ?   \n",
              "2213                ?       6.3   2142.2            0.00         ?   \n",
              "2214                ?      21.7   1331.0            1.39         ?   \n",
              "\n",
              "     PolicOperBudg LemasPctPolicOnPatr LemasGangUnitDeploy  \\\n",
              "0                ?                   ?                   ?   \n",
              "1                ?                   ?                   ?   \n",
              "2                ?                   ?                   ?   \n",
              "3                ?                   ?                   ?   \n",
              "4                ?                   ?                   ?   \n",
              "...            ...                 ...                 ...   \n",
              "2210             ?                   ?                   ?   \n",
              "2211             ?                   ?                   ?   \n",
              "2212             ?                   ?                   ?   \n",
              "2213             ?                   ?                   ?   \n",
              "2214             ?                   ?                   ?   \n",
              "\n",
              "      LemasPctOfficDrugUn PolicBudgPerPop  murders  murdPerPop rapes  \\\n",
              "0                     0.0               ?        0        0.00     0   \n",
              "1                     0.0               ?        0        0.00     1   \n",
              "2                     0.0               ?        3        8.30     6   \n",
              "3                     0.0               ?        0        0.00    10   \n",
              "4                     0.0               ?        0        0.00     ?   \n",
              "...                   ...             ...      ...         ...   ...   \n",
              "2210                  0.0               ?       10       16.49    30   \n",
              "2211                  0.0               ?        0        0.00     4   \n",
              "2212                  0.0               ?        5       13.61     5   \n",
              "2213                  0.0               ?        0        0.00     2   \n",
              "2214                  0.0               ?        5       16.53    19   \n",
              "\n",
              "     rapesPerPop robberies robbbPerPop assaults assaultPerPop burglaries  \\\n",
              "0              0         1         8.2        4         32.81         14   \n",
              "1           4.25         5       21.26       24        102.05         57   \n",
              "2           16.6        56      154.95       14         38.74        274   \n",
              "3          57.86        10       57.86       33        190.93        225   \n",
              "4              ?         4       32.04       14        112.14         91   \n",
              "...          ...       ...         ...      ...           ...        ...   \n",
              "2210       49.46       121       199.5      170        280.29       1376   \n",
              "2211       33.09         1        8.27       10         82.73        104   \n",
              "2212       13.61        24       65.32       96        261.29        628   \n",
              "2213       15.71         7       54.98       79        620.48        192   \n",
              "2214        62.8       102      337.15      152        502.41        791   \n",
              "\n",
              "     burglPerPop larcenies larcPerPop autoTheft autoTheftPerPop arsons  \\\n",
              "0         114.85       138    1132.08        16          131.26      2   \n",
              "1         242.37       376    1598.78        26          110.55      1   \n",
              "2         758.14      1797    4972.19       136           376.3     22   \n",
              "3        1301.78       716    4142.56        47          271.93      ?   \n",
              "4         728.93      1060    8490.87        91          728.93      5   \n",
              "...          ...       ...        ...       ...             ...    ...   \n",
              "2210     2268.72      2563    4225.82       489          806.25     34   \n",
              "2211      860.43       574     4748.9        24          198.56      2   \n",
              "2212     1709.26       895    2435.97       179          487.19      8   \n",
              "2213     1508.01       474     3722.9        13           102.1      1   \n",
              "2214     2614.53      1458     4819.2       405         1338.67     20   \n",
              "\n",
              "     arsonsPerPop ViolentCrimesPerPop nonViolPerPop  \n",
              "0           16.41               41.02       1394.59  \n",
              "1            4.25              127.56       1955.95  \n",
              "2           60.87              218.59       6167.51  \n",
              "3               ?              306.64             ?  \n",
              "4           40.05                   ?       9988.79  \n",
              "...           ...                 ...           ...  \n",
              "2210        56.06              545.75       7356.84  \n",
              "2211        16.55               124.1       5824.44  \n",
              "2212        21.77              353.83        4654.2  \n",
              "2213         7.85              691.17       5340.87  \n",
              "2214        66.11              918.89        8838.5  \n",
              "\n",
              "[2215 rows x 147 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPgz9d88m7uk",
        "colab_type": "text"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0o5hcj5R_Iv",
        "colab_type": "text"
      },
      "source": [
        "Drop Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAUHyexwSBCL",
        "colab_type": "text"
      },
      "source": [
        "Replace values with NaNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvejj-DXdB4x",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEHAb7dqE2wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_communities.info)\n",
        "print(df_communities.describe())\n",
        "print(df_communities.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rh9V3tHEqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = \"Risk\" # 1 = \"Good\"; 0 = \"Bad\"\n",
        "\n",
        "eda_descr_stats(df_communities, disc_feature=\"Sex\", disc_min_value=\"female\", label = \"Risk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8f4lMHddc9c",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff0bvUYDdk5i",
        "colab_type": "text"
      },
      "source": [
        "#### 0) Preprocess data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhjE6ELVFJFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")\n",
        "\n",
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd0EZIuxd4aO",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Define Hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LXwjKNF33U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_classifier_compas = hyperparameter_tuning(df_compas_train_input, df_compas_train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgdMZOueEaV",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E8VoqTFQ-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                  700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                  4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_compas[\"race\"].isin([\"African-American\"])\n",
        "is_white = df_compas[\"race\"].isin([\"Caucasian\"])\n",
        "df_compas_black = df_compas[is_black]  # Minority\n",
        "df_compas_white = df_compas[is_white]  # Majority\n",
        "\n",
        "list_dfs_compas = create_datasets(min_data = df_compas_black, maj_data = df_compas_white, training_sizes=training_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXtqNpMaeCVz",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Create dataframes with diff. metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2kvEbOCRFc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_dfs = list_dfs_compas\n",
        "label = \"two_year_recid\"\n",
        "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                              criterion='gini', max_depth=25, max_features='auto',\n",
        "                              max_leaf_nodes=None, max_samples=None,\n",
        "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                              min_samples_leaf=1, min_samples_split=5,\n",
        "                              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                              n_jobs=None, oob_score=False, random_state=None,\n",
        "                              verbose=0, warm_start=False)\n",
        "\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "\n",
        "cv = 5 # To Do: For final calculations, change to cv = 10\n",
        "discr_feature = \"race\"\n",
        "min_value = \"African-American\"\n",
        "maj_value = \"Caucasian\"\n",
        "\n",
        "results_df_compas = metrics_to_df(list_dfs=list_dfs_compas, label = label, model = model, \n",
        "                                  cv = cv, discr_feature = \"race\", min_value = min_value,\n",
        "                                  maj_value = maj_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIcGDseeUin",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF3SKKuaRPGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B: Learning Curve Function from scikit learn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) \n",
        "\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = model, \n",
        "                    title = \"Communities and Crime Dataset - Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 5, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = np.linspace(.1, 1.0, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvuD9nZRU1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_metrics_line_chart(results_df_compas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtBH_by9RaCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maj_min_metrics_line_chart(results_df_compas) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLR_jzFLgHY",
        "colab_type": "text"
      },
      "source": [
        "Ideas\n",
        "- Individual Metric dfs zusammenmergen\n",
        "- Rows Minority als key dafr nehmen\n",
        "- Auf Kernmetriken fokussieren, diese dann umbenennen\n",
        "- Kernmetriken von verschiedenen Datasets in zentralem Line Chart zeigen\n",
        "- Dann Korridor einzeichnen, wo die Knickpunkte sich jeweils befinden\n",
        "- berlappungen von Korridoren zeigen, indem sich durch alpha die Farbe gegenseitig verstrkt\n",
        "- Dann Tabelle rechts daneben platzieren, die Zusatzinformationen bietet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUBpyP0PcQZX",
        "colab_type": "text"
      },
      "source": [
        "# Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqOWar36y6bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest Hyperparameter Tuning Results\n",
        "\n",
        "# model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                               criterion='gini', max_depth=25, max_features='auto',\n",
        "#                               max_leaf_nodes=None, max_samples=None,\n",
        "#                               min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                               min_samples_leaf=1, min_samples_split=5,\n",
        "#                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "#                               n_jobs=None, oob_score=False, random_state=None,\n",
        "#                               verbose=0, warm_start=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9y_W-gJcV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RF Hyperparameter Tuning\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "def hyperparameter_tuning(df_train_input, df_train_label):\n",
        "\n",
        "  from sklearn import model_selection\n",
        "\n",
        "  # Create the hyperparameter grid\n",
        "  # Important: Keys in the dictionary must be valid hyperparameters \n",
        "  param_grid = {\"learning_rate\": [0.3, 0.1],\n",
        "                \"max_depth\": [3, 6, 9], \n",
        "                \"min_child_weight\": [1, 3, 5],       \n",
        "                \"reg_lambda\": [2, 5, 10, 15, 100]}\n",
        "\n",
        "  # 1. Fundamental hyperparameters\n",
        "  # learning rate & number of trees\n",
        "\n",
        "  xgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
        "                     min_child_weight=1, gamma=0, subsample=0.8,\n",
        "                     colsample_bytree=0.8, objective= 'binary:logistic', \n",
        "                     scale_pos_weight=1, seed=27)\n",
        "\n",
        "\n",
        "  # n_estimators = number of trees in the foreset\n",
        "  # max_features = max number of features considered for splitting a node\n",
        "  # max_depth = max number of levels in each decision tree\n",
        "  # min_samples_split = min number of data points placed in a node before the node is split\n",
        "  # min_samples_leaf = min number of data points allowed in a leaf node\n",
        "  # bootstrap = method for sampling data points (with or without replacement)\n",
        "\n",
        "  # Define classifier\n",
        "  rf_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                          max_features = \"auto\")\n",
        "\n",
        "  # Learning Curve for Slice \n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "\n",
        "  # Define model\n",
        "  f1 = make_scorer(f1_score)\n",
        "\n",
        "  # Dummy Coding\n",
        "  df_train_input_dummy = pd.get_dummies(df_train_input)\n",
        "\n",
        "  grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                      param_grid = param_grid, \n",
        "                                                      scoring= f1,\n",
        "                                                      n_jobs = 2, \n",
        "                                                      cv = 5,\n",
        "                                                      refit = True,\n",
        "                                                      return_train_score = True)\n",
        "  \n",
        "  # Fit model\n",
        "  grid_rf_class.fit(df_train_input_dummy, df_train_label)\n",
        "\n",
        "  return(grid_rf_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWQHazJKBKYc",
        "colab_type": "text"
      },
      "source": [
        "**Steps**\n",
        "1. Specify hyperparameters for the chosen classification algorithm by means of grid search using standard ranges. Specify by means of on the dataset as a whole (full min. and maj.).\n",
        "2. Specify what is the majority and what is the minority class based on a value of a feature that identifies groups that are at risk of being discriminated. \n",
        "3. First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. For each step, train a random forest and test by mans of 10-fold cross validation.\n",
        "4. Attach the final outcome predictions  to the individual dataframes of the different datsets with diff. min. group sizes as seperate feature. -> *Open questions: is that manageable because we have different models (e.g. by means of majority vote of the 10 models with regards to classification)? If not possible then necessary to store the metric information directly without data frame as intermediate step (for that check calculation of fairness metric) (-> then also easier to store validation score to have an underfit/overfit check) OR use train/test split instead of k-fold cv (least prefered)*\n",
        "- a) Then filter that dataset so that only the minority group is displayed. Then, calculate  the confusion matrix and the F1-score. \n",
        "- b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric).\n",
        "5. Store the F1-score and the fairness metric for each training set size in a data frame. The data frame should have the following features: \n",
        "- a) Absolute number of training examples for the minority group, \n",
        "- b) percentage share of the minority group in the majority, \n",
        "- c) F-1 score for the min. group (maybe also seperately including std. dev. upper and lower bound), \n",
        "- d) fairness metric based on the min. & maj. group prediction comparisons. **Extended**: in case the fairness metric is not in the range of [0, 1] like the F1-score, normalize/standardize it (make sure that this is not biasing/altering the metrics results).\n",
        "6. Based on the data frame from step 5, plot a learning curve that shows the relationship between the different absolute sizes of training examples and the performance and fairness metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIkI8slaAokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Age\"])\n",
        ")  \n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Hours-per-week\"])\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_3_adult[\"Capital-Gain\"])\n",
        ")\n",
        "\n",
        "fig.update_layout(height=600, width=800, title_text=\"Boxplots for Numeric Features\")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Age Boxplot\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = px.box(df_3_adult, y=\"Age\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4OQL4mAOv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bar Plot and Pie Plot for Discriminatory Feature \n",
        "\n",
        "def target_distribution(y_var, data):\n",
        "    val = data[y_var]\n",
        "\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "    plt.rcParams.update({'font.size': 13})\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    cnt = val.value_counts().sort_values(ascending=True)\n",
        "    labels = cnt.index.values\n",
        "\n",
        "    sizes = cnt.values\n",
        "    colors = sns.color_palette(\"PuBu\", len(labels))\n",
        "\n",
        "    #------------COUNT-----------------------\n",
        "    ax1.barh(cnt.index.values, cnt.values, color=colors)\n",
        "    ax1.set_title('Count plot of '+y_var)\n",
        "\n",
        "    #------------PERCENTAGE-------------------\n",
        "    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Distribution of '+y_var)\n",
        "    plt.show()\n",
        "\n",
        "# Distribution for Race\n",
        "\n",
        "target_distribution(y_var=\"Race\", data=df_3_adult)\n",
        "\n",
        "# Absolute number of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(dropna=False, sort=True)) # Learning: wrde genauso mit df_3_adult[\"Race\"].value_counts(dropna=False) funktionieren\n",
        "\n",
        "# Percentage of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True))\n",
        "\n",
        "# Define Histogram Function\n",
        "def plot_histo(data, col, Y_columns):\n",
        "    df = data.copy()\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    \n",
        "    for i in range(0,2):\n",
        "        cnt = []; y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt += [df[df[y_col] == val][col].values]\n",
        "        bins = df[col].nunique()\n",
        "\n",
        "        axs[i].hist(cnt, bins=bins, stacked=True)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "Y_columns = [\"Race\", \"Sex\"]\n",
        "\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Gain',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Loss',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Hours-per-week',Y_columns=Y_columns))\n",
        "\n",
        "# Define Function for Bar Plot\n",
        "\n",
        "def plot_bar(data, col, Y_columns, max_cat=10):\n",
        "    df = data.copy()\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    cat_val = df[col].value_counts()[0:max_cat].index.values\n",
        "    df = df[df[col].isin(cat_val)]\n",
        "\n",
        "    for i in range(0,2):\n",
        "        y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt = df[df[y_col] == val][col].value_counts().sort_index()\n",
        "            axs[i].barh(cnt.index.values, cnt.values)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Bar plot of the \"+col+\" column by \"+y_col)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "Y_columns = [\"Race\"]\n",
        "\n",
        "print(plot_bar(data = df_3_adult, col='Sex',Y_columns=Y_columns)) # Error because values for two subplots are expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthjEWnrFWCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Column names\n",
        "# 'f1_complete', \"f1_complete_train\", 'f1_minority', 'f1_majority', \"tpr_complete\", \n",
        "# 'tpr_minority', \"tpr_majority\", \"fpr_minority\", \"fpr_majority\", \"prob_yhat_1_minority\", \"prob_yhat_1_majority\"\n",
        "\n",
        "# results_df[\"rel_share_min_of_maj\"]\n",
        "# results_df[\"aver_abs_odds_diff\"]  \n",
        "# results_df[\"stat_parity_diff\"] \n",
        "# results_df[\"equal_opport_dist\"]  \n",
        "# results_df[\"disparate_impact\"] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEP61EfQhFyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eda_descr_stats(data, disc_feature, disc_min_value, label, second_disc_feature=\"\"):\n",
        "\n",
        "  # 1. Sensitive Feature\n",
        "  print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {data.columns}.\")\n",
        "  print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {data[disc_feature].unique()}.\")\n",
        "\n",
        "  # 2. Binary Target Feature\n",
        "  print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "  print(data.groupby([label]).agg({label: 'count'}))\n",
        "\n",
        "  # 3. Total Number of Predictor Features\n",
        "  print(f\"3. The Total Number of Predictor Features is: {data.shape[1]}.\")\n",
        "\n",
        "  # 4. Total Number of Training Examples\n",
        "  print(f\"4. The Total Number of Training Examples is: {data.shape[0]}.\")\n",
        "\n",
        "  # 5. Total Number of Training Examples in the Minority Group \n",
        "  is_min = data[disc_feature].isin([disc_min_value])\n",
        "  print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(data[is_min].index)}.\")\n",
        "\n",
        "  # 6. Sample Size Disparity\n",
        "  # Absolute number of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {data[disc_feature].value_counts(dropna=False, sort=True)}.\")\n",
        "  # Percentage of members of different \"races\"\n",
        "  print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {data[disc_feature].value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "  # 7. Class Balance\n",
        "  print(data[label].value_counts(dropna=False))\n",
        "  print(data[label].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "  # 8. Coarseness of Features\n",
        "  print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "  print(data.isna().any())\n",
        "  print(data.isna().sum())\n",
        "\n",
        "  # 9. Severity of Outliers for Numeric Features\n",
        "  print(\"9. Severity of Outliers for Numeric Features\")\n",
        "  ax = sns.boxplot(data=data, orient=\"h\", palette=\"Set2\")\n",
        "  print(ax)\n",
        "\n",
        "  # (10. Cross-sectional sample size disparity)\n",
        "  if second_disc_feature:\n",
        "    print(\"10. Cross-sectional sample size disparity\")\n",
        "    data.groupby([second_disc_feature, disc_feature]).agg({label: 'count'})\n",
        "\n",
        "  # (11. Feature Importance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH6m4e6TWiwZ",
        "colab_type": "text"
      },
      "source": [
        "*Outcomes*:\n",
        "1. Understanding of the general structure of the dataset\n",
        "2. Understanding of basic statistics of the features of the dataset\n",
        "3. Identification of **features** on the basis of which subpopulations can be identified and discrimination can happen\n",
        "4. Identification of **values** of these features that are connected to certain subpopulations\n",
        "5. Identification of the percentage of certain subpopulation based on the values of these features -> *Sample Size Disparity I*\n",
        "6. Identification of the percentage of certain cross-sectional subpopulations based on the overall dataset -> *Sample Size Disparity II*\n",
        "7. Identification of the target feature and the type of class balance\n",
        "8. Identification of percentage of missing values per feature\n",
        "9. Identification of severity of outliers per feature\n",
        "10. Identification of the distribution of class values (for majority and minority)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si8Ss0hAWdO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Sensitive Feature\n",
        "print(f\"1. Sensitive Attribute: One or more of the following features are sensitive ones: {df_3_adult.columns}.\")\n",
        "print(f\"1. Sensitive Attribute: These are the individual values for the sensitive attribute: {df_3_adult.Race.unique()}.\")\n",
        "\n",
        "# 2. Binary Target Feature\n",
        "print(\"2. Binary Target Variable: The Binary Target Feature has the following values and counts:\")\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "\n",
        "# 3. Total Number of Predictor Features\n",
        "print(f\"3. The Total Number of Predictor Features is: {df_3_adult.shape[1]}.\")\n",
        "\n",
        "# 4. Total Number of Training Examples\n",
        "df_3_adult.shape[0]\n",
        "print(f\"4. The Total Number of Training Examples is: {df_3_adult.shape[0]}.\")\n",
        "\n",
        "# 5. Total Number of Training Examples in the Minority Group \n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "print(f\"5. The Total Number of Training Examples in the Minority Group is: {len(df_3_adult[is_black].index)}.\")\n",
        "\n",
        "# 6. Sample Size Disparity\n",
        "# Absolute number of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Absolute numbers of members of different races are as follows:  {df_3_adult.Race.value_counts(dropna=False, sort=True)}.\")\n",
        "# Percentage of members of different \"races\"\n",
        "print(f\"6. Sample Size Disparity: The Percentages of the number of members of different races are as follows: {df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True)}.\")\n",
        "\n",
        "# 7. Class Balance\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(dropna=False))\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(normalize=True, dropna=False))\n",
        "\n",
        "# 8. Coarseness of Features\n",
        "print(\"8. Coarseness of Features: Details on missing values of features in the dataset:\")\n",
        "print(df_3_adult.isna().any())\n",
        "print(df_3_adult.isna().sum())\n",
        "\n",
        "# 9. Severity of Outliers for Numeric Features\n",
        "print(\"9. Severity of Outliers for Numeric Features\")\n",
        "ax = sns.boxplot(data=df_3_adult, orient=\"h\", palette=\"Set2\")\n",
        "print(ax)\n",
        "\n",
        "# (10. Cross-sectional sample size disparity)\n",
        "print(\"10. Cross-sectional sample size disparity\")\n",
        "df_3_adult.groupby(['Sex', 'Race']).agg({\"Over-50K\": 'count'})\n",
        "\n",
        "# (11. Feature Importance)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJdQ2el7iHnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Basic Performance Statistics\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix \n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_train_pred) # Validate if the correct data was used\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='k-NN')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('k-NN ROC Curve')\n",
        "plt.show();\n",
        "\n",
        "# AUC \n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_train, y_train_pred)) # Validate if the correct data was used\n",
        "\n",
        "# Full Classification Metrics Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Visual Model Evaluation\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpK9B42QzITz",
        "colab_type": "text"
      },
      "source": [
        "## Test Metric function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t4W0BvffrcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries to study\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))\n",
        "\n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "\n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "display(Markdown('### Bias metrics for the Sex model'))\n",
        "fair = get_fair_metrics_and_plot(data_orig_sex_test, rf_orig_sex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGgBB2XQFSKa",
        "colab_type": "text"
      },
      "source": [
        "Overfitting / underfitting check by means of plotly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3vxb42sFUIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A: Custom plotly visualization - Function definition\n",
        "\n",
        "def compl_fitting_line_chart(metric_df):\n",
        "\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  # Create traces\n",
        "\n",
        "  # Performance Metrics\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='F1 Minority'))\n",
        "  fig.add_trace(go.Scatter(x=metric_df[\"rows_complete\"], y=metric_df[\"f1_complete_train\"],\n",
        "                      mode='lines+markers',\n",
        "                      name='TPR/Recall Minority'))\n",
        "\n",
        "  # Edit the layout\n",
        "  fig.update_layout(title={'text': \"Learning Curve for the Complete Dataset\",\n",
        "                           'y':0.9,\n",
        "                           'x':0.5,\n",
        "                           'xanchor': 'center',\n",
        "                           'yanchor': 'top'},\n",
        "                    xaxis_title='Rows Complete',\n",
        "                    yaxis_title='Metric Score', \n",
        "                    font=dict(size=14))\n",
        "  \n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# B: Custom plotly visualization - Function Execution\n",
        "compl_fitting_line_chart(results_df)  #Change into results_df_adult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf2CHWVDzHxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST FUNCTION \n",
        "\n",
        "# Import relevant modules\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Get specific Dataframe from list of dataframes\n",
        "df_check = list_dfs[5]\n",
        "\n",
        "# Define Input and target columns\n",
        "label = \"Over-50K\"\n",
        "df_train_input = df_check.drop(columns=[label])  # Input\n",
        "df_train_label = df_check[label]                 # Target\n",
        "\n",
        "# Apply dummy coding\n",
        "df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "## TRAIN & TEST\n",
        "# Define model\n",
        "rf_test = RandomForestClassifier(criterion= \"gini\", \n",
        "                                 max_features = \"auto\",\n",
        "                                 max_depth = 4,\n",
        "                                 min_samples_leaf = 4,\n",
        "                                 n_estimators = 100)\n",
        "# Predict \n",
        "y_train_pred = cross_val_predict(rf_test,\n",
        "                                 df_train_input, \n",
        "                                 df_train_label, \n",
        "                                 cv = 10)\n",
        "\n",
        "# Append prediction labels to original dataset\n",
        "df_check['y_pred'] = y_train_pred\n",
        "\n",
        "# Create dataset version for minority class \n",
        "is_black = df_check[\"Race\"].isin([\"Black\"]) \n",
        "df_check_black = df_check[is_black]  # Minority group\n",
        "# Create dataset version for majority class\n",
        "is_white = df_check[\"Race\"].isin([\"White\"])\n",
        "df_check_white = df_check[is_white] # Majority group\n",
        "\n",
        "\n",
        "## METRICS\n",
        "# Get metrics for the COMPLETE dataset\n",
        "rows_compl_list = [] \n",
        "rows_compl = len(df_check.index)\n",
        "rows_compl_list.append(rows_compl)\n",
        "\n",
        "f1_compl_list = []\n",
        "f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "f1_compl_list.append(f1_compl) \n",
        "\n",
        "# metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)) \n",
        "# -> https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
        "\n",
        "# Get metrics for the MINORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_min_list = [] \n",
        "rows_min = len(df_check_black.index)\n",
        "rows_min_list.append(rows_min)\n",
        "# F1\n",
        "f1_min_list = []\n",
        "f1_min = f1_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "f1_min_list.append(f1_min)\n",
        "# TPR, RECALL\n",
        "tpr_min_list = []\n",
        "tpr_min = recall_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "tpr_min_list.append(tpr_min)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_min_list = []\n",
        "tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_min = tn_min / (tn_min+fp_min)\n",
        "fpr_min_list.append(fpr_min)\n",
        "\n",
        "# Get metrics for the MAJORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_maj_list = [] \n",
        "rows_maj = len(df_check_white.index)\n",
        "rows_maj_list.append(rows_maj)\n",
        "# F1\n",
        "f1_maj_list = []\n",
        "f1_maj = f1_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "f1_maj_list.append(f1_maj)\n",
        "# TPR, RECALL\n",
        "tpr_maj_list = []\n",
        "tpr_maj = recall_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "tpr_maj_list.append(tpr_maj)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_maj_list = []\n",
        "tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "\n",
        "# Store metrics for different iterations in Data Frame\n",
        "results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                           \"rows_minority\": rows_min_list,\n",
        "                           \"rows_majority\": rows_maj_list, \n",
        "                           'f1_complete': f1_compl_list,\n",
        "                           'f1_minority': f1_min_list,\n",
        "                           'f1_majority': f1_maj_list,\n",
        "                           'tpr_minority': tpr_min_list,\n",
        "                           \"tpr_majority\": tpr_maj_list,\n",
        "                           \"fpr_min\": fpr_min_list,\n",
        "                           \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "# Calculate new columns and append to df \n",
        "results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "# TBD if this calculation is correct that way or if (min/(maj+min))*100\n",
        "\n",
        "# results_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DceolfFEzQiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train scores\n",
        "\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "cross_val_results = cross_validate(estimator = rf_test, \n",
        "                                  X = df_train_input, y = df_train_label, cv = 10, \n",
        "                                  scoring = f1, return_train_score=True)\n",
        "print(cross_val_results)\n",
        "f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJmXzP_tdJXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join iso alpha codes \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW_4b7rBdJ7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store data in dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "df_3_adult = pd.read_csv(io.BytesIO(uploaded['adult (1).data']), names = csv_columns, skipinitialspace=True)# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDpY_z3mW4kb",
        "colab_type": "text"
      },
      "source": [
        "Variant A: ggplot2\n",
        "\n",
        "- http://www.sthda.com/english/wiki/ggplot2-line-plot-quick-start-guide-r-software-and-data-visualization\n",
        "- https://www.datanovia.com/en/blog/how-to-create-a-ggplot-with-multiple-lines/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjg9oXQ_WyjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "(ggplot(data = results_df)\n",
        "  + aes(x='rows_minority') \n",
        "  + geom_line(aes(y = \"f1_minority\"), color =\"darkred\") \n",
        "  + geom_line(aes(y = \"aver_abs_odds_diff\"), color = \"steelblue\") \n",
        "  + ggtitle(\"Minority Metrics\")\n",
        "  # + scale_color_identity(guide = legend()) # Add legend based on colours\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5YkTvP4Aa4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Groupy by for conditional probabilities \n",
        "\n",
        "# Prepare groupby data \n",
        "filter_race_black_y_1 = df_check[\"Race\"].isin([\"Black\"]) & df_check[\"y_pred\"].isin([1])\n",
        "filter_race_white_y_1 = df_check[\"Race\"].isin([\"White\"]) & df_check[\"y_pred\"].isin([1])\n",
        "\n",
        "filter_race_black = df_check[\"Race\"].isin([\"Black\"])\n",
        "filter_race_white = df_check[\"Race\"].isin([\"White\"])\n",
        "\n",
        "prob_race_black_y_1 = len(df_check[filter_race_black_y_1].index) / len(df_check[filter_race_black].index)\n",
        "prob_race_white_y_1 = len(df_check[filter_race_white_y_1].index) / len(df_check[filter_race_white].index)\n",
        "\n",
        "print(prob_race_black_y_1)\n",
        "print(prob_race_white_y_1)\n",
        "\n",
        "# rating_probs = df_check.groupby(\"Race\").size().div(len(df_check))\n",
        "# groupby_probs = df_check.groupby(['y_pred', \"Race\"]).size().div(len(df_check)).div(rating_probs, axis=0, level=\"Race\")\n",
        "# print(groupby_probs)\n",
        "\n",
        "# Experiments identify how to subset groupby in order to get right conditional probability \n",
        "rating_probs = df_check.groupby('Race').size().div(len(df_check))\n",
        "print(df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race'))\n",
        "group_df_1 = df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race')\n",
        "# list_groupby = list(group_df_1)\n",
        "# print(list_groupby)\n",
        "# print(list_groupby[2])\n",
        "# print(list_groupby[3])\n",
        "\n",
        "# print(group_df_1[[2]])\n",
        "# print(group_df_1[[3]])\n",
        "# test = group_df_1[[3]]\n",
        "# print(test)\n",
        "\n",
        "# # Create list\n",
        "# list_groupby_probs = list(groupby_probs)\n",
        "# print(list_groupby_probs)\n",
        "# prob_y_1_min = list_groupby_probs[2]\n",
        "# print(prob_y_1_min)\n",
        "\n",
        "# prob_y_1_maj = list_groupby_probs[3]\n",
        "# print(prob_y_1_maj)\n",
        "\n",
        "# # Cond. Prob. P(y=1|minority)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zs8SxSqZGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "  \n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    See :ref:`examples/model_selection/plot_learning_curve.py\n",
        "    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
        "    \"\"\"\n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtMtLvu3b7Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      tpr_compl = \n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min_list = [] \n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min_list = []\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min_list = []\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_min_list = []\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = tn_min / (tn_min+fp_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj_list = [] \n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj_list = []\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj_list = []\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_maj_list = []\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                              \"rows_minority\": rows_min_list,\n",
        "                              \"rows_majority\": rows_maj_list, \n",
        "                              'f1_complete': f1_compl_list,\n",
        "                              'f1_minority': f1_min_list,\n",
        "                              'f1_majority': f1_maj_list,\n",
        "                              'tpr_minority': tpr_min_list,\n",
        "                              \"tpr_majority\": tpr_maj_list,\n",
        "                              \"fpr_min\": fpr_min_list,\n",
        "                              \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcvZqMvgw-XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPROCESSING manually\n",
        "# Encoding Binary \n",
        "\n",
        "df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc12Lw3VC-_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" else val == 0)\n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace(to_replace=['<=50K', '>50K'], value=[0, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJsTc_t6-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OLD FUNCTION\n",
        "\n",
        "label = \"Over-50K\"\n",
        "model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "list_dfs = list_dfs\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv = 10):\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "  \n",
        "    ## DATA PREPROCESSING\n",
        "      # Seperate dataset by input features and labels\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]]                # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "    ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input, \n",
        "                                       df_train_label, \n",
        "                                       cv = cv)\n",
        "    \n",
        "    ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "\n",
        "      # confusion_matrix(df_train_label, y_train_pred) \n",
        "\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred)\n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Band for difference in Training and Validation Set\n",
        "        # train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring, \n",
        "                                                                # n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "        train_scores_mean = np.mean(train_scores, axis=1)\n",
        "        train_scores_std  = np.std(train_scores, axis=1)\n",
        "        test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "        test_scores_std   = np.std(test_scores, axis=1)\n",
        "        plt.grid()\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "        \n",
        "        row_min_list = []\n",
        "        rows_min = len(dataset_min.index)\n",
        "        row_min_list.append(rows_min)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "\n",
        "        f1_min_list = []\n",
        "        f1_min = f1_score(df_train_label, y_train_pred)\n",
        "        f1_min_list.append()\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "\n",
        "        # Number of rows\n",
        "        row_maj_list = []\n",
        "        rows_maj = len(dataset_maj.index)\n",
        "        row_maj_list.append(rows_maj)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "      \n",
        "        f1_maj_list = []\n",
        "        f1_maj = f1_score(df_train_label, y_train_pred)\n",
        "        f1_maj_list.append()\n",
        "\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'Total_rows':rows_compl_list, \n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list})\n",
        "  \n",
        "  # Setting a column as an index\n",
        "  # dogs_ind = dogs.set_index(\"name\")\n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFpvWx8bRWi",
        "colab_type": "text"
      },
      "source": [
        "Calculate Confusion Matrix Cell Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-VTE1ryDbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DEPRECATED\n",
        "\n",
        "  # neighbors_list = list(range(5,500, 5))\n",
        "  # accuracy_list = []\n",
        "  # for test_number in neighbors_list:\n",
        "  # model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "  # predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "  # accuracy = accuracy_score(y_test, predictions)\n",
        "  # accuracy_list.append(accuracy)\n",
        "  # results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "\n",
        "  # # Initialize df \n",
        "  # algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs']) # To Do: Change metrics here\n",
        "\n",
        "  # def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "  #     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "  #                                             columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "  #                                             index=[name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV1izcpevA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['number_training_examples', 'rel_share_min', 'min_f1', \"total_f1\", 'fairness metric'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "                                            index=[name]))\n",
        "    \n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "    \n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "# Fairness Metric\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai\n",
        "\n",
        "# 1: Average Absolute Odd Difference\n",
        "average_abs_odds_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference\n",
        "    def average_odds_difference(self):\n",
        "        r\"\"\"Average of difference in FPR and TPR for unprivileged and privileged\n",
        "        groups:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "           \\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n",
        "           + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\n",
        "\n",
        "        A value of 0 indicates equality of odds.\n",
        "        \"\"\"\n",
        "        return 0.5 * (self.difference(self.false_positive_rate)\n",
        "                    + self.difference(self.true_positive_rate))\n",
        "\n",
        "# This metric's scale would need to be \"reversed\", presumably.\n",
        "\n",
        "# 2: Equal Opportunity Distance\n",
        "equal_opportunity_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference\n",
        "\n",
        "    def equal_opportunity_difference(self):\n",
        "        \"\"\"Alias of :meth:`true_positive_rate_difference`.\"\"\"\n",
        "        return self.true_positive_rate_difference()\n",
        "\n",
        "# ClassificationMetric and BinaryLabelDatasetMetric.\n",
        "\n",
        "# Template:\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "# algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "# def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "#     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkTibSibQO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_tpr(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FN = 0\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "#     TPR = TP/(TP+FN)\n",
        "\n",
        "# Calculation of confusion matrix rates\n",
        "\n",
        "# # Sensitivity, hit rate, recall, or true positive rate\n",
        "# TPR = TP/(TP+FN)\n",
        "# # Specificity or true negative rate\n",
        "# TNR = TN/(TN+FP) \n",
        "# # Precision or positive predictive value\n",
        "# PPV = TP/(TP+FP)\n",
        "# # Negative predictive value\n",
        "# NPV = TN/(TN+FN)\n",
        "# # Fall out or false positive rate\n",
        "# FPR = FP/(FP+TN)\n",
        "# # False negative rate\n",
        "# FNR = FN/(TP+FN)\n",
        "# # False discovery rate\n",
        "# FDR = FP/(TP+FP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkh5YSJaZej",
        "colab_type": "text"
      },
      "source": [
        "Approach 2: Take slices of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKws3DBaYHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slice code for minority example\n",
        "\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "\n",
        "# Dummy coding for slice\n",
        "\n",
        "df_3_adult_black_dummies = pd.get_dummies(df_3_adult_black)\n",
        "\n",
        "# Features of complete dataset\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_black_input = df_3_adult_black_dummies.drop(columns=[\"Over-50K\"])\n",
        "print(df_3_adult_black_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_black_label = df_3_adult_black_dummies[\"Over-50K\"]\n",
        "print(df_3_adult_black_label)\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "sizes_minority = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "                  350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "                  2000, 2250, 2500, 2750] \n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = random_forest, \n",
        "                    title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_black_input, y = df_3_adult_black_label, \n",
        "                    cv = 10, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = sizes_minority)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "# plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41F18rCYhL1",
        "colab_type": "text"
      },
      "source": [
        "Fairness Metric per Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dZ5qd1YrYS",
        "colab_type": "text"
      },
      "source": [
        "https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0E7etjXYgGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Define protected class\n",
        "dataset_orig = df_3_adult(\n",
        "    protected_attribute_names=['Race'],           \n",
        "    privileged_classes=\"White\",     \n",
        "    features_to_drop=['personal_status', 'sex'] \n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'Race': \"White\"}]\n",
        "unprivileged_groups = [{'Race': \"Black\"}]\n",
        "\n",
        "\n",
        "# Convert dataset into aif360 adequate form\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XdytLDYXUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(regr.predict(diabetes_X_test))\n",
        "\n",
        "rfc_model_3 = RandomForestClassifier(n_estimators=200)\n",
        "rfc_model_3.predict(X_test)\n",
        "\n",
        "X_test['survived'] = rfc_model_3.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKx-nFqaTvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect original Learning Curve function\n",
        "import inspect\n",
        "from sklearn.model_selection import learning_curve\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOolonVAnCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Function to get TPR, FPR, usw.\n",
        "\n",
        "# # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "\n",
        "# def perf_measures(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FP = 0\n",
        "#     TN = 0\n",
        "#     FN = 0\n",
        "\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "#            FP += 1\n",
        "#         if y_actual[i]==y_hat[i]==0:\n",
        "#            TN += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "\n",
        "#     # Calculation of confusion matrix rates\n",
        "#     # Sensitivity, hit rate, recall, or true positive rate\n",
        "#     TPR = TP/(TP+FN)\n",
        "#     # Specificity or true negative rate\n",
        "#     TNR = TN/(TN+FP) \n",
        "#     # Precision or positive predictive value\n",
        "#     PPV = TP/(TP+FP)\n",
        "#     # Negative predictive value\n",
        "#     NPV = TN/(TN+FN)\n",
        "#     # Fall out or false positive rate\n",
        "#     FPR = FP/(FP+TN)\n",
        "#     # False negative rate\n",
        "#     FNR = FN/(TP+FN)\n",
        "#     # False discovery rate\n",
        "#     FDR = FP/(TP+FP)\n",
        "\n",
        "#     return TPR, FPR, TNR, FNR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYvv_KIktiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated CONFUSION MATRIX RATES\n",
        "\n",
        "# tpr_min_list = []\n",
        "# tpr_min = get_tpr(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "\n",
        "# tpr_min_list = [] \n",
        "# fpr_min_list = []\n",
        "# tnr_min_list = []\n",
        "# fnr_min_list = []\n",
        "# tpr_min, fpr_min, tnr_min, fnr_min = perf_measures(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "# fpr_min_list.append(fpr_min)\n",
        "# tnr_min_list.append(tnr_min)\n",
        "# fnr_min_list.append(fnr_min)\n",
        "\n",
        "# tpr_maj_list = []\n",
        "# fpr_maj_list = []\n",
        "# tnr_maj_list = []\n",
        "# fnr_maj_list = []\n",
        "# tpr_maj, fpr_maj, tnr_maj, fnr_maj = perf_measures(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"])\n",
        "# tpr_maj_list.append(tpr_maj)\n",
        "# fpr_maj_list.append(fpr_maj)\n",
        "# tnr_maj_list.append(tnr_maj)\n",
        "# fnr_maj_list.append(fnr_maj)\n",
        "\n",
        "#  \"tpr_majority\": tpr_maj_list\n",
        "#  \"tpr_minority\": tpr_min_list\n",
        "#  \"fpr_majority\": fpr_maj_list\n",
        "#  \"fpr_minority\": fpr_min_list\n",
        "#  \"tnr_majority\": tnr_maj_list\n",
        "#  \"tnr_minority\": tnr_min_list\n",
        "#  \"fnr_majority\": fnr_maj_list\n",
        "#  \"fnr_minority\": fnr_min_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuG9uvV5U19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_datasets_new(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "#     datasets = []\n",
        "#     for training_size in training_sizes:\n",
        "#         \n",
        "#             dataset_min_sample = min_data.sample(n=training_size, random_state=1)\n",
        "#             dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "#             datasets.append(dataset)\n",
        "#     return datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FwySPCFNK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUaC5dzFQcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "    \"\"\"\n",
        "    \n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqnx5tPkCpWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# min_dataset_list = list()\n",
        "# compl_dataset_list = list()\n",
        "\n",
        "# is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "# is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "# def create_datasets(min_data, maj_data, training_sizes):\n",
        "#     for training_size in training_sizes:\n",
        "#         while len(min_data.index) >= training_size: # Code should stop when number of rows of df with min. group is smaller than training size iteration\n",
        "#             dataset_min_slice = min_data.sample(n = training_size, random_state = 1) # Get a random subset of the minority group\n",
        "#             min_dataset_list.append(dataset_min_slice) # Create list of data frames that include observations of minority group of different sizes\n",
        "#             for min_dataset_component in min_dataset_list:\n",
        "#                 dataset_list = maj_data.append(min_dataset_component) # Merge observations of min. group of different sizes with observations from maj. group\n",
        "#                 compl_dataset_list.append(dataset_list) # Create list of data frames that include majority group (fixed size) and minority group (different sizes)\n",
        "#                 return compl_dataset_list\n",
        "\n",
        "# training_sizes_2 = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, \n",
        "                      300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750]\n",
        "\n",
        "# list_dfs_2 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_2)\n",
        "# print(len(list_dfs_2))\n",
        "# print([df.shape[0] for df in list_dfs_2])\n",
        "\n",
        "# training_sizes_3 =  [2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500]\n",
        "\n",
        "# list_dfs_3 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_3)\n",
        "# print(len(list_dfs_3))\n",
        "# print([df.shape[0] for df in list_dfs_3])\n",
        "\n",
        "# training_sizes_4 = [4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# list_dfs_4 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_4)\n",
        "# print(len(list_dfs_4))\n",
        "# print([df.shape[0] for df in list_dfs_4])\n",
        "\n",
        "# # Execute function\n",
        "# training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "#                   350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "#                   2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "#                   4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# df_3_adult_black = df_3_adult[is_black]  # Define minority group based on original data frame\n",
        "# df_3_adult_white = df_3_adult[is_white]  # Define majority group based on original data frame\n",
        "\n",
        "# list_dfs = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhAaGcytjPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_3_adult_train_input\n",
        "# df_3_adult_train_label\n",
        "\n",
        "# # Row bind\n",
        "# df1.append(df2)\n",
        "\n",
        "# def get_min_datasets(data, ):\n",
        "#   for specific_size in min_sizes:\n",
        "#     if n_row(dataset) >= specific_size:\n",
        "#     elif n_row(dataset) < specific_size:\n",
        "#       stop\n",
        "#     dataset_min_slice = training_examples.sample(frac=1) # shuffle dataset with minority group randomely before slicing\n",
        "#     dataset_min_slice = dataset_min_slice.slice[min_sizes]\n",
        "#     diff_min_datasets.append(dataset_min) #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_a7-0Dg1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ggf. feature importance fr jede Iteration auch noch berechnen und dann ans DF anhngen\n",
        "\n",
        "# Sample Code\n",
        "\n",
        "# neighbors_list = [3,5,10,20,50,75]      # use either the one or the other neighbors_list \n",
        "# neighbors_list = list(range(5,500, 5))\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# accuracy_list = []\n",
        "# for test_number in neighbors_list:\n",
        "#   model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "#   predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "#   accuracy = accuracy_score(y_test, predictions)\n",
        "#   accuracy_list.append(accuracy)\n",
        "\n",
        "  # Because I will be working with k-fold cv, most likely I will need to take the average \n",
        "  # and define the standard deviation (which then can also be shown in the plot as bands)\n",
        "\n",
        "  # Important: Get score both training and testing \n",
        "\n",
        "# results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# Matplotlib plotting code -> Alternatively, use plotly\n",
        "\n",
        "# plt.plot(results_df['neighbors'],\n",
        "# results_df['accuracy'])\n",
        "# # Add the labels and title\n",
        "# plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',\n",
        "# title='Accuracy for different n_neighbors')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Dataframe \n",
        "\n",
        "# results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
        "# print(results_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZywZ_O294Rcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# II. Model setup\n",
        "# a) Linear\n",
        "\n",
        "# b) Nonlinear\n",
        "# K-NN\n",
        "# knn = KNeighborsClassifier(n_neighbors=6) # To Do: Check appropriatness of hyperparameter\n",
        "# knn.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training\n",
        "\n",
        "# c) Others\n",
        "# Random Forest\n",
        "# random_forest = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs =-1)\n",
        "#  random_forest.fit(df_3_adult_train_input, df_3_adult_train_label)\n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLHQERS8pV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute train/test split.\n",
        "\n",
        "# Check: Probably not necessary \n",
        "\n",
        "# Execute Train/test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_3_adult_train_input, \n",
        "#                                                     df_3_adult_train_label, \n",
        "#                                                     test_size=0.3, \n",
        "#                                                     random_state=21) # seed for random number generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMoVDkicTDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 2\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "    go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='group')\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Yvu-jJcado",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 1\n",
        "# fig = px.bar(df_3_adult_aggr, x=\"Race\", y=\"counting\", color='Sex', barmode='group')\n",
        "# fig.show()\n",
        "\n",
        "# Hint: Does not work properly.\n",
        "\n",
        "# Plot Alternative 2\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# fig = go.Figure(data=[\n",
        "#     go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "#     go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "# ])\n",
        "# # Change the bar mode\n",
        "# fig.update_layout(barmode='group')\n",
        "# fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7546kms4na",
        "colab_type": "text"
      },
      "source": [
        "## Learning Curve Variants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPJKpqPtfVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Learning Curve Function\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhGUR1hti1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(estimator = random_forest, title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    axes=None, ylim=None, cv=10, train_sizes=sizes)\n",
        " \n",
        "# models = []\n",
        "\n",
        "# for model in models:\n",
        "#   plot_learning_curve(estimator=model, title=\"k-nn Learning Curve\", X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "#                     axes=None, ylim=None, cv=10,\n",
        "#                     n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzmnhnP22Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yellowbrick"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Pq7eJW2xjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from yellowbrick.datasets import load_game\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "# Encode the categorical data\n",
        "X = df_3_adult_train_input\n",
        "y = df_3_adult_train_label\n",
        "\n",
        "# Create the learning curve visualizer\n",
        "cv = StratifiedKFold(n_splits=12)\n",
        "# sizes = np.linspace(0.1, 1.0, 10)\n",
        "sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = LearningCurve(\n",
        "    model = random_forest, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
        ")\n",
        "\n",
        "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
        "visualizer.show()           # Finalize and render the figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uAPV7p-tja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
        "\n",
        "# train_sizes = [1, 100, 500, 1000, 1500, 2000, 2500, 3000, 5000, 7500, 10000]\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# features = df_3_adult_train_input.columns\n",
        "# target = df_3_adult_train_label\n",
        "\n",
        "train_sizes, train_scores, validation_scores = learning_curve(estimator = random_forest, \n",
        "                                                              X = df_3_adult_train_input, \n",
        "                                                              y = df_3_adult_train_label, \n",
        "                                                              cv = 5)\n",
        "\n",
        "train_scores_mean = train_scores.mean(axis = 1)\n",
        "validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
        "print('\\n', '-' * 20) # separator\n",
        "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('Score', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.title('Learning curves for a knn model', fontsize = 18, y = 1.03)\n",
        "plt.legend()\n",
        "plt.ylim(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxOS8Pdw-z6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Bundling our previous work into a function ###\n",
        "\n",
        "def learning_curves(estimator, data, features, target, train_sizes, cv, scoring):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        estimator, data[features], data[target], train_sizes = train_sizes,\n",
        "        cv = cv, scoring = scoring)\n",
        "    \n",
        "    train_scores_mean = train_scores.mean(axis = 1)\n",
        "    validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "\n",
        "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "\n",
        "    plt.ylabel('Score', fontsize = 14)\n",
        "    plt.xlabel('Training set size', fontsize = 14)\n",
        "    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n",
        "    plt.title(title, fontsize = 18, y = 1.03)\n",
        "    plt.legend()\n",
        "    plt.ylim(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgoEC9cb-27p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plotting the two learning curves ###\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "plt.figure(figsize = (16,5))\n",
        "\n",
        "for model, i in [(RandomForestClassifier(), 1)]:\n",
        "    plt.subplot(1,2,i)\n",
        "    learning_curves(estimator = random_forest, \n",
        "                    data = df_3_adult_dummies, \n",
        "                    features = df_3_adult_train_input.columns, \n",
        "                    target= \"Over-50K\", \n",
        "                    train_sizes = sizes,\n",
        "                    scoring = f1, \n",
        "                    cv= 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTO7VG99DTcp",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGGOb9ACbyEZ",
        "colab_type": "text"
      },
      "source": [
        "Guide: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/xgboost_caip.ipynb?hl=de\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5JGFSwCCMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install current tensorflow version\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JlJ_VWqARH_X",
        "colab": {}
      },
      "source": [
        "# Install the What-If Tool\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoYDH1TSBqO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Helper Functions for the What-If tool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import witwidget\n",
        "\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuZMXEnMdNe5",
        "colab_type": "text"
      },
      "source": [
        "**Specify feature and label columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbMGWyF0EcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the column in the dataset you wish for the model to predict\n",
        "label_column = 'Over-50K'\n",
        "\n",
        "# Set list of all columns from the dataset we will use for model input.\n",
        "input_features = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
        "                  'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
        "                  'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country']\n",
        "\n",
        "# Create a list containing all input features and the label column\n",
        "features_and_labels = input_features + [label_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQm_vccX1Y4V",
        "colab_type": "text"
      },
      "source": [
        "**Convert dataset to tf.Example protos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjPGPeI1asw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBwOj9R3PZw",
        "colab_type": "text"
      },
      "source": [
        "**Create and train the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJH-6Tf2t5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 200  #@param {type: \"number\"}\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df_what_if, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHVwhOw3ORW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "num_datapoints = 2000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 1000  #@param {type: \"number\"}\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Load up the test dataset\n",
        "test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True,\n",
        "  skiprows=1)\n",
        "make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n",
        "test_examples = df_to_examples(test_df[0:num_datapoints])\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(test_examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec).set_label_vocab(['Under 50K', 'Over 50K'])\n",
        "a = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}