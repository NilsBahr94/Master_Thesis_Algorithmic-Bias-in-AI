{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master_Thesis-Algorithmic_Bias_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aTO7VG99DTcp",
        "mU2JLfELvDd9"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTkABeSH7BNwYBNHlLJ+UB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilsBahr94/Master_Thesis_Algorithmic-Bias-in-AI/blob/Develop/Master_Thesis_Algorithmic_Bias_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LslOkLJNLSY-"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5GE201KpLiKY"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pPK2QtXDVb-",
        "colab_type": "text"
      },
      "source": [
        "### Basic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0J4SEBlzMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install relevant libraries\n",
        "!pip install plotnine  \n",
        "!pip install pandas\n",
        "!pip install plotly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFZNjOwEsaEr",
        "colab": {}
      },
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotnine import *\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Learning Curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg1LaX71h40E",
        "colab_type": "text"
      },
      "source": [
        "**Fairness Tools**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LBYqEv4Omj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installation\n",
        "!pip install aif360\n",
        "# https://github.com/IBM/AIF360/blob/master/examples/README.md\n",
        "!pip install aequitas\n",
        "# https://github.com/dssg/aequitas\n",
        "!pip install audit-AI\n",
        "# https://github.com/pymetrics/audit-ai\n",
        "!pip install responsibly\n",
        "# https://docs.responsibly.ai/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBA6AQHbR0pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTO7VG99DTcp",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5JGFSwCCMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install current tensorflow version\n",
        "\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JlJ_VWqARH_X",
        "colab": {}
      },
      "source": [
        "# Install the What-If Tool\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoYDH1TSBqO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Helper Functions for the What-If tool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import witwidget\n",
        "\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glhWrBRN0Uov",
        "colab_type": "text"
      },
      "source": [
        "**Import data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqDIdT9a0QVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dataset and convert to Pandas Dataframe \n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l8mIq8P0npB",
        "colab_type": "code",
        "outputId": "a79e3308-de32-46b7-e036-4da8bc1bc838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Binary encoding of label\n",
        "# Encoding Binary Variables\n",
        "\n",
        "df[\"Over-50K\"] = df[\"Over-50K\"].apply(lambda val:\n",
        "1 if val == \">50K\" else 0)\n",
        "\n",
        "# Check if encoding was successful \n",
        "print(df[\"Over-50K\"].dtypes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7gv3JU0hMz",
        "colab_type": "code",
        "outputId": "e044f6f0-9e7d-46f6-be68-25750527b457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
              "       'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
              "       'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country',\n",
              "       'Over-50K'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuZMXEnMdNe5",
        "colab_type": "text"
      },
      "source": [
        "**Specify feature and label columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbMGWyF0EcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the column in the dataset you wish for the model to predict\n",
        "label_column = 'Over-50K'\n",
        "\n",
        "# Set list of all columns from the dataset we will use for model input.\n",
        "input_features = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
        "                  'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
        "                  'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country']\n",
        "\n",
        "# Create a list containing all input features and the label column\n",
        "features_and_labels = input_features + [label_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQm_vccX1Y4V",
        "colab_type": "text"
      },
      "source": [
        "**Convert dataset to tf.Example protos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjPGPeI1asw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBwOj9R3PZw",
        "colab_type": "text"
      },
      "source": [
        "**Create and train the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJH-6Tf2t5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 200  #@param {type: \"number\"}\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df_what_if, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnHVwhOw3ORW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "num_datapoints = 2000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 1000  #@param {type: \"number\"}\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Load up the test dataset\n",
        "test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True,\n",
        "  skiprows=1)\n",
        "make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n",
        "test_examples = df_to_examples(test_df[0:num_datapoints])\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(test_examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec).set_label_vocab(['Under 50K', 'Over 50K'])\n",
        "a = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z0MC-9BGLofv"
      },
      "source": [
        "# Case Studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66N6XRfQf1Pb",
        "colab_type": "text"
      },
      "source": [
        "## 0) Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z52sxTTK842h",
        "colab_type": "text"
      },
      "source": [
        "**Criteria for Dataset Selection**\n",
        "\n",
        "https://www.notion.so/techlabs/Master-Thesis-Proposal-c1550763e23c485887147f59cebc695d#350e14b5c061432189b80eb1df2a81c5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1apgLLLpK8Yi",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOIKqotWhBL8",
        "colab_type": "text"
      },
      "source": [
        "8) Identification of the distribution of class values (for majority and minority)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oq31jwyd0CH",
        "colab_type": "text"
      },
      "source": [
        "Selection of classification method:\n",
        "\n",
        "- **Linear** machine learning algorithms often have a high bias but a low variance.\n",
        "    1. Logistic Regression\n",
        "    2. Linear Discriminant Analysis\n",
        "    3. Partial Least Squares Discriminant Analysis\n",
        "- **Nonlinear** machine learning algorithms often have a low bias but a high variance.\n",
        "    1. Nonlinear Discriminant Analysis\n",
        "    2. Neural Networks\n",
        "    3. Flexible Discriminant Analysis\n",
        "    4. Support Vector Machines \n",
        "    5. K-Nearest Neighbors\n",
        "    6. Naive Bayes\n",
        "- Others\n",
        "    1. Basic Classification Trees\n",
        "    2. Random Forest\n",
        "    3. Boosted Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPgu2ZCKfRsp",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQMq-gTZ27Z",
        "colab_type": "text"
      },
      "source": [
        "- Prepare data for training (split by input features and labels).\n",
        "- Execute train/test split (if applicable).\n",
        "- Train with selected classification method.\n",
        "- Predict with selected classification method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVVbUP5PfR-0",
        "colab_type": "code",
        "outputId": "a535741f-fa89-4c22-e692-910223dc813d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# Define algorithms\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "log_reg = LogisticRegression()\n",
        "svm = SVC(C = 1.0, kernel = \"rbf\")\n",
        "\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "# Estimate \n",
        "print(cross_val_score(estimator = random_forest, \n",
        "                X= df_3_adult_train_input, \n",
        "                y= df_3_adult_train_label, \n",
        "                cv= 10, \n",
        "                scoring= f1))\n",
        "\n",
        "df_3_adult_train_label\n",
        "# Predict and get confusion matrix \n",
        "y_train_pred = cross_val_predict(random_forest, df_3_adult_train_input, df_3_adult_train_label, cv = 10) \n",
        "print(confusion_matrix(df_3_adult_train_label, y_train_pred)) \n",
        "print(f1_score(df_3_adult_train_label, y_train_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.56134723 0.55464927 0.57258719 0.54681027 0.58556367 0.57491857\n",
            " 0.54636591 0.59934853 0.57894737 0.54440789]\n",
            "[[23791   929]\n",
            " [ 4399  3442]]\n",
            "0.5637078283655421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg4w9h0aLSui",
        "colab_type": "text"
      },
      "source": [
        "#### Testing & Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-98kH2ZvJ9eh",
        "colab_type": "text"
      },
      "source": [
        "1. Get basic model performance metrics. \n",
        "2. Understand the predictive ability of the features.\n",
        "3. Understand class separability.\n",
        "4. Understand the structure of the subpopulation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Nox1ZSZNTw",
        "colab_type": "text"
      },
      "source": [
        "1) Get basic model performance metrics of the base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTxbk0nHUeUJ",
        "colab_type": "code",
        "outputId": "8c4cb7eb-f58f-45cc-aca7-59a4d3ca03d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "sklearn.metrics.SCORERS.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZoN2jFMNblM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Basic Performance Statistics\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix \n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_train_pred) # Validate if the correct data was used\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='k-NN')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('k-NN ROC Curve')\n",
        "plt.show();\n",
        "\n",
        "# AUC \n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_train, y_train_pred)) # Validate if the correct data was used\n",
        "\n",
        "# Full Classification Metrics Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Visual Model Evaluation\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdHdYwBNb8s",
        "colab_type": "text"
      },
      "source": [
        "2) Understand the predictive ability of the features\n",
        "\n",
        "See *Chapter 18.2 Categorical Outcomes* in *Applied Predictive Modeling*.\n",
        "\n",
        "Approaches:\n",
        "\n",
        "- Plot ROC Curve per predictor variable \n",
        "- See metrics such as p-value and gain ratio \n",
        "- Relief algorithm\n",
        "- For Random Forest: Gini Importance or Mean Decrease in Impurity (MDI)\n",
        "- LIME\n",
        "- SHAP\n",
        "\n",
        "See https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-understand-a-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YHTUpxRyBML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Importance for Random Forest\n",
        "\n",
        "for i, item in enumerate(rfr.feature_importances_):\n",
        "  print(\"{0:s}: {1:.2f}\".format(X.columns[i], item))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV74Bu1fepbc",
        "colab_type": "text"
      },
      "source": [
        "#### Fairness Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBoeZA3-uicG",
        "colab_type": "text"
      },
      "source": [
        "From https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml/#2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjvN0212xkyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries to study\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_QSUCFOetBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmyhn3kJyk5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "\n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFUhMctizAxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Markdown('### Bias metrics for the Sex model'))\n",
        "fair = get_fair_metrics_and_plot(data_orig_sex_test, rf_orig_sex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DTSOwjLd9IU",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxdQSADOxJz",
        "colab_type": "text"
      },
      "source": [
        "##### Function Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOEoZMdzgA_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/grfiv4/learning-curves-1\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, \n",
        "                        ylim=None, cv=None, \n",
        "                        scoring=None, obj_line=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "  \n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "              A string (see model evaluation documentation)\n",
        "              or a scorer callable object / function with signature scorer(estimator, X, y)\n",
        "              For Python 3.5 the documentation is here:\n",
        "              http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "              For example, Log Loss is specified as 'neg_log_loss'\n",
        "              \n",
        "    obj_line : numeric or None (default: None)\n",
        "               draw a horizontal line \n",
        "               \n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "        \n",
        "        \n",
        "    Citation\n",
        "    --------\n",
        "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "        \n",
        "    Usage\n",
        "    -----\n",
        "        plot_learning_curve(estimator = best_estimator, \n",
        "                            title     = best_estimator_title, \n",
        "                            X         = X_train, \n",
        "                            y         = y_train, \n",
        "                            ylim      = (-1.1, 0.1), # neg_log_loss is negative\n",
        "                            cv        = StatifiedCV, # CV generator\n",
        "                            scoring   = scoring,     # eg., 'neg_log_loss'\n",
        "                            obj_line  = obj_line,    # horizontal line\n",
        "                            n_jobs    = n_jobs)      # how many CPUs\n",
        "\n",
        "         plt.show()\n",
        "    \"\"\"\n",
        "    \n",
        "    from sklearn.model_selection import learning_curve\n",
        "    import numpy as np\n",
        "    from matplotlib import pyplot as plt\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std  = np.std(train_scores, axis=1)\n",
        "    test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "    test_scores_std   = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.style.use('seaborn')\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    if obj_line:\n",
        "        plt.axhline(y=obj_line, color='blue')\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KCHSuC8O25O",
        "colab_type": "text"
      },
      "source": [
        "##### Plot Learning Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7-qv9fxhMfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, # To Do \n",
        "                                       max_depth = 12)\n",
        "\n",
        "sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Plot actual learning curve\n",
        "\n",
        "# plt.figure(figsize = (30,10))\n",
        "plot_learning_curve(estimator = random_forest, \n",
        "                    title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    cv = 10, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = sizes)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "# plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeYwW9VENg73",
        "colab_type": "text"
      },
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WubfdsuyY7Ae",
        "colab_type": "text"
      },
      "source": [
        "Steps in a Grid Search:\n",
        "1. An algorithm to tune the hyperparameters. (Sometimes called an ‘estimator’)\n",
        "2. Defining which hyperparameters we will tune\n",
        "3. Defining a range of values for each hyperparameter\n",
        "4. Setting a cross-validation scheme; and\n",
        "5. Define a score function so we can decide which square on our grid was \"the best\"\n",
        "6. Include extra useful information or functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFzXcwfFUYrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check scores\n",
        "\n",
        "from sklearn import metrics\n",
        "sorted(metrics.SCORERS.keys())\n",
        "\n",
        "# make scorer if that does not work with the default names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TO3fTw-ItvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "param_grid = {'max_depth': [2, 4, 6, 8],            # Important: Keys in the dictionary must be valid hyperparameters \n",
        "              'min_samples_leaf': [1, 2, 4, 6]}\n",
        "\n",
        "# Define classifier\n",
        "rf_grid_search = RandomForestClassifier(criterion= \"entropy\",\n",
        "                                        max_features = \"auto\")\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = model,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= 'f1', \n",
        "                                                     cv = 5,\n",
        "                                                     refit = True, \n",
        "                                                     n_jobs = 2,\n",
        "                                                     return_train_score = True)\n",
        "\n",
        "# Further steps with best model\n",
        "# We can use the best model by means of the score to fit it on our data and make predictions\n",
        "grid_rf_class.fit(X_train, y_train) # To Do: specify dataset variable name\n",
        "# Make predictions\n",
        "grid_rf_class.predict(X_test)       # To Do: specify dataset variable name\n",
        "\n",
        "type(grid_rf_class.best_estimator_) # grid_rf_class.best_estimator_ is an estimator that we can directly use for fitting and prediction\n",
        "print(grid_rf_class.best_estimator_) # print in order to check actual final hyperparameters \n",
        "\n",
        "# Outputs\n",
        "# Three different groups for the GridSearchCV properties;\n",
        "# 1. A results log: cv_results_\n",
        "# 2. The best results: best_index_ , best_params_ & best_score_\n",
        "# 3. 'Extra information': scorer_ , n_splits_ & refit_time_\n",
        "\n",
        "# Access properties with dot notation\n",
        "# e.g. grid_search_object.property\n",
        "\n",
        "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
        "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
        "print(best_row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWKp3N1k_wIE",
        "colab_type": "text"
      },
      "source": [
        "##### Compare different algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHM2sG6QNt0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define different classification algorithms\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "log_reg = LogisticRegression()\n",
        "svm = SVC(C = 1.0, kernel = \"rbf\")\n",
        "\n",
        "# Store different models in a list\n",
        "models = [random_forest, log_reg, svm]\n",
        "\n",
        "# Define other input arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Plot learning curve for different classifcation algorithms\n",
        "for model in models:\n",
        "  plt.subplot\n",
        "  plot_learning_curve(estimator = model, \n",
        "                      title = f\"{model} Learning Curve\", \n",
        "                      X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                      cv = 10, \n",
        "                      scoring = f1, \n",
        "                      ylim = (0, 1), \n",
        "                      train_sizes = sizes)\n",
        "\n",
        "# Plot different subplots \n",
        "\n",
        "# for model, i in [(RandomForestClassifier(), 1), (KNeighborsClassifier(),2)]:\n",
        "#     plt.subplot(1,2,i)\n",
        "#     learning_curves(estimator = model, \n",
        "#                     data = df_3_adult_dummies, \n",
        "#                     features = df_3_adult_train_input.columns, \n",
        "#                     target= \"Over-50K\", \n",
        "#                     train_sizes = train_sizes, \n",
        "#                     cv= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhsVjAHMqceC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def data_preprocess_fair(full_dataset, \n",
        "#                          outcome_feature, \n",
        "#                          privileged_outcome_label, \n",
        "#                          discriminatory_feature, \n",
        "#                          majority_label,\n",
        "#                          minority_label)\n",
        "\n",
        "\n",
        "## 1) Convert the labels of the target variable into a suitable  format (binary integer)\n",
        "# Encoding Binary \n",
        "df_3_adult[\"Over-50KVariables\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" \n",
        "                                                               else 0)\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "## 2. Prepare data for training (input features and label).\n",
        "# Features of complete dataset\n",
        "# print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "# print(df_3_adult_train_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]\n",
        "# print(df_3_adult_train_label)\n",
        "\n",
        "## 3) Prepare data as such that machine learning classification algorithm can handle that properly (e.g. standardization, normalization, feature scaling, dummy encoding)\n",
        "# Dummy \n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "## 4) Filter dataset based on certain feature values which identify subpopulations and create different dataset versions based on that\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "# print(df_3_adult.shape)\n",
        "# print(df_3_adult_black.shape)\n",
        "# print(df_3_adult_white.shape)\n",
        "# print(df_3_adult_female.shape)\n",
        "# print(df_3_adult_male.shape)\n",
        "\n",
        "\n",
        "  # \"\"\"\n",
        "  #   Convert the target label in an binary number format, create two different datasets sets \n",
        "\n",
        "  #   Parameters\n",
        "  #   ----------\n",
        "  #   estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "  #       An object of that type which is cloned for each validation.\n",
        "\n",
        "  #   title : string\n",
        "  #       Title for the chart.\n",
        "\n",
        "  #   X : array-like, shape (n_samples, n_features)\n",
        "  #       Training vector, where n_samples is the number of samples and\n",
        "  #       n_features is the number of features.\n",
        "\n",
        "  #   y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "  #       Target relative to X for classification or regression;\n",
        "  #       None for unsupervised learning.\n",
        "\n",
        "  #   ylim : tuple, shape (ymin, ymax), optional\n",
        "  #       Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "  # \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5sDCEhYStrq",
        "colab_type": "text"
      },
      "source": [
        "Tasks:\n",
        "1. Convert the labels of the target variable into a suitable  format (binary integer).\n",
        "2. Prepare data as such that machine learning classification algorithm can handle that properly (e.g. standardization, normalization, feature scaling, dummy coding).\n",
        "3. Prepare data for training (input features and label).\n",
        "4. Filter dataset based on certain feature values which identify subpopulations and create different dataset versions based on that (*TBD if really the right approach*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uf_LHnXJLcj",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis with the What-If Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUaBGnWWQrah",
        "colab_type": "text"
      },
      "source": [
        "Guide: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/xgboost_caip.ipynb?hl=de\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBeEuX_ZOSxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialization of the What-If Tool\n",
        "\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID'\n",
        "MODEL_NAME = 'YOUR_MODEL_NAME'\n",
        "VERSION_NAME = 'YOUR_VERSION_NAME'\n",
        "TARGET_FEATURE = 'mortgage_status'\n",
        "LABEL_VOCAB = ['denied', 'approved']\n",
        "\n",
        "config_builder = (WitConfigBuilder(test_examples.tolist(), features.columns.tolist() + ['mortgage_status'])\n",
        "  .set_ai_platform_model(PROJECT_ID, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n",
        "  .set_target_feature(TARGET_FEATURE)\n",
        "  .set_label_vocab(LABEL_VOCAB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdqX0VzxMCuy"
      },
      "source": [
        "## 1) US Adult Income Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpPHz6cXJhv",
        "colab_type": "text"
      },
      "source": [
        "UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people’s annual incomes, or various other studies [6]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1N6Co27mxY",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNF_2o9_ALP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dataset and convert to Pandas Dataframe \n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_3_adult = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWFL4a63Uu-B",
        "colab_type": "text"
      },
      "source": [
        "**Alternative Option**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa1vR7X7Tr6C",
        "colab_type": "code",
        "outputId": "c345e44f-992b-4b63-fc23-ed990bbf2825",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Join iso alpha codes \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b244cd4-6da3-4587-a16e-a184a0ba2cdd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2b244cd4-6da3-4587-a16e-a184a0ba2cdd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving adult (1).data to adult (1).data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHjtZXR2QcLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store data in dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "df_3_adult = pd.read_csv(io.BytesIO(uploaded['adult (1).data']), names = csv_columns, skipinitialspace=True)# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujN2xYl4UjvZ",
        "colab_type": "code",
        "outputId": "5b55c160-dca0-494b-c43e-2a68d92a37cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "df_3_adult"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-6df257e972e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_3_adult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl76RtG5KyC4",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSBTBxl-GoB9",
        "colab_type": "text"
      },
      "source": [
        "*Outcomes*:\n",
        "\n",
        "1. Understanding of the general structure of the dataset\n",
        "2. Understanding of basic statistics of the features of the dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgx-dW_4YGyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.head(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxvO48bML_Ql",
        "colab_type": "code",
        "outputId": "c0ac88a0-a713-447d-eb15-25d9c9a73c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Number of rows and features\n",
        "df_3_adult.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JYRH9OzMFQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of specific features\n",
        "df_3_adult.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NepETbBeWwbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datatypes of features\n",
        "print(df_3_adult.dtypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po4NL92SLzGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhPTrCmUVTsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRWegc6AOaEg",
        "colab_type": "text"
      },
      "source": [
        "**Summary of results**\n",
        "\n",
        "1. Understanding of the general structure \n",
        "- Result: \n",
        "2. Understanding of basic statistics of the features of the  the dataset \n",
        "- Result: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_M5h1S5JGoX",
        "colab_type": "text"
      },
      "source": [
        "**Outcomes**:\n",
        "\n",
        "1. Identification of **features** on the basis of which subpopulations can be identified and discrimination can happen\n",
        "2. Identification of **values** of these features that are connected to certain subpopulations\n",
        "3. Identification of the percentage of certain subpopulation based on the values of these features -> *Sample Size Disparity I*\n",
        "4. Identification of the percentage of certain cross-sectional subpopulations based on the overall dataset -> *Sample Size Disparity II*\n",
        "5. Identification of the target feature and the type of class balance\n",
        "6. Identification of percentage of missing values per feature\n",
        "7. Identification of severity of outliers per feature\n",
        "8. Identification of the distribution of class values (for majority and minority)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9DK9PmEeUvV",
        "colab_type": "text"
      },
      "source": [
        "1) **Identification of features on the basis of which subpopulations can be identified and discrimination can happen**\n",
        "\n",
        "-> ***Race, Sex, Country, Workclass***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7P0f93tehVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.columns)\n",
        "print(df_3_adult.columns.nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fA04FNWecSx",
        "colab_type": "text"
      },
      "source": [
        "2) **Identification of values of these features that are connected to certain subpopulations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCPyIlZLfSCp",
        "colab_type": "code",
        "outputId": "3b983c5b-8d21-4bf8-bf8e-037137ac9ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Race\n",
        "print(df_3_adult.Race.unique())\n",
        "print(df_3_adult.Race.nunique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other']\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xYwqW6Lg_4m",
        "colab_type": "code",
        "outputId": "fb729b64-2a64-47c1-fcd9-e7a172d437f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Sex\n",
        "print(df_3_adult.Sex.unique())\n",
        "print(df_3_adult.Sex.nunique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Male' 'Female']\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5tcmrvIhDFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Country\n",
        "print(df_3_adult.Country.unique())\n",
        "print(df_3_adult.Country.nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-13SC05hJ5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Workclass\n",
        "print(df_3_adult.Workclass.unique())\n",
        "print(df_3_adult.Workclass.nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KvSk3abhTYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Education\n",
        "print(df_3_adult.Education.unique())\n",
        "print(df_3_adult.Education.nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNhzCVhgb7zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Template for Group By Statistics \n",
        "print(df_3_adult.groupby(\"Race\")[\"Capital-Gain\"].agg([min, max, \"mean\"])) \n",
        "\n",
        "# print(df_3_adult.groupby([\"Sex\", \"Race\"])[\"Capital-Gain\"].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr4ooq5geZfr",
        "colab_type": "text"
      },
      "source": [
        "`Interim Conclusion regarding 2) `\n",
        "\n",
        "Mainly *Race* as well as *Sex* are the features on the basis of which discrimination could happen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuuv155CkdRt",
        "colab_type": "text"
      },
      "source": [
        "3) **Identification of the percentage of certain subpopulation based on the values of these features** -> *Sample Size Disparity I*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFHXHrHtQpsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualization of Count Columns\n",
        "\n",
        "def target_distribution(y_var, data):\n",
        "    val = data[y_var]\n",
        "\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "    plt.rcParams.update({'font.size': 13})\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    cnt = val.value_counts().sort_values(ascending=True)\n",
        "    labels = cnt.index.values\n",
        "\n",
        "    sizes = cnt.values\n",
        "    colors = sns.color_palette(\"PuBu\", len(labels))\n",
        "\n",
        "    #------------COUNT-----------------------\n",
        "    ax1.barh(cnt.index.values, cnt.values, color=colors)\n",
        "    ax1.set_title('Count plot of '+y_var)\n",
        "\n",
        "    #------------PERCENTAGE-------------------\n",
        "    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Distribution of '+y_var)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-lXBADnNQiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Absolute number of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(dropna=False, sort=True)) # Learning: würde genauso mit df_3_adult[\"Race\"].value_counts(dropna=False) funktionieren\n",
        "\n",
        "# Percentage of members of different \"races\"\n",
        "print(df_3_adult.Race.value_counts(normalize=True, dropna=False, sort=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wSEHZbKQzdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution for Race\n",
        "\n",
        "target_distribution(y_var=\"Race\", data=df_3_adult)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0SQjwEoNz66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Absolute number of members of different \"sexes\"\n",
        "print(df_3_adult.Sex.value_counts(dropna=False, sort=True))\n",
        "\n",
        "# Percentage of members of different \"sexes\"\n",
        "print(df_3_adult.Sex.value_counts(normalize=True, dropna=False, sort=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4nSZi7HSvFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution for Sex\n",
        "\n",
        "target_distribution(y_var=\"Sex\", data=df_3_adult)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDcJHK4mZDJ",
        "colab_type": "text"
      },
      "source": [
        "`Interim conclusion regarding 3)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd60YA-pnVx1",
        "colab_type": "text"
      },
      "source": [
        "4) **Identification of the percentage of certain cross-sectional subpopulations based on the overall dataset** -> *Sample Size Disparity II*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ang6-4g8xrQb",
        "colab_type": "code",
        "outputId": "959a6c9d-d7f7-4b83-b534-1bc81d59ebca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "# Count\n",
        "df_3_adult.groupby(['Sex', 'Race']).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Over-50K</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sex</th>\n",
              "      <th>Race</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">Female</th>\n",
              "      <th>Amer-Indian-Eskimo</th>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian-Pac-Islander</th>\n",
              "      <td>346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Black</th>\n",
              "      <td>1555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Other</th>\n",
              "      <td>109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>8642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">Male</th>\n",
              "      <th>Amer-Indian-Eskimo</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian-Pac-Islander</th>\n",
              "      <td>693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Black</th>\n",
              "      <td>1569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Other</th>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>19174</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Over-50K\n",
              "Sex    Race                        \n",
              "Female Amer-Indian-Eskimo       119\n",
              "       Asian-Pac-Islander       346\n",
              "       Black                   1555\n",
              "       Other                    109\n",
              "       White                   8642\n",
              "Male   Amer-Indian-Eskimo       192\n",
              "       Asian-Pac-Islander       693\n",
              "       Black                   1569\n",
              "       Other                    162\n",
              "       White                  19174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMB4UiT30fdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count\n",
        "df_3_adult.groupby(['Over-50K', \"Race\"]).agg({\"Race\": 'count'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdxgRtCfJ_fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative Count\n",
        "df_3_adult_aggr = df_3_adult\n",
        "df_3_adult_aggr['counting']=1\n",
        "# df_3_adult_aggr.groupby(['Sex', 'Race']).agg({\"counting\": 'count'})\n",
        "\n",
        "# To Do -> Jeweils verrechnen mit Grundgesamtheit von Subpopulationen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GqFB-S1TtxZ",
        "colab_type": "text"
      },
      "source": [
        "Histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFPlbNVGTsn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Histogram Function\n",
        "\n",
        "def plot_histo(data, col, Y_columns):\n",
        "    df = data.copy()\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    \n",
        "    for i in range(0,2):\n",
        "        cnt = []; y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt += [df[df[y_col] == val][col].values]\n",
        "        bins = df[col].nunique()\n",
        "\n",
        "        axs[i].hist(cnt, bins=bins, stacked=True)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uogV0OL0UJDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_columns = [\"Race\", \"Sex\"]\n",
        "\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Gain',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Capital-Loss',Y_columns=Y_columns))\n",
        "print(plot_histo(data = df_3_adult, col='Hours-per-week',Y_columns=Y_columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4LxsVGfWz5a",
        "colab_type": "text"
      },
      "source": [
        "Bar Plot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Wkku-KWcS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Function for Bar Plot\n",
        "\n",
        "def plot_bar(data, col, Y_columns, max_cat=10):\n",
        "    df = data.copy()\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    cat_val = df[col].value_counts()[0:max_cat].index.values\n",
        "    df = df[df[col].isin(cat_val)]\n",
        "\n",
        "    for i in range(0,2):\n",
        "        y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt = df[df[y_col] == val][col].value_counts().sort_index()\n",
        "            axs[i].barh(cnt.index.values, cnt.values)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Bar plot of the \"+col+\" column by \"+y_col)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsBhMXE2XDXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.columns)\n",
        "\n",
        "Y_columns = [\"Race\"]\n",
        "\n",
        "print(plot_bar(data = df_3_adult, col='Sex',Y_columns=Y_columns)) # Error because values for two subplots are expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbTPmIPGL79o",
        "colab_type": "text"
      },
      "source": [
        "5) **Identification of the target feature and the type of class balance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk2M_TZFMSR5",
        "colab_type": "code",
        "outputId": "8ce1b3b6-4f2c-4b49-dd52-a9501fbe588a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "print(df_3_adult[\"Over-50K\"].value_counts(dropna=False))\n",
        "print(df_3_adult[\"Over-50K\"].value_counts(normalize=True, dropna=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    24720\n",
            "1     7841\n",
            "Name: Over-50K, dtype: int64\n",
            "0    0.75919\n",
            "1    0.24081\n",
            "Name: Over-50K, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPYS349CM1xI",
        "colab_type": "text"
      },
      "source": [
        "6) **Identification of percentage of missing values per feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XX0fetkf4Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_3_adult.isna().any())\n",
        "print(df_3_adult.isna().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2awZ2WCmNnX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def assess_NA(data):\n",
        "    \"\"\"\n",
        "    Returns a pandas dataframe denoting the total number of NA values and the percentage of NA values in each column.\n",
        "    The column names are noted on the index.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: dataframe\n",
        "    \"\"\"\n",
        "    # pandas series denoting features and the sum of their null values\n",
        "    null_sum = data.isnull().sum()# instantiate columns for missing data\n",
        "    total = null_sum.sort_values(ascending=False)\n",
        "    percent = ( ((null_sum / len(data.index))*100).round(2) ).sort_values(ascending=False)\n",
        "    \n",
        "    # concatenate along the columns to create the complete dataframe\n",
        "    df_NA = pd.concat([total, percent], axis=1, keys=['Number of NA', 'Percent NA'])\n",
        "    \n",
        "    # drop rows that don't have any missing data; omit if you want to keep all rows\n",
        "    df_NA = df_NA[ (df_NA.T != 0).any() ]\n",
        "    \n",
        "    return df_NA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q4xq53hO4dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assess_NA(df_3_adult)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWPPaUOiSk7j",
        "colab_type": "text"
      },
      "source": [
        "Plot missing values per feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtJ3jzYwSj3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Function \n",
        "\n",
        "def print_missing_values(data):\n",
        "    data_null = pd.DataFrame(len(data) - data.notnull().sum(), columns = ['Count'])\n",
        "    data_null = data_null[data_null['Count'] > 0].sort_values(by='Count', ascending=False)\n",
        "    data_null = data_null/len(data)*100\n",
        "\n",
        "    trace = go.Bar(x=data_null.index, y=data_null['Count'], marker=dict(color='#c0392b'),\n",
        "              name = 'At least one missing value', opacity=0.9)\n",
        "    layout = go.Layout(barmode='group', title='Column with missing values in the dataset', showlegend=True,\n",
        "                   legend=dict(orientation=\"h\"), yaxis=dict(title='Percentage of the dataset'))\n",
        "    fig = go.Figure([trace], layout=layout)\n",
        "    py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dNxlIxJPA-M",
        "colab_type": "text"
      },
      "source": [
        "`Interim conclusion on 6)`\n",
        "\n",
        "No problems with NA\n",
        "\n",
        "**To Do**: Determine if NA are encrypted in different format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0gjLJYQYJy",
        "colab_type": "text"
      },
      "source": [
        "7) **Identification of severity of outliers per feature**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WgSpuWwQoiE",
        "colab_type": "text"
      },
      "source": [
        "Show boxplot per numeric feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5b40piOQr0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_3_adult.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dnfvzpHQahH",
        "colab_type": "code",
        "outputId": "ceb31a00-e666-4aa9-c05d-8b59df14d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Overview boxplot of all numerical features \n",
        "ax = sns.boxplot(data=df_3_adult, orient=\"h\", palette=\"Set2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAD4CAYAAABmBQicAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDLv/wOYGqD4W0+mNAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdI0dxEFRkLd",
        "colab_type": "code",
        "outputId": "6c5a132e-b322-4e6b-f083-04e19c7bf489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# Age Boxplot\n",
        "fig = px.box(df_3_adult, y=\"Age\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"144b5122-3e60-4919-a8b6-19c8dd40c126\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"144b5122-3e60-4919-a8b6-19c8dd40c126\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '144b5122-3e60-4919-a8b6-19c8dd40c126',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Age=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x0\": \" \", \"xaxis\": \"x\", \"y\": [39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32, 40, 34, 25, 32, 38, 43, 40, 54, 35, 43, 59, 56, 19, 54, 39, 49, 23, 20, 45, 30, 22, 48, 21, 19, 31, 48, 31, 53, 24, 49, 25, 57, 53, 44, 41, 29, 25, 18, 47, 50, 47, 43, 46, 35, 41, 30, 30, 32, 48, 42, 29, 36, 28, 53, 49, 25, 19, 31, 29, 23, 79, 27, 40, 67, 18, 31, 18, 52, 46, 59, 44, 53, 49, 33, 30, 43, 57, 37, 28, 30, 34, 29, 48, 37, 48, 32, 76, 44, 47, 20, 29, 32, 17, 30, 31, 42, 24, 38, 56, 28, 36, 53, 56, 49, 55, 22, 21, 40, 30, 29, 19, 47, 20, 31, 35, 39, 28, 24, 38, 37, 46, 38, 43, 27, 20, 49, 61, 27, 19, 45, 70, 31, 22, 36, 64, 43, 47, 34, 33, 21, 52, 48, 23, 71, 29, 42, 68, 25, 44, 28, 45, 36, 39, 46, 18, 66, 27, 28, 51, 27, 28, 27, 21, 34, 18, 33, 44, 43, 30, 40, 37, 34, 41, 53, 31, 58, 38, 24, 41, 47, 41, 23, 36, 40, 35, 24, 26, 19, 51, 42, 37, 18, 36, 35, 58, 17, 44, 37, 35, 60, 54, 37, 50, 38, 45, 25, 31, 64, 90, 54, 53, 18, 60, 66, 75, 65, 35, 41, 25, 33, 28, 59, 40, 41, 38, 23, 40, 41, 24, 20, 38, 56, 58, 32, 40, 45, 41, 42, 59, 19, 58, 42, 20, 32, 45, 50, 36, 45, 17, 59, 26, 37, 19, 64, 33, 33, 61, 17, 50, 27, 30, 43, 44, 35, 25, 24, 22, 42, 34, 60, 21, 57, 41, 50, 25, 50, 36, 31, 29, 21, 27, 65, 37, 39, 24, 38, 48, 21, 31, 55, 24, 43, 26, 46, 35, 41, 26, 34, 19, 36, 22, 24, 77, 22, 29, 62, 39, 43, 35, 29, 76, 63, 23, 43, 58, 66, 41, 26, 47, 55, 53, 17, 30, 49, 19, 45, 26, 38, 36, 33, 22, 43, 67, 30, 56, 31, 33, 26, 33, 46, 59, 38, 65, 40, 42, 26, 36, 62, 43, 43, 22, 28, 56, 22, 57, 39, 26, 17, 40, 45, 44, 20, 33, 23, 46, 38, 54, 46, 25, 46, 36, 23, 29, 44, 19, 19, 35, 27, 46, 34, 34, 44, 45, 20, 25, 52, 20, 28, 50, 34, 28, 41, 28, 46, 28, 32, 41, 24, 33, 46, 31, 35, 52, 30, 34, 34, 20, 17, 32, 29, 33, 25, 36, 23, 63, 47, 80, 17, 40, 30, 27, 33, 34, 34, 23, 42, 29, 45, 24, 44, 27, 20, 44, 51, 20, 17, 19, 45, 60, 42, 44, 40, 30, 38, 23, 32, 44, 54, 32, 50, 24, 37, 52, 38, 49, 30, 60, 22, 35, 30, 67, 46, 17, 22, 27, 23, 33, 43, 28, 41, 52, 25, 63, 59, 45, 38, 40, 46, 35, 34, 33, 41, 20, 23, 26, 72, 23, 62, 52, 58, 25, 24, 19, 43, 47, 39, 49, 53, 32, 34, 28, 57, 25, 20, 21, 34, 38, 50, 24, 37, 44, 28, 42, 74, 38, 44, 44, 26, 36, 41, 67, 39, 57, 29, 31, 34, 44, 29, 30, 27, 27, 32, 58, 35, 21, 28, 46, 36, 72, 35, 33, 69, 35, 31, 34, 30, 28, 54, 47, 24, 52, 20, 43, 45, 29, 47, 24, 51, 17, 37, 27, 29, 18, 36, 58, 26, 65, 57, 59, 27, 31, 21, 29, 18, 52, 57, 42, 55, 60, 31, 23, 27, 23, 42, 25, 49, 32, 19, 60, 42, 35, 48, 51, 29, 36, 17, 52, 24, 24, 26, 27, 39, 30, 50, 52, 46, 23, 45, 65, 29, 47, 30, 34, 38, 33, 49, 47, 49, 43, 30, 58, 19, 21, 33, 47, 52, 26, 60, 21, 36, 31, 50, 31, 71, 58, 30, 20, 24, 35, 38, 27, 29, 70, 34, 44, 43, 44, 35, 27, 43, 20, 42, 27, 42, 20, 24, 48, 17, 17, 33, 50, 22, 17, 32, 31, 58, 29, 37, 34, 23, 18, 34, 66, 45, 41, 26, 54, 24, 42, 20, 23, 25, 35, 31, 30, 19, 36, 31, 21, 31, 46, 44, 40, 45, 60, 18, 28, 36, 36, 40, 36, 31, 33, 19, 22, 34, 33, 41, 29, 50, 42, 43, 44, 20, 31, 65, 23, 55, 26, 25, 45, 35, 36, 43, 56, 42, 19, 55, 42, 26, 17, 42, 55, 32, 29, 46, 29, 22, 58, 23, 39, 27, 54, 33, 46, 37, 36, 59, 34, 53, 51, 32, 31, 22, 47, 32, 26, 37, 31, 37, 55, 23, 36, 34, 43, 54, 43, 28, 40, 41, 40, 42, 61, 18, 59, 21, 48, 41, 18, 23, 60, 22, 61, 25, 46, 43, 43, 24, 68, 31, 17, 32, 50, 33, 64, 20, 30, 22, 43, 22, 17, 47, 41, 56, 64, 47, 48, 31, 29, 30, 32, 57, 62, 39, 43, 24, 42, 53, 26, 73, 72, 55, 25, 41, 24, 63, 17, 35, 51, 62, 55, 43, 40, 37, 39, 31, 61, 26, 46, 26, 48, 34, 34, 59, 34, 49, 18, 43, 48, 28, 33, 24, 21, 17, 39, 29, 44, 24, 71, 51, 55, 41, 23, 38, 39, 19, 49, 32, 27, 38, 33, 63, 23, 33, 54, 29, 48, 35, 28, 37, 42, 40, 47, 22, 30, 28, 63, 51, 22, 27, 35, 46, 37, 24, 37, 53, 27, 38, 53, 34, 23, 39, 43, 67, 81, 21, 23, 25, 42, 38, 64, 32, 37, 51, 22, 33, 42, 19, 51, 23, 37, 37, 47, 41, 33, 31, 31, 35, 38, 18, 20, 54, 40, 36, 43, 44, 44, 43, 51, 18, 51, 41, 44, 33, 33, 42, 25, 32, 32, 36, 28, 20, 35, 55, 48, 36, 34, 28, 67, 37, 44, 25, 21, 40, 78, 34, 49, 22, 23, 60, 45, 63, 41, 47, 44, 51, 46, 41, 50, 47, 35, 56, 45, 48, 40, 39, 20, 50, 38, 23, 31, 58, 66, 39, 54, 26, 51, 34, 50, 42, 38, 48, 33, 51, 22, 19, 41, 48, 42, 34, 23, 56, 30, 45, 48, 48, 31, 20, 27, 32, 76, 19, 66, 37, 34, 34, 90, 23, 43, 44, 24, 28, 64, 23, 20, 36, 61, 53, 30, 52, 38, 32, 30, 41, 49, 45, 43, 61, 54, 34, 49, 38, 35, 36, 30, 36, 22, 38, 37, 48, 17, 19, 27, 22, 49, 43, 43, 19, 58, 41, 31, 30, 32, 29, 42, 38, 41, 44, 31, 34, 21, 22, 62, 19, 29, 43, 68, 45, 39, 41, 34, 34, 31, 29, 41, 41, 35, 33, 37, 69, 34, 60, 36, 41, 58, 53, 29, 45, 34, 36, 48, 20, 35, 63, 41, 28, 30, 34, 31, 37, 42, 31, 46, 35, 27, 41, 19, 20, 59, 40, 56, 33, 30, 46, 61, 50, 25, 40, 24, 51, 43, 34, 46, 28, 26, 20, 44, 51, 33, 54, 50, 33, 65, 22, 88, 40, 51, 50, 25, 20, 47, 58, 22, 50, 47, 24, 50, 40, 36, 32, 44, 51, 59, 65, 24, 27, 51, 48, 39, 28, 19, 49, 51, 59, 18, 50, 45, 30, 37, 44, 33, 61, 61, 38, 22, 36, 44, 37, 31, 54, 20, 63, 48, 33, 31, 38, 45, 48, 53, 55, 38, 29, 36, 42, 50, 75, 46, 52, 27, 43, 31, 23, 34, 41, 31, 42, 35, 55, 36, 65, 38, 62, 54, 49, 34, 32, 51, 48, 30, 52, 28, 29, 38, 57, 43, 37, 30, 53, 69, 73, 22, 31, 38, 50, 37, 44, 47, 27, 28, 25, 20, 51, 46, 67, 47, 42, 52, 42, 65, 21, 50, 46, 47, 57, 37, 50, 58, 63, 39, 46, 36, 26, 21, 41, 44, 22, 29, 29, 58, 36, 30, 37, 29, 26, 43, 49, 45, 28, 20, 51, 32, 24, 35, 35, 41, 48, 61, 20, 27, 55, 39, 17, 47, 52, 24, 29, 25, 20, 23, 57, 51, 33, 39, 40, 71, 38, 28, 57, 47, 59, 26, 35, 61, 17, 36, 62, 34, 32, 42, 39, 58, 61, 40, 31, 36, 23, 33, 52, 22, 51, 31, 49, 49, 53, 52, 20, 26, 61, 43, 46, 43, 41, 44, 54, 65, 32, 25, 23, 25, 19, 37, 45, 37, 40, 19, 17, 27, 40, 59, 19, 43, 42, 54, 54, 51, 50, 40, 55, 50, 31, 42, 24, 30, 39, 37, 33, 48, 43, 39, 53, 52, 28, 32, 31, 31, 47, 19, 40, 40, 41, 53, 46, 45, 39, 19, 57, 49, 59, 32, 18, 35, 58, 28, 36, 41, 47, 50, 61, 36, 38, 56, 50, 45, 44, 51, 44, 25, 60, 17, 49, 46, 56, 25, 22, 76, 41, 48, 46, 30, 39, 40, 70, 35, 49, 24, 40, 61, 25, 28, 69, 22, 47, 38, 26, 17, 33, 43, 58, 53, 26, 43, 31, 30, 25, 57, 37, 32, 26, 45, 53, 27, 28, 32, 35, 36, 19, 37, 49, 20, 64, 60, 32, 58, 42, 20, 42, 25, 30, 37, 66, 59, 44, 46, 59, 25, 17, 47, 47, 30, 20, 26, 20, 33, 48, 52, 37, 47, 40, 48, 49, 50, 30, 34, 43, 22, 35, 26, 62, 39, 59, 69, 27, 17, 38, 41, 27, 23, 17, 56, 24, 62, 44, 23, 41, 25, 21, 18, 45, 75, 51, 42, 47, 29, 33, 43, 20, 19, 44, 24, 19, 49, 52, 38, 24, 32, 72, 26, 31, 29, 39, 48, 40, 51, 28, 22, 23, 65, 26, 20, 55, 38, 33, 33, 37, 19, 24, 30, 57, 22, 57, 46, 67, 35, 47, 39, 50, 58, 36, 45, 54, 26, 60, 46, 26, 32, 38, 48, 44, 37, 44, 31, 61, 51, 52, 45, 36, 39, 67, 40, 27, 37, 57, 27, 28, 34, 54, 52, 42, 48, 65, 55, 71, 33, 30, 35, 59, 19, 36, 26, 34, 41, 44, 26, 47, 18, 55, 49, 44, 43, 62, 44, 39, 50, 35, 52, 46, 44, 43, 51, 20, 47, 43, 64, 24, 27, 32, 37, 26, 43, 50, 20, 37, 43, 37, 53, 44, 25, 17, 19, 53, 44, 51, 49, 35, 44, 54, 37, 18, 24, 29, 22, 35, 56, 32, 24, 43, 73, 47, 49, 38, 31, 39, 42, 41, 35, 38, 31, 51, 38, 40, 21, 44, 55, 29, 49, 31, 24, 46, 26, 35, 27, 64, 26, 21, 45, 57, 30, 45, 50, 38, 43, 42, 32, 20, 27, 45, 40, 38, 37, 60, 53, 25, 47, 40, 51, 19, 42, 65, 41, 28, 46, 49, 24, 32, 71, 26, 29, 50, 51, 17, 49, 31, 23, 59, 17, 43, 38, 37, 29, 39, 31, 42, 35, 39, 40, 66, 30, 43, 47, 45, 31, 33, 32, 28, 50, 63, 28, 38, 52, 38, 41, 50, 59, 32, 73, 52, 57, 35, 51, 22, 49, 22, 46, 48, 52, 43, 25, 31, 19, 19, 51, 51, 22, 37, 19, 45, 21, 52, 46, 42, 56, 23, 58, 70, 30, 45, 23, 34, 38, 24, 53, 45, 21, 21, 34, 32, 21, 63, 23, 28, 29, 25, 44, 18, 26, 47, 23, 46, 61, 25, 38, 60, 23, 36, 46, 32, 40, 31, 31, 32, 21, 63, 44, 42, 23, 30, 24, 42, 44, 27, 27, 22, 68, 55, 22, 25, 68, 62, 26, 34, 68, 42, 45, 26, 25, 23, 29, 65, 31, 26, 39, 44, 45, 26, 26, 36, 38, 39, 52, 39, 41, 42, 47, 28, 25, 35, 32, 38, 44, 21, 27, 30, 30, 45, 20, 57, 43, 50, 28, 25, 25, 90, 38, 35, 50, 26, 39, 56, 24, 46, 18, 37, 57, 43, 29, 40, 44, 77, 30, 21, 66, 40, 68, 64, 26, 32, 32, 51, 50, 49, 28, 29, 21, 28, 55, 45, 35, 31, 26, 30, 33, 40, 23, 35, 48, 39, 53, 49, 31, 36, 60, 29, 54, 28, 34, 30, 19, 26, 47, 50, 30, 44, 49, 75, 37, 51, 35, 28, 66, 72, 39, 27, 57, 24, 28, 23, 31, 23, 31, 27, 30, 31, 34, 57, 39, 23, 24, 36, 32, 35, 36, 23, 47, 35, 44, 32, 32, 63, 34, 47, 27, 23, 19, 50, 49, 36, 23, 61, 41, 27, 53, 17, 23, 53, 28, 24, 22, 53, 18, 24, 20, 30, 37, 34, 19, 20, 47, 20, 22, 42, 60, 32, 46, 66, 37, 34, 45, 35, 63, 51, 36, 25, 51, 25, 19, 47, 59, 34, 40, 42, 28, 37, 51, 25, 31, 37, 42, 29, 35, 51, 22, 49, 26, 56, 52, 61, 47, 52, 30, 43, 35, 17, 42, 26, 58, 66, 30, 46, 45, 57, 25, 21, 37, 45, 61, 39, 42, 20, 23, 25, 47, 24, 20, 35, 44, 47, 28, 41, 31, 27, 47, 44, 61, 42, 46, 46, 54, 49, 56, 52, 34, 25, 18, 58, 31, 29, 28, 30, 21, 44, 75, 66, 26, 33, 57, 47, 41, 21, 17, 61, 47, 22, 47, 45, 32, 59, 38, 33, 41, 51, 42, 27, 60, 64, 37, 54, 36, 34, 41, 35, 63, 46, 47, 24, 29, 74, 50, 22, 48, 30, 32, 39, 49, 25, 50, 44, 65, 28, 55, 49, 31, 47, 45, 35, 34, 49, 39, 27, 42, 60, 64, 33, 23, 45, 34, 53, 22, 57, 20, 23, 31, 23, 30, 32, 42, 51, 59, 36, 45, 35, 33, 43, 60, 37, 35, 36, 38, 54, 51, 45, 63, 51, 58, 64, 20, 44, 39, 45, 44, 22, 63, 26, 42, 20, 29, 38, 25, 27, 46, 31, 24, 43, 23, 32, 44, 40, 38, 23, 19, 41, 48, 23, 47, 42, 17, 29, 21, 18, 31, 27, 29, 23, 41, 46, 34, 42, 18, 46, 18, 52, 56, 21, 22, 31, 41, 25, 43, 45, 26, 90, 45, 30, 42, 20, 53, 64, 56, 42, 26, 31, 40, 58, 41, 58, 62, 65, 40, 45, 22, 23, 63, 39, 37, 25, 56, 32, 53, 32, 23, 35, 51, 19, 41, 22, 23, 36, 27, 45, 23, 21, 44, 29, 36, 18, 62, 25, 34, 31, 37, 33, 22, 28, 22, 47, 67, 20, 34, 26, 36, 28, 22, 47, 33, 34, 25, 37, 36, 61, 74, 57, 55, 20, 43, 54, 25, 42, 23, 17, 39, 63, 19, 56, 33, 37, 27, 36, 30, 22, 35, 20, 18, 25, 53, 21, 19, 34, 32, 20, 70, 57, 54, 34, 48, 42, 45, 27, 51, 36, 33, 50, 44, 39, 55, 24, 35, 32, 66, 68, 29, 24, 61, 34, 55, 20, 41, 36, 60, 61, 35, 42, 26, 44, 31, 39, 20, 32, 32, 61, 54, 23, 19, 31, 35, 29, 29, 25, 46, 35, 20, 24, 48, 37, 60, 39, 29, 31, 38, 26, 64, 46, 69, 39, 48, 46, 42, 39, 50, 24, 30, 39, 29, 23, 21, 32, 69, 55, 41, 44, 18, 74, 40, 27, 75, 55, 52, 54, 36, 76, 26, 18, 38, 50, 19, 32, 57, 20, 27, 31, 60, 52, 61, 57, 19, 59, 34, 31, 42, 70, 47, 47, 29, 27, 25, 51, 28, 35, 36, 54, 46, 52, 65, 43, 41, 34, 47, 49, 21, 20, 18, 27, 34, 38, 24, 34, 47, 27, 63, 34, 25, 54, 19, 35, 24, 26, 59, 43, 20, 41, 51, 27, 36, 25, 55, 22, 58, 43, 39, 40, 46, 54, 19, 24, 53, 24, 29, 56, 54, 21, 34, 56, 34, 19, 32, 48, 67, 47, 52, 40, 28, 41, 57, 51, 26, 36, 33, 46, 50, 33, 77, 39, 67, 35, 23, 37, 41, 60, 32, 56, 45, 24, 18, 31, 21, 45, 23, 45, 18, 25, 31, 19, 41, 74, 27, 23, 31, 40, 41, 25, 42, 24, 42, 45, 52, 34, 32, 54, 42, 32, 40, 55, 66, 52, 21, 33, 52, 28, 27, 42, 27, 47, 19, 46, 55, 50, 32, 28, 23, 43, 34, 19, 37, 27, 18, 31, 49, 26, 48, 39, 41, 48, 40, 70, 20, 38, 37, 49, 51, 43, 38, 18, 23, 34, 62, 33, 31, 64, 26, 55, 61, 37, 78, 27, 49, 55, 42, 22, 61, 64, 46, 24, 27, 47, 18, 48, 50, 40, 56, 34, 56, 18, 21, 40, 27, 52, 46, 23, 50, 19, 20, 39, 24, 72, 42, 42, 51, 39, 40, 34, 19, 26, 45, 28, 30, 32, 19, 21, 54, 62, 25, 49, 32, 26, 37, 58, 37, 41, 38, 47, 42, 30, 43, 34, 57, 46, 32, 20, 47, 55, 53, 43, 80, 24, 43, 22, 40, 48, 56, 34, 51, 24, 19, 34, 24, 41, 24, 22, 32, 41, 20, 31, 55, 49, 26, 27, 35, 40, 59, 61, 31, 43, 61, 54, 30, 20, 27, 44, 64, 55, 34, 61, 28, 34, 47, 26, 35, 36, 47, 17, 35, 33, 25, 18, 47, 44, 41, 30, 53, 53, 40, 24, 27, 50, 36, 41, 41, 31, 32, 22, 22, 18, 39, 38, 35, 23, 54, 36, 63, 39, 57, 58, 29, 17, 40, 69, 28, 38, 44, 45, 33, 41, 37, 50, 48, 26, 36, 21, 37, 20, 54, 28, 27, 33, 46, 28, 17, 40, 47, 44, 25, 35, 29, 30, 30, 56, 25, 28, 43, 47, 46, 41, 38, 29, 25, 56, 31, 22, 31, 28, 53, 45, 71, 65, 39, 19, 34, 36, 40, 90, 56, 21, 48, 45, 46, 64, 29, 22, 47, 67, 43, 42, 31, 53, 81, 33, 17, 55, 28, 24, 40, 30, 24, 26, 38, 30, 33, 40, 63, 23, 56, 42, 40, 49, 35, 52, 28, 35, 21, 43, 32, 29, 23, 50, 34, 19, 47, 37, 25, 75, 24, 28, 17, 31, 31, 18, 45, 31, 27, 48, 22, 34, 26, 53, 60, 49, 40, 48, 23, 68, 42, 37, 27, 38, 27, 75, 49, 27, 36, 36, 35, 36, 53, 27, 34, 61, 29, 40, 37, 22, 43, 55, 28, 23, 64, 46, 29, 44, 20, 62, 49, 29, 35, 39, 39, 37, 38, 72, 31, 55, 38, 33, 25, 19, 37, 49, 26, 35, 44, 37, 42, 64, 28, 35, 40, 55, 21, 49, 51, 57, 51, 38, 24, 45, 28, 42, 22, 28, 60, 31, 47, 40, 30, 61, 71, 42, 44, 46, 50, 27, 40, 21, 25, 61, 54, 31, 53, 39, 57, 48, 30, 35, 42, 34, 59, 69, 29, 40, 21, 40, 43, 36, 41, 72, 20, 41, 34, 48, 27, 35, 28, 27, 20, 18, 32, 38, 23, 28, 40, 35, 74, 28, 23, 20, 44, 46, 20, 51, 40, 48, 60, 48, 46, 48, 22, 24, 45, 18, 39, 65, 22, 20, 28, 46, 42, 61, 28, 18, 24, 45, 24, 44, 20, 35, 52, 25, 19, 35, 54, 21, 26, 42, 19, 55, 44, 25, 26, 64, 41, 25, 43, 45, 48, 60, 45, 51, 37, 22, 44, 27, 47, 32, 40, 41, 32, 26, 47, 55, 38, 24, 64, 56, 41, 33, 33, 21, 59, 58, 43, 21, 45, 53, 41, 27, 53, 38, 31, 41, 22, 43, 41, 27, 41, 36, 49, 20, 58, 35, 30, 21, 29, 40, 54, 35, 35, 40, 59, 43, 39, 51, 71, 33, 28, 30, 20, 51, 39, 62, 37, 28, 23, 39, 50, 31, 45, 23, 47, 59, 32, 82, 49, 74, 22, 32, 34, 59, 36, 59, 47, 53, 25, 44, 57, 42, 58, 39, 32, 45, 47, 52, 56, 52, 34, 36, 41, 30, 44, 17, 34, 41, 29, 25, 42, 48, 26, 64, 46, 38, 49, 23, 46, 30, 49, 47, 44, 31, 19, 45, 34, 26, 38, 31, 36, 31, 30, 37, 30, 21, 73, 38, 30, 33, 52, 58, 34, 35, 59, 45, 27, 31, 45, 40, 27, 19, 29, 34, 32, 49, 33, 74, 62, 39, 43, 22, 24, 76, 25, 19, 31, 53, 61, 47, 37, 50, 27, 58, 43, 39, 36, 56, 26, 23, 47, 68, 47, 45, 26, 20, 46, 37, 37, 43, 59, 39, 32, 63, 45, 59, 36, 26, 34, 41, 26, 53, 33, 46, 79, 31, 24, 57, 40, 41, 20, 45, 27, 36, 30, 36, 25, 30, 31, 49, 26, 25, 29, 24, 41, 35, 57, 21, 29, 36, 20, 51, 39, 36, 40, 61, 38, 20, 21, 40, 46, 21, 35, 39, 38, 30, 28, 32, 46, 40, 47, 44, 38, 51, 46, 51, 45, 20, 44, 33, 18, 61, 50, 47, 34, 53, 39, 58, 66, 26, 41, 47, 31, 52, 29, 43, 42, 35, 41, 34, 52, 61, 39, 19, 46, 39, 22, 30, 40, 43, 37, 23, 39, 54, 46, 20, 50, 17, 36, 37, 53, 50, 38, 63, 53, 40, 23, 20, 46, 28, 59, 41, 33, 56, 50, 31, 41, 38, 25, 18, 49, 39, 22, 21, 35, 21, 38, 55, 31, 17, 31, 25, 39, 50, 38, 50, 63, 31, 23, 18, 43, 34, 18, 46, 35, 46, 59, 31, 27, 41, 42, 55, 34, 44, 26, 36, 69, 33, 30, 35, 25, 21, 18, 35, 20, 26, 46, 31, 40, 40, 31, 32, 23, 25, 37, 18, 55, 52, 26, 37, 39, 62, 19, 47, 51, 55, 30, 59, 45, 52, 21, 31, 29, 24, 17, 47, 46, 32, 34, 63, 21, 18, 49, 81, 36, 47, 36, 39, 46, 43, 39, 29, 23, 57, 38, 25, 39, 24, 56, 39, 23, 17, 67, 41, 24, 51, 30, 47, 56, 51, 57, 34, 24, 57, 38, 23, 35, 58, 19, 19, 68, 43, 17, 53, 37, 21, 63, 31, 29, 43, 27, 25, 28, 54, 74, 31, 23, 19, 39, 61, 59, 27, 19, 23, 25, 40, 53, 32, 72, 61, 42, 17, 73, 41, 23, 23, 65, 45, 52, 24, 23, 30, 26, 37, 17, 31, 52, 63, 27, 31, 71, 35, 44, 36, 22, 32, 30, 60, 25, 76, 41, 26, 45, 32, 22, 39, 63, 23, 61, 31, 25, 33, 39, 53, 26, 21, 18, 34, 44, 31, 44, 42, 43, 33, 21, 30, 55, 57, 37, 32, 62, 31, 51, 37, 45, 47, 31, 75, 43, 39, 30, 27, 44, 53, 40, 18, 33, 20, 38, 48, 46, 51, 35, 24, 30, 70, 26, 34, 46, 70, 37, 36, 23, 28, 44, 58, 53, 23, 26, 21, 34, 43, 22, 41, 36, 41, 27, 42, 27, 41, 32, 21, 44, 29, 19, 21, 46, 38, 43, 54, 38, 18, 50, 30, 30, 28, 26, 37, 32, 64, 18, 30, 36, 18, 27, 49, 46, 37, 30, 29, 22, 25, 38, 32, 35, 56, 29, 41, 47, 23, 46, 28, 39, 42, 39, 68, 37, 57, 22, 24, 19, 55, 37, 37, 28, 27, 40, 51, 52, 74, 49, 20, 40, 80, 63, 63, 51, 23, 66, 33, 49, 20, 33, 26, 52, 33, 47, 23, 53, 19, 26, 53, 65, 68, 30, 28, 54, 30, 22, 25, 19, 19, 32, 23, 51, 25, 46, 38, 26, 46, 25, 20, 29, 51, 40, 24, 24, 31, 54, 34, 19, 21, 21, 23, 37, 40, 57, 48, 24, 43, 32, 40, 52, 49, 45, 62, 29, 49, 55, 20, 37, 43, 29, 44, 28, 38, 45, 22, 19, 20, 60, 30, 22, 51, 31, 48, 43, 51, 41, 62, 50, 46, 54, 68, 23, 49, 24, 30, 38, 34, 22, 30, 61, 36, 51, 43, 54, 63, 51, 63, 51, 27, 35, 31, 46, 53, 26, 43, 39, 50, 54, 62, 57, 19, 29, 20, 53, 18, 22, 23, 69, 56, 53, 28, 28, 33, 48, 61, 49, 48, 32, 51, 27, 19, 47, 25, 72, 29, 22, 57, 50, 45, 52, 53, 36, 46, 20, 75, 32, 28, 53, 17, 46, 39, 38, 36, 63, 28, 28, 49, 43, 38, 39, 42, 40, 45, 29, 24, 30, 44, 40, 30, 42, 52, 42, 39, 43, 62, 30, 83, 39, 41, 49, 25, 48, 59, 44, 36, 40, 21, 41, 24, 67, 24, 50, 48, 55, 29, 50, 64, 49, 20, 30, 37, 52, 22, 77, 19, 52, 42, 35, 27, 54, 37, 37, 28, 63, 19, 28, 23, 44, 31, 35, 64, 40, 59, 22, 40, 35, 20, 35, 28, 28, 37, 72, 25, 37, 19, 31, 41, 31, 44, 20, 57, 26, 44, 29, 36, 40, 21, 21, 34, 24, 28, 35, 35, 38, 44, 39, 57, 19, 29, 23, 50, 51, 28, 44, 29, 31, 27, 34, 50, 27, 31, 46, 39, 40, 28, 18, 51, 58, 66, 36, 38, 25, 25, 90, 41, 34, 33, 33, 35, 39, 59, 19, 39, 19, 27, 52, 59, 28, 37, 50, 36, 34, 23, 19, 33, 27, 31, 21, 31, 59, 40, 19, 47, 34, 57, 51, 21, 30, 46, 40, 52, 23, 90, 25, 43, 38, 22, 33, 34, 65, 50, 29, 25, 54, 49, 44, 24, 25, 65, 56, 39, 23, 20, 24, 41, 40, 55, 38, 23, 44, 45, 20, 29, 43, 30, 28, 31, 52, 36, 38, 66, 63, 21, 33, 46, 20, 47, 33, 66, 20, 28, 46, 25, 37, 31, 49, 22, 52, 29, 18, 31, 23, 27, 54, 50, 46, 24, 17, 49, 54, 41, 49, 36, 23, 63, 67, 23, 22, 66, 25, 30, 35, 67, 30, 26, 34, 21, 18, 21, 76, 51, 26, 18, 18, 34, 59, 55, 33, 25, 48, 64, 47, 26, 36, 21, 54, 30, 41, 53, 34, 20, 24, 26, 25, 35, 41, 33, 18, 27, 32, 51, 38, 41, 26, 38, 54, 30, 31, 26, 75, 29, 61, 24, 22, 34, 42, 24, 42, 27, 39, 20, 51, 68, 28, 43, 35, 44, 41, 44, 20, 46, 27, 32, 46, 26, 20, 36, 49, 32, 32, 49, 36, 22, 52, 42, 22, 72, 30, 44, 23, 22, 59, 46, 30, 64, 31, 64, 45, 59, 32, 43, 66, 23, 49, 32, 49, 33, 28, 47, 41, 34, 25, 20, 21, 25, 46, 37, 41, 44, 30, 54, 35, 58, 21, 35, 35, 19, 48, 56, 65, 40, 26, 45, 26, 32, 35, 22, 37, 25, 64, 46, 30, 63, 51, 68, 36, 28, 41, 47, 22, 33, 34, 46, 28, 36, 18, 33, 51, 56, 54, 29, 40, 53, 22, 33, 28, 63, 53, 37, 50, 22, 40, 22, 21, 25, 20, 43, 29, 32, 42, 33, 27, 22, 29, 29, 20, 35, 31, 33, 67, 46, 45, 17, 35, 48, 49, 20, 27, 64, 53, 52, 29, 32, 48, 30, 34, 19, 39, 28, 17, 54, 24, 25, 25, 48, 24, 31, 47, 45, 19, 52, 52, 19, 60, 43, 24, 20, 51, 38, 21, 40, 54, 35, 52, 53, 56, 35, 43, 29, 29, 27, 58, 64, 53, 21, 40, 52, 38, 40, 39, 56, 68, 24, 40, 43, 38, 45, 18, 57, 49, 25, 52, 56, 26, 19, 25, 25, 21, 48, 33, 31, 26, 37, 29, 20, 28, 49, 73, 49, 51, 40, 24, 18, 41, 46, 46, 64, 50, 53, 21, 22, 37, 39, 30, 25, 26, 43, 59, 27, 55, 20, 25, 28, 29, 60, 37, 25, 27, 50, 41, 46, 35, 21, 35, 40, 49, 29, 43, 39, 35, 52, 25, 20, 46, 24, 29, 21, 37, 43, 19, 47, 61, 21, 35, 60, 27, 57, 56, 27, 47, 19, 28, 43, 63, 22, 25, 35, 20, 48, 30, 31, 20, 57, 41, 33, 27, 34, 22, 50, 40, 37, 40, 53, 38, 25, 51, 22, 18, 41, 33, 33, 67, 47, 24, 32, 29, 25, 33, 55, 31, 32, 18, 36, 58, 34, 19, 21, 36, 37, 43, 52, 48, 43, 22, 57, 37, 39, 54, 27, 21, 65, 30, 22, 26, 25, 35, 47, 23, 42, 39, 55, 24, 46, 53, 44, 40, 48, 26, 22, 35, 20, 43, 37, 30, 41, 38, 77, 42, 20, 36, 32, 27, 25, 40, 18, 49, 17, 29, 47, 24, 58, 29, 45, 30, 19, 59, 47, 41, 28, 46, 40, 66, 30, 44, 58, 21, 59, 35, 45, 30, 33, 26, 48, 31, 36, 48, 30, 33, 19, 37, 53, 18, 38, 34, 45, 33, 35, 22, 35, 45, 17, 50, 38, 33, 46, 25, 64, 62, 32, 17, 61, 40, 21, 39, 35, 29, 27, 20, 34, 51, 45, 20, 31, 29, 56, 55, 26, 46, 32, 28, 27, 23, 59, 26, 36, 18, 33, 60, 40, 48, 27, 19, 38, 26, 45, 20, 32, 22, 22, 38, 34, 46, 39, 79, 60, 24, 31, 37, 23, 31, 29, 22, 69, 55, 43, 23, 29, 22, 22, 25, 29, 25, 33, 33, 31, 30, 55, 36, 47, 28, 42, 51, 62, 65, 23, 60, 47, 50, 70, 32, 37, 31, 76, 54, 32, 31, 31, 65, 30, 63, 21, 40, 26, 18, 34, 66, 44, 30, 36, 31, 22, 28, 23, 49, 39, 67, 19, 30, 25, 24, 46, 37, 41, 40, 44, 32, 58, 48, 34, 29, 46, 36, 59, 29, 50, 25, 27, 48, 51, 36, 42, 49, 33, 40, 26, 69, 59, 30, 31, 30, 27, 20, 56, 24, 47, 51, 34, 47, 47, 20, 34, 31, 42, 52, 56, 53, 26, 81, 28, 57, 34, 31, 24, 20, 68, 60, 37, 25, 36, 45, 34, 31, 47, 60, 75, 43, 18, 25, 20, 32, 57, 43, 18, 41, 48, 35, 41, 23, 32, 33, 58, 22, 33, 42, 25, 39, 35, 50, 49, 25, 47, 46, 42, 37, 25, 29, 26, 26, 45, 34, 33, 43, 28, 51, 21, 23, 30, 46, 38, 20, 36, 37, 44, 33, 45, 47, 54, 17, 43, 45, 37, 44, 55, 41, 23, 38, 23, 35, 18, 47, 25, 41, 23, 41, 26, 30, 42, 35, 28, 29, 21, 48, 44, 34, 19, 32, 31, 25, 47, 46, 18, 31, 28, 38, 33, 17, 41, 46, 38, 24, 49, 60, 44, 63, 40, 45, 49, 36, 65, 41, 25, 20, 60, 17, 28, 31, 47, 45, 51, 23, 44, 34, 19, 51, 46, 30, 37, 56, 29, 46, 55, 45, 34, 45, 41, 22, 34, 27, 29, 19, 47, 43, 52, 33, 29, 23, 23, 39, 43, 31, 47, 26, 58, 47, 55, 26, 28, 36, 40, 19, 63, 42, 76, 23, 38, 33, 48, 56, 32, 44, 27, 33, 22, 18, 44, 42, 34, 45, 57, 59, 33, 52, 17, 29, 63, 20, 42, 50, 55, 27, 28, 55, 44, 53, 23, 25, 44, 55, 48, 46, 26, 30, 32, 52, 52, 39, 57, 21, 22, 26, 26, 44, 42, 40, 22, 62, 44, 18, 33, 45, 18, 32, 33, 20, 68, 61, 48, 45, 36, 32, 35, 39, 53, 30, 37, 40, 25, 41, 52, 33, 28, 25, 53, 44, 35, 38, 34, 33, 24, 65, 29, 39, 46, 37, 47, 62, 54, 90, 33, 57, 25, 44, 52, 21, 31, 39, 59, 29, 49, 34, 22, 32, 39, 46, 50, 34, 28, 18, 20, 20, 55, 22, 60, 40, 47, 23, 57, 52, 42, 25, 49, 41, 40, 38, 58, 26, 57, 28, 42, 28, 17, 39, 28, 42, 51, 27, 41, 28, 54, 73, 49, 47, 27, 42, 32, 32, 27, 29, 22, 41, 49, 58, 41, 26, 21, 32, 37, 23, 42, 25, 67, 41, 49, 36, 18, 25, 43, 38, 21, 65, 66, 33, 51, 27, 17, 32, 61, 38, 43, 40, 63, 76, 42, 43, 76, 41, 17, 30, 30, 22, 53, 28, 25, 24, 47, 34, 26, 63, 25, 28, 36, 24, 51, 44, 18, 54, 61, 51, 43, 51, 50, 27, 24, 20, 52, 57, 49, 42, 22, 38, 21, 39, 52, 25, 50, 35, 50, 58, 48, 30, 18, 29, 52, 63, 36, 24, 30, 23, 35, 21, 28, 20, 18, 21, 27, 42, 21, 51, 27, 23, 57, 22, 59, 38, 20, 90, 35, 50, 45, 44, 23, 34, 23, 39, 19, 54, 40, 44, 31, 25, 33, 32, 36, 47, 80, 21, 35, 77, 30, 35, 49, 33, 53, 41, 57, 21, 19, 28, 42, 29, 38, 26, 41, 40, 36, 28, 25, 42, 32, 32, 37, 39, 49, 28, 24, 19, 31, 40, 29, 59, 52, 27, 45, 58, 53, 23, 35, 49, 49, 25, 47, 28, 20, 74, 19, 28, 31, 61, 30, 20, 35, 23, 25, 19, 30, 26, 34, 49, 35, 38, 36, 33, 54, 19, 18, 60, 28, 43, 49, 25, 17, 41, 30, 90, 25, 27, 29, 55, 31, 43, 63, 46, 59, 42, 26, 26, 20, 19, 61, 23, 24, 35, 42, 31, 31, 63, 29, 43, 22, 17, 23, 35, 32, 50, 25, 26, 32, 29, 57, 90, 35, 31, 60, 21, 71, 22, 50, 37, 23, 74, 67, 59, 28, 45, 63, 39, 33, 32, 34, 39, 29, 22, 53, 46, 51, 44, 56, 27, 52, 39, 45, 37, 33, 35, 46, 35, 33, 48, 32, 18, 20, 35, 19, 21, 56, 28, 45, 32, 44, 55, 34, 33, 36, 25, 28, 39, 53, 47, 49, 22, 31, 44, 19, 37, 60, 23, 44, 54, 23, 21, 19, 51, 41, 61, 19, 22, 22, 49, 35, 38, 29, 56, 41, 62, 27, 55, 25, 33, 26, 43, 53, 43, 56, 25, 60, 28, 21, 40, 29, 20, 25, 18, 32, 27, 48, 20, 55, 33, 21, 43, 35, 39, 19, 25, 44, 27, 39, 69, 34, 20, 37, 43, 62, 34, 31, 42, 26, 39, 25, 54, 44, 34, 21, 50, 33, 40, 44, 33, 21, 35, 43, 57, 51, 29, 51, 35, 18, 58, 57, 19, 37, 38, 45, 27, 64, 38, 30, 46, 18, 60, 30, 47, 39, 42, 27, 44, 33, 54, 58, 45, 59, 29, 27, 44, 44, 33, 27, 33, 55, 32, 31, 57, 30, 78, 28, 30, 29, 66, 60, 31, 26, 24, 29, 44, 34, 59, 23, 23, 55, 39, 39, 57, 35, 39, 47, 53, 23, 35, 44, 28, 26, 23, 32, 49, 36, 36, 41, 21, 54, 43, 35, 31, 46, 46, 56, 45, 21, 51, 47, 42, 19, 25, 37, 35, 40, 57, 38, 48, 34, 46, 30, 45, 32, 35, 62, 56, 50, 25, 41, 35, 33, 38, 37, 56, 19, 31, 53, 33, 72, 54, 30, 51, 30, 30, 32, 30, 35, 40, 38, 19, 32, 49, 76, 60, 60, 55, 41, 24, 53, 69, 71, 41, 32, 45, 59, 31, 40, 44, 63, 31, 51, 51, 23, 49, 56, 39, 39, 30, 43, 31, 44, 43, 37, 41, 43, 47, 38, 46, 19, 19, 24, 39, 37, 28, 40, 27, 22, 21, 32, 20, 45, 46, 46, 26, 47, 22, 34, 49, 20, 28, 30, 23, 20, 31, 46, 42, 60, 30, 27, 20, 56, 41, 61, 47, 60, 33, 40, 48, 56, 50, 57, 20, 46, 21, 37, 42, 20, 54, 32, 62, 50, 49, 48, 24, 46, 42, 52, 58, 37, 34, 26, 18, 33, 41, 21, 23, 26, 33, 24, 43, 52, 27, 61, 35, 35, 45, 43, 51, 35, 47, 22, 49, 20, 19, 27, 42, 27, 23, 20, 47, 17, 76, 39, 25, 31, 44, 53, 34, 45, 34, 35, 36, 45, 37, 53, 35, 38, 65, 49, 24, 54, 17, 19, 60, 46, 78, 26, 58, 37, 41, 30, 19, 48, 22, 50, 25, 18, 21, 47, 33, 39, 67, 19, 18, 36, 23, 25, 40, 73, 35, 31, 20, 20, 27, 66, 66, 64, 48, 24, 51, 25, 33, 50, 51, 34, 41, 30, 59, 41, 36, 37, 28, 19, 28, 62, 36, 58, 42, 43, 29, 32, 64, 39, 26, 19, 21, 32, 30, 26, 20, 19, 33, 23, 29, 42, 30, 63, 34, 60, 48, 25, 27, 55, 32, 22, 25, 18, 24, 29, 73, 39, 33, 57, 22, 33, 33, 42, 43, 57, 42, 22, 21, 42, 39, 41, 20, 47, 44, 29, 52, 71, 52, 36, 53, 37, 26, 34, 35, 20, 34, 56, 39, 24, 24, 23, 27, 25, 29, 42, 51, 31, 59, 29, 18, 24, 38, 36, 34, 42, 45, 21, 64, 32, 45, 53, 42, 32, 49, 62, 28, 29, 32, 37, 38, 44, 19, 22, 24, 58, 56, 37, 43, 32, 34, 44, 28, 35, 38, 26, 20, 24, 40, 23, 79, 44, 51, 30, 52, 41, 33, 50, 51, 41, 35, 34, 28, 31, 22, 67, 65, 39, 23, 29, 57, 47, 51, 34, 52, 29, 29, 25, 21, 38, 55, 61, 34, 41, 27, 78, 40, 26, 43, 55, 25, 26, 43, 25, 22, 52, 61, 42, 21, 44, 65, 56, 33, 22, 43, 45, 42, 37, 56, 35, 40, 31, 26, 26, 42, 45, 47, 22, 33, 37, 48, 57, 47, 58, 55, 52, 29, 42, 23, 23, 46, 30, 43, 25, 32, 56, 33, 19, 20, 34, 34, 55, 55, 31, 30, 31, 51, 59, 58, 40, 54, 25, 18, 41, 57, 37, 46, 39, 19, 41, 54, 39, 41, 26, 57, 33, 56, 60, 22, 58, 48, 45, 18, 56, 48, 40, 38, 58, 23, 63, 51, 22, 44, 46, 64, 32, 31, 36, 21, 30, 31, 54, 59, 28, 57, 44, 53, 58, 19, 39, 31, 27, 46, 37, 27, 33, 20, 55, 39, 41, 30, 39, 28, 25, 31, 51, 24, 70, 23, 61, 45, 47, 56, 79, 20, 30, 27, 40, 60, 58, 71, 50, 29, 33, 60, 33, 43, 33, 33, 32, 21, 38, 38, 39, 17, 21, 42, 42, 36, 32, 37, 28, 37, 50, 49, 40, 40, 49, 35, 33, 24, 41, 43, 36, 84, 23, 41, 55, 30, 46, 55, 34, 54, 27, 52, 36, 25, 52, 24, 36, 31, 66, 90, 67, 22, 47, 32, 55, 32, 48, 21, 33, 27, 47, 18, 44, 23, 18, 56, 24, 43, 23, 26, 42, 23, 53, 51, 40, 27, 24, 42, 41, 23, 42, 38, 63, 31, 18, 21, 24, 55, 20, 31, 23, 35, 51, 19, 38, 52, 70, 53, 59, 39, 18, 41, 19, 24, 25, 32, 34, 35, 50, 18, 37, 30, 40, 22, 50, 70, 43, 77, 30, 33, 51, 58, 22, 33, 27, 35, 32, 41, 37, 17, 50, 31, 54, 23, 40, 29, 38, 30, 20, 43, 25, 39, 48, 41, 24, 20, 25, 57, 39, 66, 34, 36, 46, 42, 46, 34, 43, 42, 31, 70, 32, 46, 42, 21, 24, 23, 50, 37, 19, 44, 29, 30, 36, 49, 29, 45, 75, 42, 37, 51, 61, 33, 50, 59, 25, 22, 56, 44, 35, 57, 47, 27, 50, 30, 46, 48, 61, 33, 40, 58, 46, 35, 51, 37, 64, 41, 18, 44, 41, 33, 37, 34, 24, 20, 38, 28, 20, 22, 55, 20, 26, 20, 40, 33, 54, 24, 25, 18, 49, 17, 34, 37, 49, 74, 37, 20, 40, 34, 30, 44, 40, 58, 44, 42, 39, 19, 41, 29, 34, 38, 27, 54, 40, 19, 17, 33, 80, 24, 28, 55, 21, 51, 19, 42, 60, 20, 51, 21, 37, 26, 42, 40, 55, 33, 62, 47, 26, 59, 70, 26, 47, 25, 52, 46, 67, 34, 53, 22, 39, 54, 24, 37, 38, 41, 36, 43, 39, 26, 49, 37, 43, 54, 53, 57, 28, 29, 21, 51, 46, 50, 50, 36, 31, 20, 36, 38, 37, 39, 18, 33, 35, 23, 37, 25, 51, 35, 54, 66, 32, 37, 29, 45, 61, 40, 24, 49, 51, 34, 28, 20, 30, 49, 26, 63, 36, 63, 22, 40, 31, 49, 35, 23, 25, 34, 28, 44, 33, 21, 55, 17, 75, 61, 53, 26, 48, 28, 30, 29, 25, 37, 43, 40, 34, 55, 31, 21, 32, 26, 26, 44, 49, 21, 26, 40, 19, 58, 61, 28, 26, 64, 42, 41, 45, 39, 41, 51, 34, 33, 23, 35, 41, 39, 41, 33, 22, 25, 25, 77, 33, 66, 19, 44, 45, 28, 19, 43, 39, 33, 44, 62, 40, 41, 23, 48, 38, 37, 45, 29, 66, 19, 23, 34, 39, 33, 43, 27, 33, 26, 56, 46, 38, 90, 72, 20, 29, 49, 23, 20, 57, 46, 33, 24, 49, 54, 46, 31, 43, 18, 19, 46, 43, 58, 43, 37, 21, 52, 34, 28, 34, 29, 39, 49, 35, 31, 18, 47, 46, 43, 51, 21, 18, 31, 39, 58, 36, 38, 31, 19, 38, 26, 38, 36, 40, 25, 41, 33, 63, 51, 34, 33, 25, 30, 19, 47, 26, 21, 36, 60, 28, 19, 42, 36, 35, 22, 54, 36, 55, 57, 56, 24, 35, 51, 34, 36, 51, 46, 42, 25, 22, 37, 48, 61, 40, 58, 52, 18, 27, 48, 63, 38, 31, 40, 41, 28, 31, 32, 30, 39, 28, 31, 21, 33, 44, 22, 42, 36, 39, 61, 31, 21, 39, 34, 21, 75, 21, 81, 54, 34, 54, 41, 61, 63, 24, 20, 44, 47, 31, 41, 53, 63, 44, 37, 28, 71, 57, 22, 33, 30, 20, 28, 54, 20, 34, 21, 29, 51, 39, 19, 22, 59, 18, 22, 42, 39, 31, 28, 22, 67, 17, 47, 23, 20, 22, 47, 49, 64, 35, 52, 29, 73, 38, 59, 35, 18, 24, 33, 40, 55, 44, 40, 22, 55, 45, 47, 56, 44, 32, 17, 44, 32, 35, 27, 32, 68, 20, 33, 24, 48, 45, 48, 28, 60, 39, 23, 23, 36, 25, 52, 35, 41, 45, 37, 58, 67, 48, 63, 24, 43, 36, 32, 36, 43, 27, 17, 45, 41, 41, 18, 48, 51, 26, 54, 22, 19, 47, 32, 46, 30, 35, 52, 73, 17, 32, 43, 19, 26, 41, 49, 52, 35, 57, 35, 33, 56, 42, 17, 49, 39, 34, 37, 22, 27, 21, 59, 44, 53, 26, 48, 39, 34, 54, 36, 60, 50, 48, 35, 40, 30, 65, 31, 46, 67, 62, 38, 24, 24, 27, 43, 48, 41, 30, 38, 45, 45, 55, 46, 24, 31, 35, 37, 29, 56, 60, 25, 33, 44, 21, 21, 46, 44, 63, 44, 31, 26, 37, 60, 33, 38, 26, 46, 24, 47, 27, 46, 47, 47, 69, 32, 51, 36, 50, 29, 32, 27, 60, 31, 46, 40, 39, 69, 54, 56, 47, 25, 34, 24, 63, 29, 64, 44, 26, 36, 55, 37, 37, 63, 33, 19, 57, 31, 20, 58, 20, 25, 40, 18, 26, 29, 45, 35, 32, 61, 26, 56, 22, 49, 40, 37, 23, 28, 27, 34, 49, 41, 28, 38, 47, 28, 44, 41, 50, 73, 21, 38, 64, 47, 52, 24, 44, 27, 61, 58, 51, 49, 23, 19, 21, 56, 35, 37, 41, 30, 35, 47, 59, 55, 22, 31, 30, 24, 49, 23, 50, 38, 20, 49, 45, 46, 19, 31, 38, 56, 24, 58, 28, 22, 46, 30, 56, 32, 56, 50, 28, 30, 24, 42, 31, 18, 46, 31, 38, 44, 55, 19, 45, 50, 53, 41, 45, 21, 37, 42, 37, 46, 45, 32, 30, 57, 37, 18, 32, 43, 46, 36, 33, 48, 34, 34, 33, 36, 23, 39, 22, 19, 47, 46, 47, 47, 52, 38, 58, 32, 54, 20, 25, 25, 36, 69, 28, 20, 21, 37, 21, 25, 39, 34, 33, 47, 29, 48, 42, 29, 34, 20, 41, 45, 22, 23, 19, 24, 20, 32, 52, 36, 18, 27, 26, 52, 24, 42, 39, 20, 20, 39, 68, 35, 54, 24, 24, 20, 64, 28, 36, 44, 38, 22, 56, 24, 24, 22, 21, 42, 41, 19, 67, 20, 32, 46, 52, 44, 65, 37, 27, 35, 30, 64, 48, 35, 28, 63, 40, 33, 40, 25, 45, 30, 53, 63, 27, 31, 33, 39, 42, 19, 31, 53, 47, 27, 22, 29, 52, 32, 23, 27, 53, 18, 41, 27, 30, 41, 21, 58, 45, 40, 21, 29, 50, 26, 23, 27, 36, 22, 37, 54, 36, 44, 38, 49, 18, 26, 48, 32, 35, 40, 40, 42, 30, 19, 36, 31, 50, 38, 35, 45, 29, 41, 33, 49, 28, 49, 27, 47, 42, 30, 37, 37, 51, 27, 53, 45, 37, 40, 54, 47, 35, 33, 28, 39, 25, 33, 34, 53, 21, 31, 63, 21, 59, 50, 66, 19, 31, 23, 45, 35, 32, 20, 38, 49, 64, 60, 25, 21, 26, 23, 67, 24, 46, 38, 20, 28, 26, 58, 31, 37, 53, 45, 33, 33, 35, 60, 21, 38, 40, 40, 44, 27, 34, 20, 49, 37, 52, 46, 20, 43, 46, 25, 48, 36, 37, 18, 42, 60, 37, 44, 32, 29, 42, 17, 41, 34, 56, 28, 28, 19, 49, 36, 25, 42, 47, 26, 35, 37, 39, 42, 41, 23, 31, 36, 20, 36, 50, 37, 25, 39, 32, 59, 29, 50, 42, 32, 48, 20, 50, 18, 60, 60, 42, 49, 35, 30, 59, 24, 55, 19, 39, 48, 44, 21, 23, 22, 55, 19, 23, 46, 23, 47, 23, 42, 22, 18, 37, 34, 30, 62, 51, 34, 28, 56, 46, 46, 23, 33, 27, 21, 18, 36, 37, 39, 36, 24, 26, 37, 34, 33, 33, 20, 26, 47, 17, 53, 31, 34, 23, 39, 22, 21, 40, 54, 32, 29, 68, 27, 54, 23, 38, 54, 21, 40, 19, 21, 35, 39, 38, 41, 26, 35, 70, 45, 83, 59, 18, 31, 43, 24, 33, 48, 27, 41, 25, 53, 66, 31, 30, 22, 47, 74, 37, 35, 51, 40, 51, 26, 53, 32, 29, 42, 22, 62, 33, 21, 46, 25, 29, 29, 33, 50, 22, 40, 28, 54, 29, 49, 46, 71, 27, 56, 64, 23, 36, 39, 26, 31, 36, 43, 25, 27, 24, 50, 30, 24, 41, 29, 31, 54, 20, 37, 28, 39, 46, 35, 36, 34, 44, 34, 28, 38, 64, 19, 35, 61, 45, 36, 34, 27, 22, 26, 19, 36, 51, 63, 51, 20, 42, 40, 25, 40, 41, 49, 46, 32, 28, 19, 19, 18, 39, 60, 46, 38, 36, 45, 60, 30, 22, 23, 48, 46, 43, 25, 50, 61, 35, 65, 56, 43, 55, 22, 40, 35, 22, 20, 46, 43, 46, 39, 34, 35, 30, 25, 44, 35, 41, 43, 32, 50, 20, 29, 38, 20, 41, 27, 37, 62, 31, 49, 33, 31, 19, 44, 31, 62, 65, 17, 60, 31, 29, 53, 44, 43, 40, 18, 47, 38, 37, 40, 57, 43, 32, 64, 18, 40, 17, 23, 45, 45, 27, 23, 58, 35, 47, 33, 27, 46, 24, 40, 44, 31, 37, 23, 30, 31, 24, 32, 27, 35, 27, 30, 33, 34, 29, 26, 39, 37, 51, 40, 53, 58, 29, 33, 57, 69, 20, 56, 49, 34, 70, 44, 37, 58, 42, 41, 28, 47, 24, 58, 18, 50, 69, 84, 47, 19, 24, 48, 53, 58, 22, 36, 34, 43, 37, 43, 42, 21, 49, 35, 49, 28, 51, 30, 19, 56, 20, 22, 33, 54, 22, 68, 56, 23, 69, 41, 33, 18, 17, 31, 50, 62, 41, 20, 23, 18, 53, 42, 23, 43, 47, 53, 28, 28, 44, 58, 25, 26, 36, 27, 39, 25, 17, 39, 52, 24, 26, 24, 32, 49, 20, 40, 52, 33, 19, 37, 21, 33, 28, 48, 43, 32, 28, 47, 37, 26, 55, 40, 31, 39, 54, 38, 53, 27, 42, 62, 47, 26, 33, 59, 42, 25, 58, 30, 21, 61, 33, 51, 70, 55, 29, 46, 35, 31, 37, 43, 23, 41, 42, 26, 45, 25, 33, 38, 28, 68, 42, 35, 36, 33, 22, 45, 24, 57, 36, 41, 26, 25, 53, 28, 30, 38, 37, 30, 41, 29, 31, 36, 59, 33, 29, 49, 50, 33, 28, 81, 24, 32, 47, 44, 24, 37, 51, 52, 22, 28, 47, 43, 25, 43, 25, 59, 29, 30, 24, 38, 37, 26, 30, 73, 30, 33, 46, 40, 18, 35, 22, 50, 62, 39, 22, 33, 31, 29, 38, 24, 29, 49, 33, 28, 22, 37, 30, 49, 29, 25, 36, 36, 21, 18, 30, 44, 40, 43, 45, 29, 40, 43, 20, 32, 19, 21, 23, 24, 52, 51, 19, 54, 50, 29, 17, 47, 46, 30, 58, 48, 23, 23, 45, 20, 26, 50, 49, 26, 47, 31, 19, 23, 35, 46, 28, 58, 21, 50, 30, 31, 31, 35, 33, 34, 67, 37, 48, 34, 24, 55, 50, 40, 33, 51, 30, 41, 47, 27, 37, 40, 36, 37, 38, 40, 35, 47, 20, 33, 45, 66, 35, 32, 23, 19, 44, 20, 44, 41, 19, 48, 45, 40, 34, 20, 33, 30, 23, 34, 48, 32, 42, 22, 60, 42, 45, 47, 29, 34, 34, 39, 29, 42, 48, 53, 51, 44, 46, 51, 32, 22, 24, 33, 47, 64, 62, 52, 39, 42, 38, 55, 29, 42, 30, 32, 36, 50, 27, 47, 33, 48, 23, 31, 26, 61, 50, 49, 57, 19, 18, 44, 54, 35, 27, 50, 43, 28, 47, 21, 30, 19, 40, 55, 67, 31, 30, 54, 51, 34, 59, 50, 29, 17, 24, 52, 39, 18, 32, 25, 34, 44, 37, 39, 32, 23, 45, 30, 20, 41, 21, 48, 31, 34, 25, 41, 35, 42, 34, 44, 32, 25, 23, 36, 53, 27, 27, 57, 17, 59, 34, 30, 31, 18, 23, 19, 20, 52, 39, 27, 19, 38, 19, 34, 41, 23, 24, 24, 22, 45, 45, 48, 42, 35, 32, 51, 59, 31, 27, 29, 57, 40, 31, 23, 42, 60, 53, 20, 27, 31, 29, 22, 42, 63, 28, 79, 39, 29, 60, 58, 20, 44, 47, 59, 37, 21, 68, 43, 23, 23, 40, 45, 34, 22, 38, 24, 23, 39, 26, 24, 22, 36, 20, 18, 38, 43, 56, 42, 35, 31, 73, 32, 37, 66, 22, 51, 31, 65, 39, 48, 20, 28, 48, 29, 36, 35, 31, 48, 36, 40, 57, 26, 37, 65, 26, 42, 47, 62, 27, 76, 20, 29, 29, 32, 22, 30, 43, 23, 44, 41, 49, 30, 23, 39, 32, 50, 19, 35, 23, 25, 40, 30, 52, 49, 24, 46, 39, 45, 62, 43, 45, 62, 48, 56, 28, 23, 43, 26, 36, 55, 48, 43, 38, 18, 56, 39, 46, 36, 42, 50, 17, 57, 73, 50, 35, 27, 24, 27, 23, 25, 51, 26, 47, 40, 58, 25, 29, 36, 46, 69, 69, 28, 20, 54, 25, 64, 17, 53, 50, 57, 51, 20, 47, 25, 50, 20, 43, 46, 54, 53, 21, 58, 30, 61, 56, 59, 72, 33, 46, 46, 48, 33, 21, 27, 31, 28, 45, 51, 46, 25, 61, 17, 26, 35, 33, 37, 39, 42, 35, 45, 47, 64, 39, 43, 34, 62, 53, 31, 26, 56, 43, 33, 45, 46, 26, 24, 43, 40, 61, 34, 85, 41, 31, 22, 60, 29, 34, 35, 24, 45, 21, 30, 50, 41, 21, 45, 34, 28, 19, 26, 48, 69, 41, 43, 28, 65, 31, 28, 51, 45, 43, 31, 31, 45, 20, 25, 36, 31, 56, 26, 47, 40, 32, 34, 37, 38, 19, 49, 18, 26, 82, 40, 28, 23, 19, 67, 43, 41, 37, 20, 40, 47, 40, 39, 24, 78, 67, 49, 66, 52, 59, 72, 35, 30, 36, 26, 23, 39, 40, 29, 38, 63, 58, 30, 38, 25, 44, 43, 21, 41, 38, 75, 29, 52, 47, 28, 24, 30, 30, 40, 27, 26, 32, 36, 26, 25, 39, 27, 24, 38, 35, 29, 49, 36, 47, 21, 35, 38, 66, 45, 19, 18, 37, 49, 33, 27, 34, 54, 24, 44, 48, 23, 29, 21, 18, 18, 29, 29, 41, 32, 61, 79, 34, 46, 50, 29, 23, 64, 70, 28, 51, 20, 34, 29, 49, 77, 32, 27, 22, 35, 42, 35, 62, 33, 45, 19, 38, 26, 29, 33, 35, 42, 20, 43, 69, 39, 37, 28, 26, 32, 43, 49, 28, 35, 19, 35, 57, 45, 23, 45, 42, 35, 45, 28, 64, 44, 59, 41, 21, 29, 51, 22, 49, 53, 38, 23, 20, 40, 25, 27, 20, 37, 40, 28, 38, 42, 32, 20, 62, 31, 31, 24, 62, 43, 21, 30, 22, 18, 59, 24, 49, 19, 48, 32, 33, 23, 33, 50, 30, 32, 39, 21, 23, 32, 57, 22, 70, 34, 53, 21, 37, 60, 23, 32, 57, 65, 25, 30, 31, 19, 23, 44, 43, 42, 34, 32, 25, 21, 31, 36, 49, 44, 42, 30, 33, 36, 17, 20, 32, 33, 37, 60, 35, 43, 46, 29, 36, 27, 50, 22, 33, 61, 28, 41, 22, 27, 25, 32, 28, 25, 28, 37, 58, 39, 33, 59, 58, 38, 34, 31, 48, 65, 45, 80, 23, 33, 38, 44, 48, 40, 19, 45, 34, 70, 32, 33, 24, 30, 22, 52, 67, 40, 55, 29, 62, 22, 64, 29, 28, 25, 47, 26, 18, 52, 48, 36, 23, 41, 56, 53, 26, 22, 25, 31, 36, 46, 46, 32, 43, 30, 41, 50, 37, 22, 51, 47, 40, 46, 23, 30, 27, 31, 31, 30, 50, 27, 22, 29, 28, 54, 47, 34, 26, 53, 23, 42, 36, 52, 22, 27, 42, 39, 49, 36, 20, 55, 47, 33, 36, 39, 55, 21, 48, 30, 24, 57, 20, 18, 56, 36, 44, 57, 57, 33, 61, 39, 28, 31, 60, 36, 58, 50, 23, 30, 20, 90, 49, 47, 34, 37, 25, 37, 47, 34, 34, 35, 39, 32, 24, 28, 41, 69, 32, 58, 47, 41, 35, 24, 46, 69, 22, 36, 31, 23, 35, 37, 35, 18, 43, 53, 54, 29, 46, 24, 30, 23, 27, 36, 38, 33, 66, 45, 68, 37, 17, 31, 46, 40, 34, 56, 42, 46, 30, 33, 47, 57, 34, 40, 33, 22, 29, 53, 36, 45, 36, 18, 27, 30, 34, 36, 36, 48, 41, 21, 47, 41, 61, 64, 35, 44, 20, 23, 52, 43, 32, 63, 29, 42, 65, 24, 42, 35, 40, 38, 26, 46, 39, 29, 42, 36, 26, 57, 55, 35, 41, 24, 47, 34, 44, 18, 27, 36, 21, 29, 25, 73, 45, 28, 43, 47, 20, 22, 76, 24, 54, 30, 33, 30, 40, 27, 49, 30, 22, 19, 46, 51, 56, 53, 60, 27, 46, 37, 41, 65, 30, 57, 44, 32, 27, 57, 59, 32, 90, 22, 39, 43, 38, 61, 24, 25, 44, 55, 90, 30, 44, 36, 63, 60, 34, 49, 47, 42, 53, 23, 28, 20, 43, 32, 43, 31, 29, 24, 39, 29, 20, 23, 42, 53, 20, 58, 51, 25, 38, 41, 58, 25, 50, 32, 31, 22, 37, 32, 46, 62, 40, 56, 37, 41, 56, 20, 34, 45, 30, 51, 27, 38, 39, 57, 47, 77, 43, 46, 41, 20, 30, 30, 22, 25, 18, 28, 50, 28, 21, 37, 28, 23, 32, 27, 55, 44, 25, 29, 50, 18, 74, 22, 27, 27, 41, 24, 23, 34, 46, 32, 41, 29, 48, 51, 34, 58, 52, 37, 53, 54, 36, 25, 20, 23, 39, 20, 28, 73, 67, 56, 32, 42, 33, 37, 33, 31, 59, 21, 58, 24, 26, 26, 33, 25, 21, 24, 39, 68, 66, 38, 41, 71, 37, 54, 27, 49, 19, 25, 47, 40, 38, 25, 25, 23, 28, 29, 32, 34, 52, 33, 19, 68, 41, 18, 29, 40, 51, 49, 20, 38, 57, 35, 66, 27, 25, 77, 19, 70, 59, 25, 44, 37, 75, 21, 21, 47, 20, 32, 28, 53, 19, 58, 22, 23, 21, 36, 39, 34, 41, 72, 34, 51, 32, 38, 37, 21, 21, 26, 34, 18, 27, 20, 36, 31, 32, 54, 34, 46, 33, 32, 42, 30, 37, 28, 25, 44, 40, 36, 21, 39, 24, 46, 47, 47, 35, 60, 35, 33, 21, 37, 48, 58, 21, 22, 22, 28, 33, 23, 58, 24, 38, 49, 47, 50, 21, 55, 47, 19, 47, 36, 28, 42, 41, 46, 32, 34, 33, 60, 53, 24, 39, 23, 50, 36, 19, 26, 38, 41, 22, 38, 72, 39, 35, 38, 19, 26, 52, 41, 45, 58, 34, 36, 19, 26, 60, 35, 39, 27, 26, 17, 59, 53, 35, 34, 36, 36, 21, 29, 35, 59, 22, 50, 24, 36, 31, 42, 38, 52, 46, 23, 54, 27, 42, 33, 19, 59, 54, 48, 19, 45, 36, 35, 47, 33, 48, 42, 19, 64, 51, 20, 28, 52, 29, 51, 36, 43, 52, 50, 25, 36, 44, 47, 32, 37, 40, 34, 35, 26, 49, 63, 46, 31, 22, 20, 52, 26, 34, 31, 46, 22, 38, 35, 27, 27, 46, 40, 19, 69, 20, 34, 49, 27, 59, 45, 21, 51, 64, 41, 18, 49, 51, 20, 69, 18, 43, 50, 32, 27, 38, 62, 46, 19, 17, 21, 36, 36, 23, 58, 40, 64, 66, 42, 55, 53, 25, 39, 20, 32, 28, 23, 18, 43, 29, 32, 31, 60, 42, 46, 38, 18, 31, 54, 46, 27, 50, 38, 36, 25, 19, 60, 45, 21, 42, 38, 22, 19, 23, 24, 30, 62, 26, 42, 33, 54, 35, 33, 45, 62, 24, 50, 54, 58, 39, 32, 27, 27, 20, 64, 55, 46, 45, 26, 36, 25, 58, 39, 47, 35, 38, 50, 36, 28, 44, 40, 55, 29, 33, 19, 32, 45, 23, 31, 22, 59, 32, 49, 22, 26, 23, 55, 47, 44, 27, 40, 32, 20, 84, 40, 37, 44, 47, 62, 19, 47, 44, 26, 36, 52, 33, 48, 33, 25, 62, 38, 46, 18, 46, 38, 52, 63, 49, 17, 33, 70, 19, 27, 61, 33, 46, 29, 40, 43, 24, 29, 61, 48, 19, 30, 24, 39, 46, 40, 27, 25, 31, 58, 33, 40, 40, 31, 33, 49, 24, 46, 58, 20, 65, 43, 48, 41, 27, 52, 26, 52, 52, 19, 39, 47, 27, 48, 50, 60, 28, 47, 69, 40, 37, 41, 38, 46, 21, 44, 38, 47, 42, 42, 56, 32, 24, 26, 36, 49, 65, 73, 34, 22, 49, 25, 39, 19, 24, 34, 35, 21, 43, 35, 39, 48, 33, 19, 17, 43, 62, 53, 45, 47, 30, 53, 24, 41, 58, 27, 63, 50, 43, 29, 41, 55, 36, 23, 22, 58, 25, 40, 38, 37, 19, 55, 55, 28, 53, 32, 60, 32, 34, 49, 21, 24, 53, 29, 34, 52, 41, 21, 43, 31, 38, 42, 62, 51, 30, 24, 36, 57, 46, 33, 42, 62, 64, 18, 43, 62, 29, 43, 39, 69, 34, 44, 25, 47, 17, 41, 19, 57, 25, 37, 29, 32, 35, 52, 41, 29, 23, 31, 40, 19, 21, 37, 49, 64, 37, 70, 22, 29, 37, 29, 38, 27, 58, 60, 20, 51, 37, 22, 56, 26, 19, 37, 27, 27, 52, 45, 28, 20, 28, 44, 30, 26, 55, 46, 51, 25, 43, 19, 36, 36, 48, 30, 24, 30, 24, 62, 41, 49, 36, 47, 49, 52, 52, 26, 51, 18, 30, 46, 28, 48, 62, 49, 69, 35, 78, 61, 24, 29, 36, 17, 27, 64, 33, 32, 19, 42, 24, 23, 51, 31, 44, 38, 44, 44, 38, 36, 29, 46, 47, 30, 37, 27, 43, 44, 31, 34, 39, 36, 57, 78, 24, 39, 21, 34, 22, 36, 61, 67, 42, 49, 58, 37, 39, 25, 45, 30, 49, 36, 23, 51, 55, 35, 45, 36, 32, 39, 47, 49, 52, 44, 36, 40, 17, 18, 30, 66, 59, 30, 50, 24, 28, 44, 25, 45, 20, 67, 40, 52, 34, 37, 17, 25, 35, 63, 38, 33, 51, 64, 50, 18, 67, 46, 60, 29, 30, 56, 29, 37, 55, 24, 21, 46, 25, 40, 23, 43, 29, 59, 18, 28, 17, 53, 22, 63, 32, 40, 61, 35, 43, 20, 48, 56, 51, 34, 44, 61, 37, 36, 34, 23, 63, 63, 47, 55, 31, 27, 58, 28, 52, 31, 53, 43, 43, 50, 68, 37, 26, 22, 22, 59, 30, 24, 30, 50, 38, 29, 76, 21, 65, 43, 29, 32, 37, 64, 49, 44, 68, 34, 35, 53, 24, 31, 25, 36, 29, 37, 42, 30, 32, 47, 44, 19, 22, 51, 69, 73, 18, 33, 22, 33, 20, 28, 25, 27, 22, 67, 31, 36, 43, 33, 37, 27, 36, 33, 63, 48, 25, 41, 26, 23, 33, 30, 67, 32, 48, 51, 39, 40, 40, 19, 28, 47, 20, 41, 29, 40, 61, 25, 26, 43, 17, 43, 73, 46, 19, 37, 26, 47, 47, 29, 29, 33, 20, 28, 26, 41, 67, 43, 35, 18, 55, 44, 64, 34, 38, 25, 21, 37, 39, 34, 23, 54, 49, 25, 51, 29, 27, 53, 68, 56, 30, 24, 70, 24, 74, 23, 41, 45, 35, 21, 45, 18, 49, 49, 45, 27, 28, 35, 33, 35, 19, 33, 28, 49, 61, 39, 22, 42, 22, 18, 49, 33, 28, 42, 20, 50, 32, 31, 29, 46, 32, 61, 43, 35, 39, 18, 42, 23, 32, 52, 36, 38, 50, 23, 71, 27, 56, 28, 45, 28, 28, 18, 62, 35, 39, 24, 58, 48, 57, 31, 25, 34, 29, 38, 21, 34, 50, 50, 49, 30, 29, 22, 42, 19, 31, 42, 20, 51, 50, 27, 26, 61, 20, 51, 52, 33, 31, 24, 56, 19, 29, 18, 43, 43, 21, 51, 20, 41, 45, 20, 28, 50, 47, 80, 47, 19, 45, 17, 37, 27, 17, 18, 57, 26, 26, 36, 51, 38, 77, 51, 24, 35, 35, 51, 28, 37, 43, 23, 42, 58, 52, 29, 27, 28, 59, 28, 24, 20, 54, 65, 18, 38, 29, 27, 36, 55, 22, 28, 20, 24, 36, 68, 49, 18, 50, 37, 60, 43, 43, 17, 43, 39, 21, 42, 48, 56, 21, 28, 39, 40, 38, 59, 51, 19, 23, 31, 42, 43, 45, 34, 21, 34, 55, 30, 29, 19, 45, 42, 32, 90, 44, 34, 35, 39, 29, 24, 49, 52, 54, 31, 44, 18, 27, 24, 30, 27, 34, 25, 39, 28, 51, 21, 33, 27, 29, 33, 37, 20, 24, 29, 31, 41, 42, 20, 54, 55, 21, 52, 19, 33, 22, 59, 17, 31, 74, 45, 36, 35, 45, 42, 50, 39, 44, 52, 24, 56, 27, 47, 40, 38, 27, 52, 22, 41, 30, 55, 25, 32, 28, 33, 17, 27, 55, 63, 39, 51, 67, 41, 27, 54, 59, 18, 19, 31, 17, 42, 37, 51, 48, 23, 27, 25, 69, 30, 33, 28, 45, 25, 37, 59, 26, 48, 64, 39, 24, 40, 46, 35, 43, 31, 22, 35, 35, 35, 40, 50, 48, 70, 46, 36, 37, 24, 47, 35, 43, 18, 54, 62, 36, 27, 23, 61, 39, 33, 28, 22, 31, 26, 44, 73, 44, 35, 33, 27, 47, 29, 33, 50, 32, 41, 19, 45, 65, 42, 33, 32, 42, 49, 38, 38, 37, 24, 27, 25, 29, 46, 44, 26, 46, 38, 38, 22, 33, 50, 31, 26, 42, 27, 34, 64, 43, 39, 47, 56, 25, 60, 52, 43, 46, 19, 56, 22, 28, 39, 28, 31, 22, 39, 39, 28, 67, 31, 50, 34, 48, 51, 54, 44, 20, 37, 38, 59, 59, 51, 60, 29, 37, 42, 28, 34, 50, 22, 27, 45, 39, 25, 63, 24, 18, 19, 23, 20, 17, 63, 28, 51, 29, 64, 17, 45, 18, 31, 48, 41, 65, 27, 39, 51, 49, 58, 36, 25, 41, 31, 63, 50, 63, 44, 48, 51, 18, 23, 31, 38, 19, 56, 31, 46, 39, 56, 31, 42, 53, 18, 18, 20, 27, 31, 52, 46, 30, 26, 48, 37, 29, 49, 24, 74, 56, 39, 21, 29, 43, 23, 26, 47, 27, 45, 30, 35, 33, 37, 21, 42, 48, 46, 36, 54, 25, 26, 36, 56, 37, 26, 23, 23, 41, 55, 39, 67, 60, 54, 38, 47, 49, 19, 20, 66, 22, 51, 19, 36, 33, 41, 90, 78, 36, 30, 35, 46, 38, 24, 40, 49, 19, 77, 46, 41, 28, 21, 24, 36, 23, 38, 29, 38, 47, 39, 40, 21, 32, 23, 44, 37, 70, 51, 66, 57, 36, 48, 24, 51, 43, 27, 32, 20, 33, 19, 32, 35, 26, 39, 24, 45, 20, 49, 35, 30, 23, 33, 35, 31, 20, 44, 33, 50, 25, 28, 60, 38, 38, 57, 50, 39, 69, 51, 42, 53, 37, 37, 38, 20, 38, 52, 48, 30, 37, 47, 33, 33, 20, 41, 32, 54, 36, 46, 45, 18, 62, 33, 42, 18, 32, 43, 20, 50, 42, 36, 22, 30, 33, 43, 38, 34, 40, 40, 41, 57, 65, 52, 59, 18, 48, 45, 24, 27, 40, 44, 24, 71, 17, 57, 71, 37, 33, 41, 25, 42, 18, 34, 57, 27, 46, 64, 24, 23, 33, 58, 25, 44, 53, 38, 36, 25, 54, 31, 18, 40, 26, 68, 33, 22, 50, 24, 44, 37, 34, 45, 77, 25, 20, 46, 26, 45, 70, 57, 28, 35, 36, 32, 29, 36, 19, 27, 48, 49, 55, 61, 42, 42, 30, 32, 37, 35, 30, 30, 55, 36, 33, 56, 26, 37, 28, 38, 76, 35, 48, 35, 39, 64, 22, 45, 27, 43, 23, 25, 39, 63, 37, 26, 33, 45, 28, 20, 34, 34, 41, 38, 36, 38, 46, 43, 49, 26, 28, 54, 53, 24, 23, 55, 58, 26, 19, 28, 45, 30, 76, 47, 34, 40, 24, 30, 67, 24, 43, 29, 40, 61, 45, 21, 58, 52, 34, 20, 25, 46, 54, 43, 25, 21, 65, 37, 29, 44, 37, 38, 35, 32, 44, 39, 23, 25, 23, 47, 47, 25, 35, 68, 50, 47, 35, 24, 21, 35, 30, 17, 49, 27, 22, 22, 50, 51, 27, 27, 23, 20, 42, 32, 45, 35, 22, 34, 28, 38, 48, 22, 34, 25, 23, 38, 43, 49, 23, 57, 24, 44, 34, 29, 29, 35, 48, 38, 42, 53, 29, 59, 31, 34, 51, 25, 28, 30, 46, 45, 23, 33, 44, 25, 48, 23, 33, 20, 23, 71, 22, 36, 27, 40, 30, 30, 30, 35, 39, 39, 74, 32, 25, 64, 47, 37, 47, 33, 40, 22, 56, 37, 30, 20, 53, 43, 23, 43, 30, 20, 22, 38, 54, 23, 46, 59, 28, 18, 41, 42, 58, 24, 39, 28, 25, 29, 53, 63, 26, 21, 45, 29, 72, 51, 21, 38, 38, 46, 31, 68, 43, 21, 44, 56, 20, 36, 21, 21, 52, 56, 52, 51, 29, 36, 29, 21, 58, 37, 50, 30, 29, 48, 50, 39, 35, 54, 24, 20, 30, 19, 26, 38, 25, 33, 60, 30, 30, 19, 23, 27, 20, 39, 33, 65, 34, 20, 23, 25, 28, 31, 28, 37, 52, 66, 31, 37, 45, 45, 44, 32, 35, 53, 54, 43, 30, 49, 52, 20, 60, 45, 48, 28, 61, 48, 32, 27, 38, 33, 43, 36, 38, 24, 22, 30, 55, 46, 39, 50, 34, 47, 46, 23, 43, 45, 55, 68, 39, 39, 37, 19, 20, 49, 33, 21, 72, 58, 43, 62, 21, 26, 27, 34, 49, 28, 34, 29, 58, 45, 47, 53, 52, 47, 36, 48, 35, 49, 43, 33, 49, 17, 28, 25, 69, 41, 31, 57, 49, 40, 55, 47, 50, 36, 17, 30, 79, 42, 67, 36, 35, 27, 21, 46, 45, 20, 17, 40, 28, 33, 55, 55, 47, 52, 29, 36, 37, 41, 43, 24, 39, 47, 42, 39, 48, 47, 27, 29, 22, 37, 38, 32, 48, 46, 51, 28, 35, 18, 45, 36, 73, 52, 17, 29, 31, 20, 42, 37, 40, 32, 29, 20, 26, 25, 37, 18, 33, 26, 24, 33, 23, 20, 36, 37, 24, 38, 50, 44, 46, 57, 37, 44, 21, 31, 32, 45, 20, 52, 47, 41, 40, 41, 46, 49, 65, 50, 59, 18, 47, 47, 45, 17, 45, 73, 48, 64, 38, 37, 62, 26, 46, 46, 51, 54, 65, 29, 37, 36, 60, 33, 62, 20, 37, 66, 32, 29, 70, 74, 37, 51, 50, 42, 26, 27, 58, 30, 32, 66, 23, 35, 50, 17, 35, 31, 30, 84, 35, 45, 46, 50, 37, 32, 53, 27, 22, 29, 35, 38, 29, 26, 34, 31, 30, 20, 37, 32, 36, 54, 78, 46, 18, 57, 37, 43, 45, 32, 38, 25, 70, 40, 40, 20, 29, 24, 28, 58, 38, 59, 18, 40, 39, 35, 45, 62, 24, 22, 23, 39, 28, 51, 44, 73, 24, 24, 52, 43, 67, 31, 36, 43, 31, 23, 26, 48, 40, 55, 49, 34, 29, 50, 19, 63, 66, 65, 60, 60, 19, 26, 75, 58, 18, 65, 50, 26, 45, 19, 29, 52, 23, 21, 48, 59, 50, 41, 32, 40, 19, 30, 42, 23, 37, 21, 28, 50, 20, 21, 21, 45, 49, 53, 41, 48, 73, 34, 24, 58, 41, 53, 40, 28, 32, 44, 18, 37, 43, 22, 28, 51, 63, 29, 44, 44, 37, 62, 31, 20, 48, 24, 56, 36, 29, 34, 39, 29, 38, 23, 37, 30, 58, 42, 59, 45, 46, 25, 41, 49, 37, 31, 22, 22, 17, 47, 17, 39, 23, 41, 36, 52, 21, 33, 17, 41, 19, 38, 24, 38, 39, 19, 31, 42, 36, 29, 17, 47, 24, 61, 40, 31, 59, 29, 26, 46, 56, 25, 17, 29, 23, 67, 29, 33, 32, 34, 49, 54, 39, 41, 31, 47, 27, 23, 44, 25, 23, 52, 62, 31, 19, 22, 53, 25, 45, 69, 49, 42, 44, 56, 37, 37, 22, 34, 36, 54, 28, 28, 36, 73, 37, 24, 30, 38, 50, 69, 43, 42, 54, 27, 39, 27, 30, 30, 37, 50, 36, 22, 21, 33, 76, 22, 37, 34, 29, 29, 28, 73, 43, 39, 74, 27, 47, 90, 47, 42, 69, 18, 23, 44, 41, 47, 43, 33, 48, 39, 43, 18, 52, 18, 27, 27, 45, 79, 24, 25, 42, 63, 40, 47, 42, 42, 63, 37, 65, 30, 62, 46, 23, 47, 20, 21, 28, 61, 30, 30, 29, 39, 23, 32, 63, 19, 57, 41, 23, 30, 43, 64, 56, 32, 55, 17, 17, 29, 23, 37, 39, 36, 50, 41, 31, 71, 56, 51, 20, 53, 33, 27, 40, 45, 54, 33, 27, 36, 42, 33, 19, 52, 41, 30, 22, 47, 26, 36, 35, 46, 24, 30, 38, 50, 41, 38, 36, 47, 59, 35, 59, 52, 37, 52, 51, 67, 58, 68, 71, 49, 35, 18, 29, 47, 31, 22, 20, 57, 18, 32, 39, 37, 51, 48, 39, 35, 40, 19, 28, 23, 18, 23, 47, 42, 25, 17, 34, 30, 29, 22, 46, 69, 50, 19, 59, 45, 41, 47, 64, 29, 29, 17, 24, 28, 35, 48, 36, 38, 49, 42, 36, 25, 41, 35, 32, 41, 48, 56, 47, 30, 51, 22, 49, 67, 26, 35, 37, 34, 51, 26, 28, 32, 55, 17, 40, 38, 67, 44, 26, 25, 45, 38, 62, 20, 45, 47, 49, 36, 24, 29, 23, 50, 44, 44, 32, 73, 24, 46, 61, 37, 29, 40, 17, 30, 19, 30, 90, 76, 47, 65, 25, 41, 47, 27, 42, 49, 37, 19, 24, 34, 33, 44, 37, 45, 35, 58, 45, 54, 45, 69, 24, 36, 61, 57, 56, 35, 51, 50, 25, 46, 23, 44, 38, 35, 25, 78, 18, 40, 61, 31, 33, 35, 55, 19, 31, 43, 49, 48, 27, 38, 35, 29, 27, 31, 26, 53, 23, 32, 54, 36, 22, 62, 41, 59, 43, 34, 27, 35, 41, 42, 24, 35, 40, 46, 45, 33, 68, 33, 23, 42, 30, 35, 40, 36, 46, 49, 41, 26, 37, 50, 20, 59, 30, 23, 44, 28, 48, 34, 58, 33, 38, 49, 67, 23, 42, 54, 49, 35, 26, 31, 36, 25, 35, 40, 52, 42, 25, 50, 52, 51, 19, 43, 57, 35, 30, 38, 45, 23, 22, 21, 60, 45, 26, 49, 29, 47, 40, 34, 42, 33, 63, 53, 38, 44, 43, 26, 56, 39, 44, 34, 40, 38, 44, 34, 22, 58, 43, 23, 41, 44, 32, 38, 47, 24, 39, 32, 40, 34, 34, 57, 71, 45, 54, 26, 37, 28, 42, 45, 37, 57, 39, 45, 30, 27, 44, 44, 38, 19, 39, 55, 17, 43, 36, 45, 44, 26, 32, 21, 43, 34, 33, 40, 46, 20, 54, 23, 58, 25, 50, 42, 25, 60, 43, 25, 31, 35, 48, 43, 27, 24, 44, 52, 49, 31, 59, 27, 38, 51, 24, 58, 46, 55, 76, 25, 51, 33, 46, 51, 42, 53, 33, 30, 41, 42, 34, 31, 46, 35, 40, 46, 43, 35, 23, 50, 43, 61, 29, 21, 36, 40, 23, 90, 24, 20, 40, 37, 31, 22, 25, 46, 47, 48, 36, 26, 28, 20, 51, 32, 55, 41, 46, 45, 65, 45, 22, 22, 42, 37, 24, 67, 28, 56, 31, 33, 52, 45, 53, 47, 40, 35, 65, 42, 23, 39, 39, 37, 25, 31, 39, 39, 25, 30, 37, 22, 46, 54, 33, 70, 36, 53, 55, 17, 36, 36, 40, 36, 28, 39, 49, 63, 61, 48, 55, 18, 20, 49, 27, 37, 32, 33, 47, 34, 61, 25, 26, 18, 35, 36, 56, 40, 24, 66, 42, 28, 39, 65, 20, 52, 24, 39, 18, 49, 21, 35, 38, 55, 27, 34, 54, 38, 32, 33, 32, 34, 46, 48, 23, 48, 67, 36, 25, 43, 27, 56, 54, 39, 60, 27, 21, 24, 31, 22, 26, 45, 33, 55, 29, 72, 35, 53, 40, 20, 69, 41, 45, 27, 46, 55, 31, 45, 64, 38, 17, 33, 22, 23, 25, 33, 31, 37, 53, 28, 33, 41, 61, 25, 20, 35, 64, 28, 39, 53, 28, 38, 25, 19, 20, 27, 55, 52, 21, 30, 47, 22, 24, 26, 27, 57, 18, 30, 34, 40, 60, 25, 39, 48, 27, 33, 65, 52, 27, 32, 45, 18, 27, 57, 44, 30, 26, 28, 61, 49, 33, 50, 53, 33, 22, 45, 26, 50, 47, 52, 57, 21, 58, 50, 33, 34, 20, 37, 57, 57, 21, 40, 23, 54, 38, 32, 22, 27, 42, 36, 22, 40, 26, 46, 28, 46, 55, 76, 27, 50, 75, 42, 36, 51, 43, 44, 20, 60, 47, 73, 27, 50, 35, 31, 45, 57, 34, 19, 25, 39, 28, 48, 38, 53, 19, 41, 23, 32, 35, 21, 24, 34, 29, 24, 55, 28, 46, 38, 29, 35, 29, 41, 40, 30, 72, 17, 46, 38, 36, 24, 28, 28, 60, 42, 48, 37, 32, 55, 30, 68, 45, 26, 22, 22, 36, 52, 68, 26, 52, 46, 53, 17, 29, 41, 45, 57, 30, 46, 45, 59, 40, 38, 22, 59, 36, 40, 18, 62, 63, 29, 27, 47, 20, 48, 29, 22, 42, 36, 60, 41, 43, 33, 34, 32, 22, 52, 27, 60, 41, 34, 30, 35, 39, 64, 17, 29, 37, 43, 30, 17, 29, 24, 27, 69, 30, 29, 17, 61, 70, 32, 21, 27, 27, 19, 44, 36, 45, 29, 34, 28, 34, 18, 39, 34, 35, 24, 41, 50, 68, 35, 47, 43, 35, 34, 26, 50, 22, 20, 18, 52, 71, 55, 17, 36, 25, 39, 27, 42, 22, 57, 34, 31, 24, 41, 30, 17, 26, 32, 55, 19, 28, 55, 49, 23, 43, 40, 41, 23, 59, 20, 21, 28, 44, 31, 50, 54, 28, 34, 46, 28, 23, 29, 35, 41, 90, 35, 20, 42, 56, 31, 23, 60, 28, 32, 36, 44, 34, 27, 49, 48, 47, 34, 27, 31, 24, 24, 25, 36, 38, 57, 24, 41, 56, 19, 44, 34, 25, 27, 22, 59, 34, 30, 21, 24, 19, 82, 35, 32, 45, 30, 28, 28, 29, 60, 28, 38, 31, 28, 42, 40, 24, 37, 36, 39, 22, 46, 28, 34, 42, 43, 35, 60, 33, 22, 39, 33, 39, 37, 63, 46, 48, 33, 65, 39, 43, 28, 50, 24, 48, 35, 62, 42, 60, 50, 40, 39, 43, 43, 35, 39, 30, 32, 18, 37, 26, 51, 35, 33, 46, 40, 34, 29, 47, 34, 45, 53, 52, 64, 42, 44, 41, 53, 25, 20, 29, 56, 45, 56, 38, 62, 28, 31, 24, 65, 40, 31, 35, 42, 20, 30, 25, 22, 65, 31, 56, 51, 36, 44, 49, 28, 57, 33, 49, 74, 48, 38, 39, 46, 56, 27, 17, 28, 18, 42, 39, 29, 21, 36, 56, 49, 47, 44, 40, 43, 62, 52, 25, 63, 51, 25, 62, 27, 48, 32, 36, 30, 45, 27, 41, 61, 70, 55, 35, 36, 29, 30, 45, 39, 18, 28, 27, 38, 33, 25, 25, 38, 23, 40, 36, 20, 41, 44, 49, 22, 26, 47, 48, 28, 37, 44, 20, 53, 36, 77, 21, 24, 30, 51, 22, 23, 47, 17, 48, 43, 34, 19, 42, 23, 61, 63, 65, 37, 41, 35, 21, 48, 51, 43, 22, 56, 47, 26, 39, 45, 49, 23, 69, 41, 40, 25, 52, 28, 50, 21, 48, 27, 63, 47, 37, 28, 58, 41, 67, 62, 28, 26, 46, 29, 39, 25, 20, 21, 25, 44, 37, 50, 29, 18, 32, 58, 50, 29, 58, 29, 36, 21, 49, 60, 39, 50, 30, 55, 64, 29, 40, 42, 55, 43, 28, 46, 34, 47, 43, 24, 21, 65, 20, 31, 38, 22, 20, 44, 44, 41, 38, 21, 24, 24, 28, 39, 48, 24, 50, 50, 68, 31, 22, 18, 25, 28, 17, 24, 20, 45, 44, 57, 33, 24, 22, 41, 51, 46, 52, 50, 26, 17, 64, 51, 28, 51, 39, 48, 44, 47, 52, 58, 19, 40, 23, 31, 40, 48, 46, 32, 50, 18, 26, 36, 26, 19, 53, 54, 29, 81, 37, 40, 36, 18, 44, 36, 69, 17, 38, 27, 52, 24, 57, 19, 38, 46, 21, 29, 33, 26, 28, 26, 37, 49, 28, 57, 60, 20, 38, 51, 60, 37, 44, 19, 59, 50, 35, 46, 18, 32, 32, 39, 25, 30, 37, 38, 36, 56, 35, 56, 17, 46, 56, 61, 25, 27, 50, 33, 33, 38, 51, 27, 49, 26, 42, 37, 38, 48, 41, 33, 30, 25, 50, 54, 37, 59, 17, 65, 56, 57, 33, 41, 45, 50, 18, 19, 26, 61, 67, 41, 29, 27, 35, 37, 22, 25, 26, 27, 26, 51, 45, 60, 35, 24, 40, 69, 44, 65, 52, 38, 18, 24, 52, 21, 67, 64, 25, 34, 39, 26, 19, 35, 26, 38, 45, 18, 51, 36, 18, 37, 37, 39, 21, 46, 33, 23, 52, 28, 23, 48, 17, 22, 26, 40, 90, 28, 44, 29, 45, 43, 17, 49, 27, 60, 33, 53, 19, 28, 22, 48, 76, 19, 28, 58, 19, 37, 22, 52, 31, 50, 62, 26, 64, 38, 44, 28, 19, 39, 50, 39, 36, 39, 51, 28, 57, 40, 27, 32, 49, 34, 24, 54, 52, 43, 84, 79, 35, 56, 30, 54, 35, 32, 44, 32, 60, 40, 50, 58, 41, 28, 53, 71, 17, 45, 23, 25, 40, 28, 27, 19, 24, 38, 24, 18, 58, 35, 43, 45, 24, 63, 40, 21, 20, 42, 49, 47, 48, 61, 23, 35, 46, 18, 28, 62, 20, 28, 57, 56, 33, 19, 29, 30, 53, 25, 44, 22, 40, 25, 30, 51, 17, 46, 50, 34, 42, 41, 32, 32, 47, 50, 77, 33, 47, 64, 34, 34, 67, 37, 34, 22, 50, 60, 46, 44, 43, 48, 36, 48, 37, 51, 52, 27, 43, 23, 66, 37, 35, 42, 49, 51, 71, 50, 63, 41, 24, 41, 33, 42, 23, 49, 28, 31, 28, 37, 19, 51, 25, 49, 47, 48, 41, 32, 26, 46, 17, 34, 44, 23, 35, 30, 65, 34, 34, 39, 28, 53, 34, 21, 53, 54, 32, 39, 39, 29, 29, 43, 20, 39, 45, 36, 71, 62, 43, 31, 39, 32, 45, 19, 43, 32, 43, 34, 66, 37, 46, 23, 41, 63, 55, 35, 28, 37, 63, 43, 30, 36, 54, 36, 78, 77, 28, 39, 37, 34, 22, 54, 21, 36, 62, 40, 47, 38, 28, 46, 22, 37, 69, 43, 53, 20, 22, 48, 40, 68, 39, 57, 23, 25, 22, 23, 41, 25, 58, 33, 28, 19, 44, 68, 33, 24, 44, 38, 31, 29, 65, 55, 49, 27, 41, 33, 22, 41, 66, 38, 43, 40, 32, 30, 35, 41, 33, 36, 33, 53, 27, 22, 26, 64, 48, 23, 26, 35, 46, 65, 53, 18, 34, 28, 37, 61, 59, 32, 31, 25, 81, 47, 39, 37, 44, 37, 41, 27, 55, 60, 22, 23, 19, 45, 36, 46, 48, 39, 29, 39, 26, 37, 49, 27, 28, 35, 54, 17, 18, 33, 33, 51, 49, 49, 41, 64, 40, 49, 23, 34, 38, 47, 30, 36, 23, 47, 61, 47, 56, 20, 33, 36, 38, 34, 56, 32, 26, 49, 71, 19, 20, 42, 51, 38, 38, 35, 24, 40, 21, 49, 18, 54, 52, 27, 20, 38, 31, 25, 19, 33, 23, 34, 42, 37, 25, 40, 33, 31, 32, 63, 54, 32, 29, 26, 39, 57, 54, 57, 23, 27, 42, 31, 49, 50, 46, 37, 26, 27, 24, 24, 29, 47, 33, 26, 20, 25, 57, 36, 25, 46, 39, 29, 18, 46, 61, 41, 50, 53, 36, 38, 75, 26, 44, 25, 31, 49, 34, 31, 47, 38, 18, 47, 42, 29, 38, 22, 32, 20, 41, 46, 44, 35, 18, 60, 36, 30, 40, 21, 25, 56, 42, 26, 66, 48, 35, 41, 19, 55, 42, 45, 29, 42, 65, 30, 43, 61, 22, 23, 34, 51, 21, 31, 34, 33, 32, 44, 21, 75, 55, 43, 31, 41, 26, 23, 42, 29, 21, 28, 64, 35, 65, 34, 58, 63, 50, 59, 46, 30, 28, 51, 47, 30, 34, 51, 41, 57, 28, 62, 22, 38, 34, 45, 37, 21, 49, 24, 47, 20, 27, 40, 19, 31, 43, 48, 64, 39, 50, 52, 44, 50, 54, 17, 33, 28, 39, 33, 59, 74, 54, 31, 31, 20, 54, 42, 57, 49, 40, 19, 38, 39, 43, 32, 57, 38, 25, 40, 34, 35, 53, 21, 43, 43, 22, 64, 19, 40, 34, 46, 35, 32, 26, 25, 55, 22, 29, 38, 55, 26, 34, 25, 39, 18, 58, 34, 29, 49, 38, 21, 28, 42, 29, 48, 40, 36, 34, 57, 24, 32, 18, 39, 39, 18, 20, 27, 54, 40, 27, 27, 43, 22, 17, 37, 49, 33, 21, 45, 33, 25, 71, 23, 41, 21, 47, 30, 40, 44, 50, 31, 68, 45, 38, 78, 31, 39, 36, 46, 30, 34, 30, 47, 40, 62, 44, 40, 20, 45, 55, 51, 38, 49, 49, 30, 20, 34, 42, 46, 43, 62, 21, 58, 30, 37, 22, 64, 42, 43, 34, 25, 21, 45, 55, 24, 70, 27, 34, 57, 37, 52, 32, 42, 24, 50, 36, 42, 40, 39, 51, 38, 18, 63, 50, 82, 33, 37, 24, 31, 59, 18, 47, 48, 27, 34, 50, 57, 60, 39, 56, 18, 26, 37, 20, 29, 30, 21, 24, 51, 31, 45, 36, 29, 20, 46, 44, 39, 39, 45, 28, 44, 48, 27, 34, 35, 28, 25, 21, 18, 63, 18, 32, 25, 19, 23, 38, 36, 43, 45, 30, 46, 24, 42, 23, 27, 48, 31, 31, 18, 36, 58, 51, 22, 72, 61, 42, 37, 30, 40, 20, 65, 36, 37, 33, 64, 61, 38, 27, 24, 39, 44, 47, 73, 66, 53, 43, 57, 44, 59, 52, 46, 20, 37, 41, 39, 30, 59, 22, 40, 39, 24, 63, 51, 34, 38, 17, 20, 42, 41, 56, 17, 31, 23, 33, 36, 37, 52, 28, 59, 30, 32, 30, 53, 54, 26, 34, 31, 34, 58, 30, 53, 27, 23, 24, 36, 33, 22, 47, 34, 44, 41, 45, 24, 65, 45, 23, 54, 40, 45, 50, 40, 24, 42, 50, 58, 47, 37, 37, 22, 29, 45, 49, 44, 72, 63, 26, 58, 40, 56, 55, 28, 57, 37, 44, 31, 42, 18, 57, 53, 38, 33, 42, 70, 50, 50, 28, 72, 33, 47, 48, 64, 19, 48, 37, 55, 24, 42, 43, 34, 55, 22, 39, 50, 47, 25, 52, 30, 37, 73, 50, 34, 24, 66, 22, 69, 23, 55, 35, 20, 70, 54, 34, 38, 33, 43, 60, 81, 32, 34, 46, 30, 23, 29, 51, 31, 36, 23, 35, 28, 33, 40, 27, 38, 47, 25, 45, 38, 66, 65, 30, 54, 49, 41, 48, 23, 77, 19, 30, 41, 35, 20, 66, 49, 52, 24, 50, 22, 19, 39, 45, 61, 45, 36, 40, 38, 52, 29, 33, 40, 34, 46, 44, 19, 44, 40, 20, 24, 76, 20, 33, 31, 33, 19, 29, 30, 48, 42, 37, 54, 42, 28, 42, 60, 76, 21, 29, 43, 44, 45, 38, 28, 29, 20, 49, 36, 40, 17, 45, 42, 29, 57, 23, 20, 23, 39, 34, 43, 25, 40, 52, 48, 37, 36, 33, 36, 24, 42, 35, 40, 50, 40, 22, 57, 51, 61, 71, 47, 38, 25, 33, 40, 39, 29, 33, 46, 24, 28, 20, 34, 34, 25, 36, 61, 38, 57, 20, 56, 24, 60, 37, 27, 54, 40, 18, 51, 40, 64, 24, 33, 31, 26, 54, 31, 39, 22, 31, 68, 59, 35, 25, 23, 35, 17, 19, 53, 59, 37, 74, 28, 40, 30, 20, 80, 51, 17, 37, 19, 58, 57, 37, 23, 36, 33, 30, 30, 41, 57, 23, 40, 53, 31, 41, 36, 30, 44, 36, 42, 48, 22, 38, 31, 32, 35, 21, 26, 19, 43, 33, 30, 52, 38, 28, 57, 51, 50, 32, 32, 37, 69, 33, 37, 36, 47, 63, 21, 17, 56, 90, 27, 38, 33, 26, 20, 42, 55, 32, 66, 66, 49, 34, 38, 40, 22, 46, 29, 19, 71, 37, 52, 27, 52, 31, 37, 28, 25, 31, 29, 45, 69, 50, 54, 17, 34, 32, 23, 66, 61, 55, 24, 30, 49, 21, 56, 25, 49, 58, 40, 62, 54, 34, 18, 28, 32, 22, 34, 53, 47, 19, 41, 46, 56, 42, 25, 42, 43, 17, 57, 40, 62, 20, 47, 40, 44, 48, 39, 46, 72, 53, 47, 40, 34, 41, 41, 20, 42, 54, 39, 37, 59, 69, 24, 36, 21, 68, 20, 26, 48, 39, 21, 22, 34, 33, 46, 47, 65, 23, 28, 34, 20, 28, 52, 40, 22, 29, 31, 42, 38, 22, 21, 43, 43, 33, 17, 54, 45, 55, 51, 65, 21, 40, 58, 53, 39, 56, 37, 44, 49, 36, 59, 33, 36, 26, 22, 25, 28, 41, 45, 43, 30, 33, 34, 21, 31, 48, 22, 47, 27, 18, 43, 36, 37, 24, 24, 38, 48, 75, 43, 64, 67, 36, 53, 22, 73, 46, 45, 43, 20, 43, 21, 40, 36, 64, 19, 46, 46, 41, 29, 28, 23, 30, 44, 27, 25, 29, 54, 27, 34, 38, 24, 56, 35, 23, 23, 19, 42, 46, 48, 27, 22, 49, 23, 24, 34, 25, 33, 22, 54, 39, 25, 75, 28, 50, 62, 40, 58, 24, 21, 40, 45, 48, 34, 23, 43, 42, 39, 52, 21, 56, 48, 37, 25, 40, 34, 52, 41, 36, 21, 28, 31, 20, 68, 35, 32, 64, 17, 25, 74, 31, 51, 67, 42, 29, 31, 21, 67, 41, 27, 51, 21, 28, 21, 28, 29, 39, 21, 43, 44, 28, 31, 18, 27, 34, 25, 34, 43, 40, 21, 25, 21, 19, 49, 20, 36, 32, 23, 21, 25, 26, 24, 22, 26, 22, 41, 18, 24, 38, 36, 45, 38, 38, 34, 53, 32, 17, 45, 31, 47, 27, 61, 33, 67, 38, 25, 33, 27, 45, 24, 23, 48, 61, 22, 34, 32, 35, 36, 24, 60, 28, 64, 22, 27, 36, 27, 52, 57, 23, 29, 34, 38, 50, 50, 51, 17, 25, 19, 43, 59, 39, 18, 27, 39, 44, 19, 32, 48, 46, 65, 57, 48, 46, 27, 29, 21, 20, 51, 35, 29, 30, 65, 20, 47, 41, 17, 38, 38, 39, 24, 52, 46, 22, 56, 37, 59, 19, 31, 22, 48, 63, 33, 53, 38, 39, 59, 45, 39, 60, 38, 31, 61, 66, 26, 60, 68, 32, 25, 43, 26, 55, 66, 31, 35, 58, 56, 22, 46, 33, 26, 42, 45, 43, 19, 27, 51, 42, 43, 38, 44, 36, 37, 29, 53, 42, 80, 61, 55, 20, 23, 48, 24, 71, 49, 39, 22, 67, 18, 38, 38, 50, 56, 23, 26, 32, 44, 27, 40, 43, 31, 57, 25, 22, 20, 27, 39, 27, 41, 31, 41, 25, 29, 22, 62, 29, 44, 32, 19, 20, 65, 24, 37, 43, 38, 28, 33, 32, 38, 33, 42, 27, 41, 47, 55, 30, 54, 40, 21, 34, 30, 31, 22, 37, 41, 52, 57, 48, 58, 35, 40, 30, 26, 46, 26, 50, 22, 36, 30, 58, 17, 72, 19, 29, 20, 30, 47, 51, 26, 24, 44, 43, 35, 51, 41, 38, 39, 54, 62, 45, 30, 32, 23, 84, 46, 67, 29, 33, 23, 63, 55, 42, 17, 60, 31, 28, 31, 65, 31, 61, 43, 40, 39, 37, 47, 45, 30, 30, 49, 51, 31, 48, 20, 32, 31, 19, 50, 33, 25, 49, 30, 23, 44, 24, 28, 56, 38, 20, 82, 52, 32, 37, 36, 39, 30, 49, 40, 42, 35, 39, 32, 22, 53, 25, 17, 44, 52, 42, 34, 27, 37, 39, 26, 48, 57, 36, 27, 17, 30, 37, 31, 23, 32, 58, 31, 44, 39, 59, 42, 45, 20, 35, 50, 56, 48, 58, 32, 23, 48, 33, 48, 55, 40, 50, 52, 19, 55, 36, 39, 22, 44, 34, 33, 55, 35, 50, 43, 29, 56, 33, 24, 46, 60, 48, 65, 29, 42, 44, 73, 21, 32, 43, 21, 30, 54, 19, 55, 20, 53, 51, 33, 62, 24, 17, 26, 36, 32, 58, 38, 72, 26, 45, 72, 60, 27, 38, 37, 28, 38, 29, 19, 28, 70, 40, 26, 43, 29, 25, 29, 60, 31, 51, 18, 36, 35, 48, 78, 72, 24, 58, 36, 48, 28, 26, 29, 47, 50, 35, 17, 41, 37, 35, 56, 23, 33, 79, 28, 55, 59, 22, 19, 41, 32, 67, 45, 61, 34, 43, 38, 35, 65, 24, 37, 34, 42, 18, 43, 31, 19, 40, 32, 47, 32, 46, 40, 36, 33, 23, 21, 40, 53, 20, 53, 58, 51, 31, 35, 23, 31, 21, 39, 36, 51, 18, 32, 56, 35, 63, 61, 23, 52, 19, 33, 35, 35, 38, 23, 22, 20, 19, 47, 59, 38, 36, 27, 24, 35, 22, 37, 36, 28, 55, 29, 32, 66, 47, 23, 50, 40, 51, 34, 28, 52, 39, 43, 36, 25, 42, 35, 43, 34, 50, 30, 44, 52, 35, 36, 30, 36, 39, 25, 26, 20, 20, 70, 26, 24, 48, 62, 48, 52, 25, 58, 40, 29, 45, 19, 37, 23, 31, 64, 28, 32, 49, 35, 19, 45, 39, 55, 46, 19, 29, 25, 37, 30, 28, 36, 32, 34, 34, 20, 27, 36, 35, 37, 32, 32, 52, 50, 23, 25, 21, 31, 31, 41, 47, 20, 20, 29, 51, 30, 23, 48, 39, 62, 22, 46, 51, 20, 40, 31, 30, 52, 20, 37, 65, 40, 60, 27, 62, 53, 38, 49, 21, 18, 25, 36, 52, 19, 23, 63, 63, 62, 51, 76, 50, 45, 44, 51, 42, 43, 53, 29, 41, 44, 35, 27, 35, 60, 49, 33, 35, 31, 54, 57, 30, 45, 41, 25, 51, 42, 28, 47, 53, 31, 46, 25, 47, 26, 30, 48, 22, 32, 33, 31, 43, 26, 34, 50, 30, 17, 35, 29, 27, 32, 34, 32, 47, 40, 26, 30, 54, 53, 38, 43, 25, 39, 57, 47, 45, 24, 32, 25, 57, 28, 28, 27, 28, 74, 44, 27, 61, 25, 28, 20, 33, 38, 31, 40, 43, 38, 19, 56, 38, 30, 22, 18, 68, 35, 42, 32, 43, 30, 62, 38, 27, 28, 26, 21, 19, 20, 25, 28, 36, 23, 24, 23, 24, 25, 24, 30, 26, 62, 36, 45, 31, 34, 65, 42, 27, 40, 53, 54, 30, 59, 42, 23, 28, 20, 22, 73, 47, 44, 40, 37, 39, 49, 44, 27, 50, 39, 41, 55, 53, 53, 31, 21, 30, 74, 53, 47, 60, 59, 37, 47, 49, 73, 58, 18, 35, 24, 62, 58, 51, 21, 30, 27, 25, 31, 31, 44, 52, 30, 29, 36, 54, 59, 55, 70, 22, 42, 36, 32, 44, 22, 25, 31, 33, 54, 42, 49, 30, 54, 39, 24, 21, 23, 54, 23, 39, 23, 67, 19, 41, 32, 57, 18, 60, 21, 53, 38, 21, 53, 24, 36, 29, 23, 44, 32, 25, 41, 18, 57, 62, 29, 19, 35, 52, 28, 38, 42, 41, 23, 31, 31, 46, 18, 57, 25, 32, 27, 46, 17, 43, 56, 19, 20, 42, 90, 25, 27, 48, 48, 27, 46, 32, 22, 24, 31, 20, 28, 40, 39, 54, 59, 17, 25, 49, 55, 39, 67, 28, 35, 44, 51, 55, 45, 32, 45, 45, 35, 55, 59, 22, 46, 23, 47, 34, 51, 40, 24, 20, 44, 26, 35, 23, 40, 49, 32, 28, 51, 62, 31, 47, 36, 44, 18, 46, 32, 51, 48, 28, 57, 33, 42, 17, 63, 31, 34, 35, 20, 36, 24, 33, 36, 31, 29, 47, 30, 34, 23, 33, 59, 67, 26, 27, 20, 24, 44, 31, 28, 19, 31, 20, 64, 47, 36, 48, 19, 22, 56, 22, 37, 57, 38, 34, 22, 38, 30, 32, 29, 56, 42, 66, 71, 20, 25, 42, 20, 61, 24, 51, 27, 41, 31, 39, 36, 51, 52, 49, 43, 45, 49, 19, 44, 67, 58, 36, 67, 61, 55, 23, 35, 35, 20, 51, 23, 39, 36, 47, 27, 29, 21, 18, 24, 39, 51, 47, 62, 49, 20, 24, 25, 54, 25, 20, 43, 26, 24, 18, 45, 45, 26, 47, 54, 21, 29, 68, 21, 28, 46, 35, 50, 48, 17, 59, 23, 53, 50, 32, 53, 52, 38, 43, 35, 41, 22, 39, 45, 31, 40, 47, 53, 53, 33, 24, 34, 46, 29, 61, 31, 29, 42, 50, 34, 39, 74, 40, 75, 40, 36, 61, 42, 28, 19, 36, 33, 50, 47, 48, 60, 30, 32, 45, 51, 53, 34, 48, 68, 32, 63, 31, 58, 32, 32, 40, 50, 38, 30, 21, 28, 63, 32, 25, 59, 22, 49, 36, 31, 46, 37, 48, 43, 41, 54, 36, 29, 66, 59, 62, 29, 28, 27, 42, 60, 58, 27, 45, 50, 43, 39, 21, 24, 59, 36, 72, 35, 33, 30, 40, 26, 37, 68, 45, 56, 36, 50, 38, 29, 28, 39, 26, 52, 52, 84, 33, 46, 30, 38, 66, 62, 31, 31, 39, 60, 27, 37, 38, 25, 48, 42, 45, 22, 50, 21, 44, 60, 21, 51, 30, 46, 43, 57, 48, 39, 68, 33, 22, 18, 28, 32, 56, 36, 34, 18, 39, 25, 27, 56, 33, 49, 44, 30, 27, 25, 30, 38, 38, 46, 37, 62, 55, 47, 45, 50, 36, 45, 23, 37, 38, 27, 29, 45, 41, 38, 27, 51, 44, 33, 28, 32, 43, 45, 52, 24, 19, 47, 53, 31, 30, 56, 49, 38, 19, 39, 38, 47, 58, 27, 59, 24, 42, 53, 35, 36, 26, 33, 29, 43, 35, 48, 45, 43, 27, 21, 63, 39, 52, 41, 48, 27, 53, 33, 37, 18, 43, 20, 47, 36, 41, 67, 38, 47, 39, 34, 36, 24, 50, 21, 21, 40, 19, 38, 34, 44, 20, 44, 49, 45, 24, 34, 58, 39, 22, 38, 32, 45, 23, 35, 52, 55, 23, 22, 27, 24, 21, 49, 32, 64, 37, 61, 44, 53, 43, 60, 23, 25, 29, 51, 47, 20, 25, 27, 23, 18, 23, 59, 46, 66, 46, 36, 44, 17, 27, 32, 53, 22, 30, 47, 68, 39, 22, 62, 20, 21, 34, 30, 25, 23, 40, 26, 57, 36, 43, 49, 58, 32, 59, 42, 37, 40, 47, 56, 30, 42, 21, 21, 47, 22, 35, 41, 43, 60, 73, 90, 62, 28, 45, 19, 30, 19, 23, 22, 19, 45, 26, 42, 62, 61, 34, 22, 56, 32, 24, 26, 32, 58, 25, 23, 23, 54, 28, 35, 30, 23, 42, 72, 21, 37, 54, 58, 32, 65, 22, 52, 65, 33, 35, 27, 34, 53, 23, 43, 58, 38, 26, 60, 28, 33, 29, 54, 41, 63, 31, 23, 31, 41, 51, 59, 23, 24, 67, 65, 47, 21, 38, 55, 39, 24, 24, 38, 19, 32, 63, 18, 36, 41, 57, 48, 44, 25, 48, 20, 74, 44, 26, 54, 39, 38, 18, 32, 24, 19, 28, 28, 49, 50, 63, 58, 68, 41, 21, 27, 24, 50, 47, 64, 54, 28, 26, 35, 48, 50, 39, 46, 41, 24, 57, 32, 41, 55, 41, 69, 22, 45, 33, 57, 38, 40, 48, 38, 26, 24, 35, 37, 42, 37, 37, 28, 22, 24, 45, 29, 23, 24, 25, 58, 28, 58, 18, 59, 39, 53, 54, 51, 59, 42, 36, 37, 32, 38, 19, 55, 30, 27, 65, 44, 23, 32, 60, 28, 44, 29, 57, 41, 34, 44, 44, 57, 37, 35, 51, 27, 41, 28, 37, 18, 24, 53, 40, 54, 22, 44, 43, 58, 59, 31, 35, 17, 45, 40, 35, 25, 58, 39, 69, 23, 32, 48, 41, 74, 43, 18, 24, 58, 54, 60, 28, 41, 62, 27, 30, 63, 33, 34, 20, 51, 21, 50, 43, 27, 19, 20, 17, 47, 25, 71, 21, 69, 48, 57, 58, 34, 33, 56, 40, 28, 42, 19, 41, 28, 40, 23, 46, 39, 39, 45, 21, 50, 33, 34, 55, 62, 31, 35, 30, 57, 50, 51, 47, 37, 58, 39, 34, 45, 19, 42, 40, 39, 36, 56, 19, 37, 60, 56, 51, 34, 23, 19, 37, 58, 33, 39, 36, 26, 22, 32, 17, 57, 68, 70, 30, 19, 51, 19, 47, 41, 36, 30, 26, 41, 25, 62, 23, 26, 25, 30, 48, 20, 51, 19, 31, 36, 21, 40, 30, 28, 57, 18, 54, 32, 52, 27, 45, 18, 37, 52, 59, 27, 43, 56, 44, 37, 28, 48, 58, 29, 37, 35, 36, 52, 30, 22, 55, 61, 33, 46, 64, 28, 19, 36, 43, 39, 56, 24, 40, 45, 56, 46, 35, 36, 50, 61, 31, 63, 41, 48, 45, 24, 34, 27, 26, 27, 55, 38, 31, 51, 56, 39, 50, 34, 44, 21, 37, 34, 18, 34, 38, 19, 46, 19, 22, 38, 33, 83, 34, 38, 33, 27, 38, 25, 38, 52, 33, 17, 28, 52, 34, 25, 17, 22, 22, 46, 58, 39, 59, 61, 32, 50, 51, 43, 20, 38, 44, 28, 50, 53, 42, 47, 39, 38, 34, 40, 63, 53, 43, 28, 68, 22, 46, 57, 38, 66, 29, 54, 38, 35, 30, 39, 31, 50, 30, 25, 24, 59, 45, 64, 61, 42, 28, 53, 33, 34, 41, 67, 46, 32, 41, 30, 54, 33, 20, 34, 20, 17, 45, 21, 25, 38, 17, 35, 38, 48, 25, 57, 32, 39, 23, 21, 47, 25, 18, 41, 19, 24, 57, 51, 32, 28, 40, 21, 45, 17, 55, 42, 49, 38, 50, 47, 43, 39, 42, 32, 44, 50, 55, 37, 59, 56, 33, 35, 44, 57, 31, 18, 43, 33, 18, 47, 53, 59, 35, 49, 26, 25, 36, 29, 52, 50, 22, 44, 32, 49, 31, 33, 50, 62, 26, 46, 55, 49, 22, 43, 58, 48, 21, 67, 39, 52, 25, 27, 18, 41, 59, 23, 42, 54, 39, 57, 51, 31, 24, 35, 26, 42, 31, 46, 38, 51, 27, 33, 22, 72, 25, 58, 23, 63, 23, 51, 75, 52, 69, 52, 40, 34, 21, 30, 46, 21, 44, 51, 40, 33, 22, 23, 75, 55, 24, 45, 31, 49, 26, 61, 44, 28, 79, 51, 44, 24, 46, 36, 23, 25, 48, 18, 25, 19, 26, 33, 42, 22, 36, 33, 34, 33, 62, 30, 22, 34, 43, 25, 35, 28, 28, 25, 40, 35, 48, 63, 31, 36, 39, 24, 32, 46, 19, 30, 35, 23, 33, 25, 64, 18, 59, 45, 30, 24, 22, 49, 35, 48, 18, 35, 27, 28, 21, 26, 44, 39, 40, 70, 34, 34, 66, 40, 61, 59, 28, 47, 33, 18, 25, 22, 40, 68, 30, 73, 45, 23, 40, 39, 23, 51, 53, 24, 53, 49, 34, 37, 40, 23, 36, 35, 53, 42, 32, 33, 59, 36, 37, 31, 46, 44, 27, 29, 39, 62, 48, 42, 60, 35, 39, 34, 22, 18, 44, 44, 49, 21, 49, 52, 39, 23, 54, 31, 30, 21, 38, 51, 34, 22, 20, 26, 26, 41, 35, 27, 65, 31, 45, 37, 47, 47, 33, 40, 38, 21, 40, 33, 29, 41, 20, 20, 34, 78, 46, 40, 59, 23, 23, 47, 31, 42, 46, 29, 21, 55, 38, 44, 61, 38, 50, 53, 37, 49, 30, 19, 59, 43, 29, 49, 36, 26, 18, 60, 43, 42, 41, 55, 31, 20, 37, 51, 28, 24, 21, 21, 20, 35, 30, 44, 45, 42, 36, 32, 33, 27, 17, 45, 41, 19, 45, 41, 34, 33, 17, 51, 33, 23, 52, 36, 25, 38, 64, 42, 29, 26, 38, 24, 43, 38, 28, 18, 44, 81, 55, 52, 31, 35, 39, 27, 23, 26, 33, 39, 44, 38, 18, 54, 45, 43, 23, 37, 33, 45, 32, 47, 44, 22, 17, 64, 30, 51, 26, 31, 50, 18, 63, 48, 37, 51, 29, 69, 56, 26, 39, 42, 41, 45, 23, 29, 34, 31, 46, 46, 33, 46, 26, 49, 66, 55, 35, 27, 31, 47, 48, 27, 38, 27, 23, 17, 44, 26, 37, 49, 42, 26, 73, 44, 47, 28, 49, 17, 42, 30, 36, 53, 23, 35, 32, 39, 40, 28, 32, 42, 35, 35, 21, 71, 31, 25, 35, 24, 69, 35, 35, 42, 51, 48, 49, 53, 43, 53, 35, 36, 34, 48, 53, 37, 49, 30, 41, 25, 24, 34, 29, 23, 46, 20, 63, 40, 33, 24, 36, 25, 54, 50, 58, 17, 40, 27, 20, 22, 80, 52, 40, 48, 62, 32, 21, 58, 37, 29, 25, 26, 50, 30, 30, 59, 36, 37, 42, 34, 27, 39, 41, 34, 41, 50, 18, 23, 26, 38, 27, 21, 41, 42, 25, 43, 23, 40, 23, 18, 37, 58, 36, 36, 25, 62, 34, 27, 23, 55, 42, 22, 44, 42, 21, 25, 31, 42, 26, 62, 28, 34, 51, 45, 44, 47, 35, 46, 46, 46, 66, 34, 26, 29, 30, 23, 46, 44, 54, 54, 71, 51, 32, 75, 57, 34, 40, 21, 50, 32, 37, 19, 64, 17, 37, 32, 20, 39, 18, 42, 43, 42, 53, 52, 51, 77, 25, 38, 20, 23, 27, 19, 41, 47, 72, 42, 36, 42, 24, 22, 29, 42, 24, 41, 29, 36, 78, 43, 34, 20, 24, 36, 49, 21, 51, 30, 37, 30, 20, 46, 24, 32, 36, 30, 60, 56, 57, 61, 50, 56, 61, 45, 31, 60, 26, 56, 45, 28, 45, 58, 38, 23, 49, 19, 22, 23, 23, 40, 55, 34, 21, 28, 43, 17, 51, 37, 54, 24, 35, 48, 23, 24, 24, 50, 34, 45, 36, 33, 21, 32, 22, 53, 36, 22, 47, 25, 30, 49, 23, 40, 45, 61, 35, 20, 27, 23, 25, 65, 25, 49, 45, 46, 67, 40, 51, 44, 35, 28, 44, 37, 54, 33, 46, 45, 27, 49, 51, 27, 36, 59, 24, 66, 31, 35, 34, 50, 43, 21, 33, 48, 22, 29, 42, 28, 36, 66, 44, 45, 49, 36, 48, 19, 36, 41, 32, 49, 51, 20, 39, 38, 19, 29, 52, 22, 44, 43, 41, 63, 40, 40, 18, 57, 54, 44, 20, 40, 29, 22, 44, 45, 52, 57, 51, 23, 22, 37, 58, 61, 30, 23, 30, 18, 34, 23, 23, 57, 58, 59, 27, 50, 44, 25, 33, 22, 21, 44, 50, 44, 53, 41, 54, 60, 58, 27, 43, 33, 24, 41, 21, 39, 27, 50, 20, 46, 49, 35, 56, 41, 41, 42, 58, 43, 25, 57, 49, 23, 59, 50, 33, 52, 37, 30, 39, 54, 52, 22, 19, 65, 32, 25, 34, 47, 24, 63, 48, 26, 28, 23, 57, 64, 28, 28, 34, 38, 44, 34, 18, 36, 73, 24, 29, 33, 57, 41, 55, 21, 36, 26, 32, 40, 32, 21, 59, 29, 29, 59, 58, 47, 44, 31, 42, 19, 66, 41, 22, 31, 30, 36, 75, 41, 19, 60, 24, 25, 52, 30, 37, 55, 20, 32, 18, 24, 33, 76, 57, 53, 19, 50, 27, 40, 26, 47, 39, 49, 28, 33, 50, 28, 30, 47, 39, 37, 50, 37, 26, 55, 46, 32, 26, 32, 43, 19, 38, 38, 65, 70, 48, 44, 32, 30, 49, 23, 25, 19, 27, 43, 38, 42, 33, 30, 29, 30, 26, 43, 30, 38, 48, 57, 30, 42, 31, 32, 28, 63, 32, 20, 51, 36, 24, 41, 21, 27, 17, 45, 39, 21, 29, 39, 42, 22, 21, 59, 40, 45, 35, 58, 45, 29, 21, 51, 34, 40, 45, 38, 22, 37, 62, 32, 36, 43, 37, 33, 28, 49, 47, 59, 35, 35, 29, 57, 41, 37, 20, 44, 20, 38, 23, 24, 46, 32, 21, 32, 41, 51, 69, 39, 43, 51, 43, 35, 32, 25, 64, 28, 44, 40, 21, 23, 23, 19, 61, 47, 39, 51, 25, 33, 34, 54, 24, 28, 33, 63, 31, 70, 43, 36, 50, 21, 31, 19, 39, 29, 39, 17, 46, 62, 32, 28, 19, 61, 18, 39, 24, 38, 38, 17, 28, 52, 61, 69, 19, 26, 34, 70, 21, 43, 52, 59, 21, 53, 55, 46, 36, 39, 78, 30, 22, 43, 36, 36, 50, 27, 46, 24, 41, 68, 38, 47, 41, 31, 29, 26, 51, 26, 27, 36, 28, 53, 40, 38, 21, 53, 34, 43, 64, 21, 25, 38, 47, 48, 57, 37, 37, 51, 32, 49, 52, 64, 46, 24, 39, 23, 30, 34, 30, 25, 58, 32, 37, 35, 18, 46, 71, 44, 45, 40, 38, 52, 47, 49, 59, 22, 44, 76, 50, 33, 43, 27, 37, 40, 49, 48, 49, 22, 42, 36, 57, 30, 40, 42, 56, 55, 25, 52, 33, 55, 47, 23, 55, 43, 31, 62, 42, 38, 29, 26, 26, 37, 79, 61, 28, 60, 51, 34, 35, 27, 44, 25, 32, 36, 27, 40, 43, 25, 41, 30, 26, 52, 32, 33, 35, 48, 62, 40, 41, 28, 23, 50, 51, 63, 63, 40, 20, 61, 40, 41, 41, 27, 20, 28, 35, 31, 34, 57, 28, 38, 37, 43, 26, 26, 27, 33, 45, 65, 57, 59, 18, 44, 37, 27, 43, 59, 19, 36, 68, 63, 37, 46, 31, 23, 22, 49, 36, 38, 46, 24, 26, 47, 40, 31, 42, 48, 56, 50, 17, 21, 50, 49, 34, 32, 28, 62, 23, 24, 31, 36, 58, 46, 39, 48, 21, 39, 21, 33, 37, 29, 30, 43, 22, 25, 69, 17, 18, 54, 65, 28, 27, 35, 38, 33, 26, 56, 38, 51, 26, 33, 44, 36, 42, 44, 44, 21, 41, 41, 62, 27, 38, 60, 24, 24, 22, 32, 22, 51, 31, 42, 48, 49, 37, 37, 66, 18, 46, 52, 23, 67, 19, 37, 42, 39, 54, 46, 47, 33, 69, 21, 27, 39, 23, 26, 42, 36, 24, 35, 42, 37, 41, 27, 42, 44, 18, 41, 31, 26, 67, 37, 51, 26, 40, 25, 38, 43, 23, 44, 60, 38, 21, 18, 49, 43, 26, 36, 31, 38, 43, 58, 55, 39, 38, 54, 57, 35, 44, 36, 58, 39, 25, 54, 23, 37, 60, 66, 36, 29, 29, 25, 62, 23, 38, 33, 49, 57, 57, 46, 37, 43, 26, 40, 39, 39, 31, 31, 27, 36, 40, 57, 24, 30, 28, 22, 31, 33, 20, 25, 49, 21, 45, 36, 41, 23, 47, 41, 30, 29, 31, 64, 55, 19, 31, 18, 68, 50, 36, 44, 58, 20, 18, 32, 49, 34, 22, 40, 35, 26, 44, 55, 21, 54, 39, 52, 57, 67, 58, 27, 66, 41, 18, 58, 50, 31, 32, 37, 26, 33, 35, 44, 26, 48, 40, 62, 37, 18, 48, 45, 35, 35, 34, 29, 19, 53, 49, 71, 38, 39, 23, 29, 52, 41, 20, 30, 56, 26, 36, 26, 58, 24, 41, 27, 28, 57, 21, 50, 27, 58, 27, 36, 68, 33, 19, 26, 37, 53, 32, 18, 53, 40, 50, 43, 60, 28, 34, 37, 25, 46, 21, 26, 47, 18, 22, 36, 59, 33, 33, 53, 44, 55, 56, 54, 51, 31, 43, 44, 22, 43, 26, 30, 30, 46, 27, 26, 36, 25, 30, 46, 47, 66, 43, 71, 36, 46, 44, 42, 39, 36, 60, 37, 26, 24, 17, 45, 54, 22, 63, 22, 27, 33, 64, 50, 44, 31, 44, 53, 33, 34, 27, 43, 80, 31, 40, 53, 58, 38, 33, 48, 23, 31, 52, 24, 45, 36, 52, 54, 39, 40, 28, 21, 23, 36, 22, 55, 71, 47, 17, 31, 32, 46, 27, 39, 40, 30, 17, 33, 72, 34, 65, 36, 32, 26, 22, 47, 31, 28, 38, 34, 44, 27, 50, 47, 20, 24, 29, 37, 42, 20, 68, 22, 72, 45, 30, 33, 43, 56, 25, 41, 25, 31, 36, 37, 58, 26, 55, 46, 44, 44, 28, 42, 41, 39, 64, 20, 45, 31, 36, 32, 33, 51, 36, 40, 44, 40, 46, 31, 30, 26, 25, 35, 29, 39, 44, 37, 79, 36, 44, 23, 27, 46, 58, 36, 36, 21, 41, 49, 34, 29, 63, 29, 65, 26, 42, 44, 51, 41, 22, 36, 18, 33, 46, 49, 41, 52, 40, 53, 45, 30, 56, 24, 71, 20, 26, 34, 36, 49, 48, 22, 30, 51, 46, 32, 42, 52, 57, 35, 52, 59, 36, 33, 67, 31, 31, 28, 72, 36, 58, 56, 35, 37, 44, 26, 51, 31, 30, 20, 21, 37, 53, 54, 32, 34, 30, 43, 48, 42, 62, 31, 22, 45, 34, 64, 42, 60, 44, 67, 45, 53, 37, 32, 23, 28, 55, 58, 74, 27, 37, 22, 44, 23, 41, 45, 29, 33, 61, 29, 22, 28, 30, 20, 28, 45, 19, 42, 34, 31, 23, 50, 40, 57, 35, 24, 27, 27, 45, 80, 23, 46, 38, 50, 90, 55, 72, 65, 37, 39, 38, 46, 50, 54, 40, 30, 50, 28, 34, 55, 20, 19, 21, 39, 43, 33, 26, 45, 31, 32, 60, 41, 21, 27, 33, 43, 27, 18, 48, 37, 58, 57, 42, 24, 26, 43, 34, 38, 51, 31, 49, 26, 28, 22, 26, 45, 39, 23, 62, 42, 39, 43, 33, 68, 30, 32, 25, 53, 38, 62, 20, 26, 23, 43, 29, 34, 24, 44, 19, 47, 40, 45, 43, 22, 50, 72, 29, 47, 24, 22, 21, 30, 37, 74, 70, 27, 32, 30, 29, 18, 31, 39, 21, 31, 33, 39, 29, 34, 52, 35, 33, 40, 43, 66, 51, 36, 17, 32, 39, 19, 54, 45, 47, 49, 32, 28, 39, 27, 42, 24, 45, 19, 46, 50, 32, 47, 69, 43, 29, 43, 90, 41, 24, 24, 24, 35, 30, 46, 19, 39, 43, 35, 65, 34, 41, 24, 24, 34, 33, 37, 39, 58, 36, 33, 42, 31, 57, 67, 23, 35, 43, 39, 36, 29, 25, 54, 24, 50, 70, 61, 51, 38, 51, 28, 66, 57, 44, 29, 21, 42, 74, 50, 65, 64, 44, 34, 17, 17, 43, 32, 54, 20, 23, 34, 30, 32, 40, 28, 44, 29, 28, 37, 56, 27, 34, 37, 66, 19, 60, 54, 47, 47, 48, 57, 50, 62, 45, 55, 62, 25, 49, 32, 25, 39, 27, 43, 30, 21, 33, 33, 33, 70, 25, 17, 20, 19, 71, 31, 26, 52, 26, 38, 25, 22, 48, 53, 30, 44, 32, 59, 49, 65, 31, 53, 44, 36, 36, 37, 63, 67, 29, 52, 37, 21, 36, 24, 31, 54, 44, 18, 41, 37, 30, 26, 68, 31, 48, 80, 26, 63, 19, 30, 42, 64, 32, 50, 73, 32, 53, 58, 37, 24, 53, 23, 42, 30, 38, 28, 19, 45, 26, 33, 29, 54, 23, 43, 37, 31, 56, 25, 17, 37, 60, 59, 59, 46, 28, 33, 24, 29, 33, 29, 23, 20, 60, 44, 18, 52, 31, 30, 33, 20, 27, 33, 25, 63, 63, 43, 58, 70, 35, 42, 20, 35, 62, 41, 43, 19, 46, 47, 36, 45, 48, 29, 48, 36, 58, 43, 59, 42, 33, 23, 21, 48, 32, 31, 66, 23, 52, 17, 35, 39, 21, 24, 45, 25, 29, 56, 34, 64, 27, 47, 36, 44, 31, 23, 45, 31, 45, 42, 33, 33, 30, 25, 33, 70, 22, 30, 46, 43, 37, 59, 51, 47, 36, 73, 32, 34, 51, 20, 57, 19, 45, 55, 60, 19, 35, 20, 34, 26, 73, 27, 37, 73, 46, 36, 54, 49, 68, 34, 30, 28, 28, 25, 26, 40, 32, 60, 18, 36, 26, 24, 90, 20, 27, 58, 37, 65, 28, 36, 45, 23, 69, 58, 22, 58, 53, 44, 31, 46, 48, 46, 43, 43, 17, 20, 22, 19, 35, 29, 18, 25, 54, 23, 36, 26, 25, 46, 41, 31, 48, 52, 22, 18, 46, 34, 30, 31, 21, 41, 45, 30, 21, 44, 54, 36, 50, 26, 47, 36, 32, 21, 30, 34, 51, 55, 43, 37, 28, 39, 35, 59, 26, 68, 46, 21, 23, 17, 23, 34, 33, 31, 31, 56, 21, 45, 41, 26, 41, 46, 47, 43, 35, 44, 58, 25, 44, 57, 24, 38, 33, 25, 33, 50, 35, 36, 27, 22, 30, 90, 39, 34, 39, 19, 19, 29, 66, 38, 44, 34, 35, 29, 22, 33, 73, 35, 41, 27, 25, 47, 36, 36, 25, 37, 26, 53, 36, 30, 53, 37, 41, 53, 23, 28, 53, 32, 33, 27, 52, 31, 27, 44, 37, 39, 18, 42, 49, 35, 43, 43, 31, 27, 46, 54, 37, 53, 26, 59, 36, 32, 53, 26, 35, 48, 30, 38, 58, 44, 30, 68, 26, 26, 41, 43, 48, 55, 50, 41, 52, 24, 31, 54, 31, 31, 47, 40, 46, 45, 39, 34, 48, 19, 56, 47, 35, 25, 43, 59, 66, 63, 32, 58, 43, 27, 21, 44, 40, 65, 50, 18, 51, 44, 20, 29, 50, 50, 43, 23, 41, 30, 58, 50, 37, 21, 33, 52, 47, 45, 34, 39, 31, 19, 45, 45, 34, 27, 37, 20, 32, 24, 39, 18, 62, 24, 37, 46, 44, 53, 25, 44, 24, 61, 45, 33, 48, 26, 40, 68, 24, 25, 31, 55, 46, 58, 56, 45, 24, 35, 51, 47, 17, 21, 45, 42, 56, 39, 31, 20, 65, 44, 33, 25, 38, 31, 33, 49, 28, 29, 23, 37, 52, 27, 47, 21, 41, 61, 32, 21, 63, 34, 21, 44, 26, 36, 39, 52, 44, 42, 60, 31, 30, 23, 81, 39, 38, 21, 28, 34, 47, 36, 27, 56, 57, 42, 43, 48, 53, 58, 19, 37, 41, 31, 31, 53, 19, 56, 55, 51, 62, 43, 45, 28, 46, 62, 57, 60, 23, 47, 21, 51, 61, 52, 76, 24, 31, 43, 58, 18, 19, 44, 41, 21, 40, 25, 62, 42, 35, 19, 60, 20, 23, 18, 40, 19, 51, 41, 18, 41, 27, 71, 25, 73, 45, 25, 50, 44, 44, 49, 44, 42, 20, 66, 19, 60, 46, 42, 59, 58, 47, 58, 69, 17, 19, 28, 37, 30, 36, 45, 59, 45, 53, 37, 26, 53, 44, 27, 22, 41, 53, 58, 34, 32, 32, 47, 27, 25, 40, 36, 23, 44, 19, 42, 60, 41, 45, 67, 36, 52, 28, 83, 17, 27, 35, 40, 23, 51, 51, 82, 28, 53, 29, 19, 30, 23, 66, 27, 35, 17, 27, 27, 44, 64, 62, 26, 40, 40, 35, 25, 52, 46, 30, 52, 28, 17, 46, 45, 21, 18, 17, 90, 23, 41, 19, 69, 23, 28, 37, 57, 30, 48, 73, 25, 33, 36, 28, 57, 31, 35, 40, 32, 35, 26, 27, 17, 29, 18, 20, 69, 18, 56, 40, 46, 41, 53, 50, 23, 40, 54, 45, 53, 17, 50, 22, 30, 40, 36, 25, 29, 29, 22, 56, 60, 37, 53, 23, 18, 36, 67, 31, 26, 18, 45, 64, 26, 53, 27, 32, 49, 59, 44, 61, 41, 46, 32, 29, 48, 32, 21, 50, 44, 51, 42, 27, 70, 33, 65, 39, 32, 30, 43, 31, 64, 37, 49, 29, 35, 37, 42, 21, 23, 35, 29, 23, 21, 45, 24, 38, 28, 23, 39, 45, 68, 60, 38, 41, 48, 33, 48, 23, 26, 55, 35, 52, 46, 72, 18, 62, 22, 40, 26, 35, 25, 46, 39, 41, 59, 37, 38, 33, 23, 67, 60, 39, 32, 26, 39, 28, 29, 43, 23, 32, 33, 45, 48, 43, 50, 63, 55, 22, 32, 27, 22, 44, 34, 28, 40, 45, 17, 53, 58, 55, 44, 40, 58, 38, 62, 31, 31, 30, 37, 38, 58, 30, 28, 46, 34, 39, 61, 30, 31, 21, 50, 47, 20, 23, 46, 20, 33, 30, 43, 23, 34, 25, 50, 44, 26, 35, 48, 26, 41, 25, 33, 24, 31, 24, 41, 38, 19, 19, 24, 59, 67, 49, 35, 44, 58, 22, 21, 50, 44, 36, 30, 52, 35, 48, 23, 37, 44, 44, 36, 26, 31, 23, 29, 53, 52, 55, 31, 19, 64, 55, 48, 20, 50, 45, 17, 38, 43, 22, 63, 36, 56, 43, 41, 41, 36, 59, 46, 23, 46, 53, 47, 46, 47, 29, 32, 90, 63, 23, 76, 23, 44, 81, 17, 56, 58, 45, 25, 36, 32, 25, 29, 23, 41, 48, 29, 27, 55, 30, 45, 50, 22, 80, 30, 20, 29, 56, 63, 20, 38, 31, 21, 47, 25, 31, 28, 38, 30, 39, 27, 26, 47, 27, 43, 32, 20, 21, 32, 37, 45, 37, 54, 24, 28, 23, 54, 40, 38, 71, 51, 52, 30, 34, 38, 43, 43, 36, 24, 29, 55, 51, 43, 28, 54, 47, 47, 47, 38, 60, 19, 22, 55, 44, 53, 44, 28, 30, 39, 43, 24, 27, 64, 41, 28, 61, 27, 49, 20, 41, 28, 36, 44, 51, 40, 19, 54, 55, 57, 29, 51, 44, 35, 40, 46, 25, 33, 32, 22, 39, 18, 43, 46, 63, 40, 59, 40, 23, 64, 36, 48, 43, 18, 17, 36, 53, 36, 20, 22, 34, 39, 57, 30, 37, 51, 38, 34, 47, 28, 25, 25, 50, 50, 43, 27, 32, 44, 19, 53, 21, 26, 68, 31, 32, 33, 49, 24, 30, 49, 18, 30, 51, 52, 52, 23, 23, 36, 19, 53, 17, 69, 46, 33, 40, 56, 29, 41, 22, 28, 31, 35, 38, 17, 39, 27, 50, 56, 80, 38, 52, 26, 43, 45, 41, 30, 24, 43, 44, 54, 23, 33, 18, 38, 41, 22, 66, 70, 58, 60, 24, 61, 35, 19, 52, 38, 39, 33, 52, 41, 37, 38, 27, 38, 38, 49, 38, 22, 40, 20, 34, 24, 58, 24, 43, 38, 42, 27, 18, 37, 48, 40, 50, 57, 36, 41, 90, 66, 34, 23, 35, 32, 33, 32, 31, 32, 40, 27, 34, 35, 29, 41, 33, 22, 52, 20, 18, 63, 49, 58, 23, 59, 49, 23, 17, 20, 36, 64, 34, 63, 51, 40, 55, 35, 30, 47, 29, 17, 32, 55, 47, 50, 42, 32, 60, 53, 58, 45, 31, 32, 37, 28, 26, 47, 34, 42, 48, 32, 35, 46, 38, 49, 35, 61, 45, 31, 52, 20, 64, 20, 24, 29, 23, 34, 54, 21, 23, 79, 55, 42, 60, 37, 47, 20, 53, 34, 43, 24, 23, 29, 58, 55, 40, 43, 25, 23, 34, 26, 60, 39, 21, 45, 33, 26, 39, 29, 29, 52, 34, 24, 77, 34, 53, 31, 58, 50, 32, 57, 32, 77, 37, 51, 33, 53, 32, 30, 19, 33, 31, 28, 19, 27, 21, 24, 46, 31, 60, 21, 38, 31, 26, 46, 25, 23, 53, 42, 29, 45, 70, 55, 40, 17, 18, 34, 41, 28, 23, 32, 56, 36, 26, 62, 35, 25, 28, 21, 31, 23, 58, 55, 54, 33, 28, 40, 33, 27, 55, 53, 23, 66, 34, 39, 21, 26, 21, 59, 61, 30, 22, 47, 50, 67, 59, 35, 56, 28, 34, 63, 48, 41, 26, 47, 29, 30, 19, 27, 59, 19, 45, 37, 44, 34, 30, 30, 56, 19, 37, 45, 21, 34, 27, 44, 33, 32, 37, 26, 49, 20, 37, 22, 44, 30, 55, 36, 29, 19, 45, 38, 42, 27, 20, 43, 21, 33, 36, 34, 72, 54, 30, 63, 36, 25, 72, 55, 44, 41, 20, 47, 71, 41, 35, 18, 23, 57, 56, 25, 64, 35, 61, 36, 18, 21, 43, 32, 52, 48, 40, 35, 22, 42, 60, 34, 27, 47, 65, 61, 18, 49, 48, 26, 30, 68, 23, 65, 32, 44, 48, 24, 58, 32, 54, 35, 35, 24, 50, 38, 57, 38, 49, 25, 56, 32, 22, 23, 33, 42, 62, 24, 26, 46, 33, 65, 65, 46, 43, 43, 35, 48, 55, 45, 56, 59, 56, 21, 26, 38, 27, 48, 37, 63, 27, 20, 26, 21, 51, 28, 39, 35, 21, 65, 54, 39, 24, 36, 70, 52, 64, 43, 44, 47, 32, 20, 33, 52, 49, 31, 62, 44, 28, 68, 40, 46, 22, 37, 62, 37, 35, 44, 41, 36, 26, 48, 21, 39, 38, 44, 56, 25, 32, 47, 26, 31, 23, 41, 55, 23, 45, 42, 29, 48, 36, 52, 46, 34, 27, 18, 18, 42, 64, 36, 20, 52, 44, 53, 36, 27, 52, 60, 41, 64, 25, 63, 54, 21, 27, 63, 37, 22, 54, 54, 60, 19, 25, 51, 47, 44, 18, 57, 22, 36, 33, 39, 49, 41, 35, 40, 30, 25, 25, 37, 53, 34, 22, 18, 52, 45, 19, 33, 45, 43, 40, 41, 25, 22, 52, 45, 32, 26, 24, 37, 49, 43, 35, 37, 17, 48, 40, 26, 35, 21, 54, 40, 51, 60, 22, 20, 21, 21, 43, 36, 65, 48, 37, 20, 41, 55, 39, 41, 80, 59, 52, 39, 56, 69, 49, 33, 28, 49, 51, 42, 66, 44, 26, 43, 28, 25, 17, 52, 34, 24, 41, 65, 36, 19, 43, 39, 48, 50, 21, 36, 33, 50, 49, 34, 70, 39, 44, 32, 40, 46, 21, 40, 47, 19, 22, 20, 46, 41, 17, 41, 28, 20, 24, 33, 44, 26, 47, 24, 58, 41, 38, 21, 33, 23, 40, 47, 41, 38, 50, 45, 65, 33, 34, 65, 24, 26, 55, 66, 43, 37, 22, 21, 69, 27, 48, 24, 18, 53, 26, 60, 21, 39, 45, 76, 19, 34, 54, 23, 44, 27, 23, 37, 41, 21, 37, 63, 35, 47, 41, 50, 26, 36, 47, 51, 41, 40, 32, 37, 40, 56, 50, 28, 28, 19, 44, 42, 47, 31, 61, 21, 26, 58, 35, 35, 29, 40, 33, 38, 50, 66, 36, 33, 55, 46, 36, 41, 30, 29, 60, 24, 42, 26, 37, 27, 26, 23, 60, 49, 46, 21, 48, 70, 44, 58, 41, 54, 42, 28, 65, 35, 82, 48, 44, 68, 42, 24, 54, 22, 43, 17, 46, 59, 37, 70, 57, 47, 31, 23, 17, 63, 44, 30, 23, 44, 24, 34, 37, 24, 32, 42, 20, 34, 37, 47, 25, 36, 60, 64, 32, 33, 71, 55, 85, 57, 39, 25, 58, 57, 30, 19, 40, 28, 43, 40, 19, 24, 34, 22, 64, 55, 26, 80, 79, 58, 43, 43, 46, 35, 51, 52, 43, 68, 34, 37, 22, 32, 65, 28, 37, 40, 40, 52, 35, 62, 49, 49, 47, 45, 37, 36, 36, 44, 51, 25, 28, 57, 46, 59, 28, 25, 35, 27, 29, 43, 38, 26, 55, 36, 37, 23, 34, 41, 44, 48, 35, 43, 57, 44, 28, 55, 52, 30, 31, 21, 50, 43, 33, 23, 35, 18, 21, 33, 30, 21, 36, 38, 33, 24, 24, 39, 43, 43, 62, 30, 25, 45, 22, 73, 26, 24, 18, 35, 22, 28, 38, 68, 40, 32, 42, 31, 33, 54, 63, 67, 47, 67, 33, 33, 25, 25, 35, 54, 35, 18, 54, 30, 29, 41, 40, 46, 39, 42, 49, 23, 34, 42, 31, 72, 31, 90, 27, 18, 30, 27, 27, 28, 40, 43, 26, 48, 41, 32, 23, 35, 42, 27, 34, 25, 46, 34, 39, 33, 25, 45, 46, 23, 37, 39, 38, 35, 49, 37, 45, 71, 44, 19, 38, 42, 29, 48, 56, 32, 19, 57, 55, 48, 46, 36, 66, 26, 58, 62, 31, 77, 25, 38, 33, 51, 20, 40, 20, 41, 44, 35, 35, 46, 24, 18, 37, 58, 55, 68, 41, 50, 48, 42, 45, 43, 37, 43, 36, 60, 27, 36, 47, 33, 19, 33, 34, 33, 57, 59, 36, 60, 33, 41, 20, 28, 24, 31, 38, 40, 55, 49, 33, 52, 24, 46, 20, 50, 22, 32, 46, 62, 39, 26, 20, 37, 40, 22, 32, 41, 24, 40, 37, 42, 56, 20, 55, 29, 37, 61, 26, 49, 18, 30, 24, 24, 37, 37, 36, 40, 44, 43, 19, 50, 17, 39, 43, 43, 40, 31, 25, 30, 55, 55, 29, 28, 30, 33, 23, 71, 22, 21, 62, 22, 49, 31, 27, 27, 26, 30, 17, 26, 45, 50, 31, 51, 56, 24, 29, 26, 51, 32, 47, 30, 41, 49, 40, 62, 43, 76, 40, 35, 27, 54, 55, 21, 20, 39, 26, 20, 28, 56, 39, 44, 55, 54, 30, 34, 42, 50, 34, 23, 27, 81, 47, 57, 65, 33, 37, 53, 27, 47, 42, 39, 58, 20, 19, 39, 51, 52, 53, 50, 36, 30, 30, 50, 76, 46, 24, 26, 56, 50, 22, 23, 51, 58, 38, 45, 55, 45, 42, 44, 60, 27, 25, 61, 43, 44, 63, 46, 47, 37, 43, 46, 48, 61, 53, 79, 44, 54, 22, 47, 18, 25, 24, 37, 51, 35, 59, 46, 45, 19, 22, 41, 33, 26, 22, 29, 22, 34, 23, 20, 25, 59, 44, 55, 77, 42, 47, 42, 45, 23, 60, 23, 52, 49, 20, 20, 23, 30, 45, 30, 17, 22, 25, 52, 40, 18, 58, 30, 59, 29, 26, 39, 33, 21, 22, 25, 19, 44, 35, 40, 19, 37, 32, 52, 36, 38, 57, 22, 80, 20, 40, 36, 27, 52, 43, 35, 29, 27, 24, 49, 26, 47, 20, 19, 34, 20, 23, 25, 21, 25, 63, 25, 28, 39, 28, 30, 41, 52, 36, 47, 27, 32, 52, 50, 21, 60, 47, 54, 32, 61, 21, 29, 36, 38, 59, 43, 19, 42, 33, 47, 51, 28, 61, 23, 31, 38, 21, 22, 35, 53, 40, 36, 33, 23, 51, 31, 28, 26, 32, 43, 19, 46, 49, 31, 45, 41, 22, 25, 46, 44, 25, 30, 54, 34, 34, 30, 21, 27, 29, 60, 36, 27, 57, 24, 63, 29, 34, 27, 31, 31, 55, 64, 20, 44, 35, 41, 38, 21, 64, 42, 31, 42, 35, 36, 23, 20, 26, 56, 40, 19, 33, 45, 41, 29, 46, 48, 38, 29, 35, 59, 27, 43, 70, 30, 24, 76, 25, 39, 36, 23, 60, 20, 54, 20, 22, 24, 41, 35, 23, 54, 17, 43, 33, 30, 38, 54, 39, 32, 61, 24, 22, 34, 18, 20, 53, 32, 68, 28, 28, 54, 21, 52, 25, 30, 33, 49, 58, 45, 67, 42, 69, 61, 34, 28, 19, 31, 32, 76, 53, 46, 43, 48, 39, 59, 19, 40, 63, 44, 46, 22, 36, 57, 58, 25, 59, 36, 29, 49, 42, 43, 46, 43, 40, 43, 48, 26, 30, 58, 43, 53, 74, 64, 45, 32, 23, 38, 42, 24, 63, 25, 37, 28, 31, 46, 44, 42, 45, 52, 40, 26, 55, 17, 27, 71, 35, 17, 26, 22, 51, 33, 41, 50, 26, 21, 53, 32, 19, 22, 58, 47, 51, 42, 28, 29, 50, 22, 47, 60, 46, 58, 62, 47, 59, 41, 31, 18, 64, 51, 47, 68, 32, 39, 37, 29, 44, 46, 40, 19, 65, 52, 40, 61, 30, 61, 33, 32, 32, 32, 37, 53, 51, 29, 48, 69, 28, 55, 45, 18, 60, 38, 19, 18, 33, 33, 29, 37, 50, 19, 28, 43, 23, 47, 21, 19, 47, 36, 62, 65, 46, 35, 50, 20, 17, 33, 57, 53, 47, 62, 35, 27, 23, 27, 39, 25, 24, 23, 29, 30, 34, 32, 18, 20, 38, 18, 41, 21, 36, 61, 30, 26, 27, 45, 47, 19, 29, 51, 34, 42, 26, 20, 43, 20, 27, 46, 39, 33, 42, 23, 29, 27, 43, 23, 40, 35, 30, 59, 65, 28, 79, 43, 54, 27, 30, 22, 34, 18, 40, 28, 18, 46, 23, 77, 41, 48, 29, 41, 41, 34, 49, 49, 30, 28, 62, 41, 20, 61, 30, 29, 50, 44, 25, 62, 57, 35, 26, 60, 22, 62, 26, 26, 44, 32, 66, 24, 19, 54, 37, 38, 37, 22, 34, 35, 42, 47, 29, 34, 38, 62, 35, 37, 47, 37, 22, 60, 31, 74, 63, 30, 44, 40, 35, 50, 18, 33, 59, 17, 47, 51, 26, 32, 55, 36, 54, 19, 32, 52, 39, 39, 59, 55, 54, 31, 19, 30, 47, 31, 32, 33, 31, 32, 46, 29, 41, 39, 34, 23, 53, 45, 34, 47, 24, 42, 53, 38, 26, 18, 43, 26, 47, 26, 21, 32, 48, 39, 58, 55, 40, 33, 74, 49, 56, 29, 25, 57, 29, 28, 59, 30, 28, 19, 40, 28, 48, 20, 58, 19, 75, 24, 31, 28, 41, 23, 46, 33, 34, 47, 81, 45, 35, 59, 46, 39, 23, 25, 37, 49, 25, 32, 34, 40, 33, 78, 31, 61, 22, 29, 37, 35, 52, 37, 18, 37, 46, 25, 47, 55, 32, 25, 50, 18, 25, 23, 51, 47, 37, 37, 27, 19, 52, 25, 17, 21, 41, 55, 20, 24, 41, 23, 33, 29, 50, 37, 48, 19, 40, 30, 65, 44, 32, 41, 28, 44, 39, 59, 31, 34, 20, 33, 52, 31, 39, 35, 37, 44, 28, 17, 23, 35, 19, 30, 27, 29, 27, 37, 44, 34, 43, 41, 34, 23, 51, 24, 38, 24, 53, 33, 24, 45, 31, 38, 28, 38, 25, 29, 29, 48, 24, 31, 36, 62, 25, 55, 21, 60, 41, 43, 30, 47, 50, 41, 23, 38, 30, 35, 25, 61, 71, 35, 35, 38, 27, 51, 55, 45, 33, 25, 52, 46, 34, 52, 47, 46, 36, 69, 34, 36, 31, 31, 38, 57, 24, 37, 69, 39, 38, 34, 21, 21, 61, 35, 29, 18, 41, 42, 52, 38, 51, 55, 20, 23, 24, 48, 22, 22, 28, 28, 34, 25, 48, 32, 31, 65, 56, 22, 27, 27, 30, 33, 50, 19, 42, 37, 26, 32, 43, 47, 26, 24, 34, 33, 35, 57, 45, 42, 25, 56, 25, 23, 35, 28, 41, 26, 37, 22, 30, 42, 38, 49, 27, 43, 68, 41, 25, 23, 68, 17, 45, 45, 30, 49, 42, 29, 21, 20, 59, 26, 25, 33, 35, 45, 66, 67, 33, 38, 72, 21, 39, 41, 22, 49, 24, 25, 26, 50, 23, 41, 60, 67, 77, 62, 22, 37, 17, 52, 31, 23, 27, 38, 23, 22, 61, 55, 23, 39, 38, 25, 25, 66, 30, 32, 43, 18, 20, 20, 34, 33, 40, 33, 30, 41, 20, 40, 44, 50, 38, 30, 45, 25, 22, 25, 63, 55, 26, 22, 41, 21, 39, 82, 18, 60, 22, 39, 34, 50, 53, 27, 37, 25, 33, 34, 35, 51, 33, 57, 25, 56, 34, 36, 46, 55, 88, 60, 40, 21, 46, 56, 36, 67, 35, 20, 34, 23, 19, 19, 25, 53, 18, 31, 33, 33, 45, 35, 20, 36, 36, 51, 45, 45, 24, 28, 63, 35, 25, 36, 44, 49, 52, 27, 35, 51, 49, 35, 28, 37, 65, 24, 30, 38, 58, 57, 40, 42, 31, 17, 42, 25, 40, 65, 29, 31, 71, 34, 25, 40, 67, 40, 18, 38, 38, 42, 36, 23, 56, 36, 22, 39, 36, 44, 38, 19, 37, 57, 54, 18, 22, 63, 38, 26, 40, 20, 39, 28, 30, 42, 42, 21, 40, 36, 67, 65, 50, 18, 31, 36, 20, 34, 24, 38, 17, 52, 34, 30, 49, 45, 46, 34, 21, 17, 23, 32, 22, 48, 27, 43, 44, 55, 31, 34, 34, 37, 21, 35, 28, 62, 40, 34, 32, 70, 23, 30, 57, 34, 24, 41, 38, 48, 43, 18, 35, 42, 30, 42, 48, 70, 37, 51, 35, 59, 57, 26, 26, 19, 34, 56, 25, 46, 22, 32, 72, 36, 28, 76, 37, 42, 44, 33, 53, 28, 69, 20, 33, 47, 50, 22, 20, 42, 23, 24, 32, 41, 35, 40, 25, 34, 23, 30, 55, 26, 42, 39, 52, 42, 25, 27, 30, 23, 47, 58, 18, 52, 24, 58, 35, 31, 46, 43, 21, 52, 24, 22, 39, 34, 58, 26, 29, 67, 28, 62, 54, 36, 18, 25, 48, 56, 30, 41, 59, 36, 30, 27, 19, 30, 47, 39, 31, 44, 37, 28, 38, 26, 44, 40, 53, 31, 18, 43, 30, 19, 36, 19, 25, 36, 22, 42, 33, 38, 21, 33, 52, 30, 18, 24, 17, 48, 66, 46, 55, 28, 33, 44, 43, 42, 39, 23, 20, 29, 54, 34, 30, 59, 40, 37, 45, 55, 49, 67, 49, 30, 56, 29, 33, 25, 28, 23, 39, 37, 39, 32, 26, 57, 36, 30, 27, 21, 22, 43, 20, 30, 57, 42, 29, 44, 28, 42, 26, 29, 42, 30, 21, 59, 21, 56, 39, 21, 48, 35, 43, 23, 17, 41, 35, 22, 53, 42, 36, 44, 27, 39, 59, 34, 17, 31, 59, 39, 46, 31, 27, 18, 46, 55, 48, 53, 44, 43, 50, 56, 27, 20, 18, 34, 38, 40, 55, 35, 26, 54, 22, 52, 33, 42, 19, 45, 53, 46, 25, 90, 33, 45, 47, 38, 35, 37, 53, 56, 18, 44, 50, 26, 36, 37, 32, 38, 62, 46, 41, 37, 27, 38, 35, 34, 50, 28, 67, 34, 19, 23, 50, 45, 17, 59, 25, 21, 50, 17, 63, 31, 45, 25, 20, 31, 42, 56, 47, 69, 42, 19, 56, 33, 61, 38, 50, 39, 35, 27, 40, 59, 20, 42, 59, 54, 38, 53, 20, 54, 41, 33, 35, 22, 45, 29, 56, 23, 30, 17, 35, 25, 42, 42, 19, 32, 44, 53, 47, 24, 30, 42, 19, 37, 26, 25, 52, 50, 40, 32, 37, 42, 47, 47, 28, 27, 61, 47, 30, 54, 56, 69, 30, 36, 42, 45, 39, 26, 28, 31, 32, 51, 29, 38, 64, 26, 68, 32, 29, 51, 21, 26, 33, 26, 52, 31, 29, 19, 51, 45, 20, 40, 43, 52, 46, 24, 20, 38, 44, 24, 32, 49, 36, 31, 46, 46, 46, 29, 37, 44, 28, 28, 48, 52, 19, 31, 47, 37, 22, 46, 51, 50, 38, 40, 58, 36, 26, 17, 49, 39, 46, 26, 40, 53, 60, 28, 22, 31, 45, 34, 27, 40, 68, 51, 55, 43, 23, 48, 42, 51, 32, 34, 31, 71, 17, 37, 26, 17, 34, 46, 33, 41, 59, 50, 46, 45, 27, 31, 32, 28, 32, 36, 39, 23, 44, 37, 36, 45, 43, 49, 37, 26, 34, 51, 24, 67, 40, 32, 21, 39, 33, 48, 21, 68, 24, 24, 23, 40, 48, 42, 45, 31, 53, 25, 32, 34, 41, 57, 25, 40, 17, 17, 82, 40, 26, 37, 25, 65, 39, 36, 37, 23, 34, 63, 59, 49, 62, 17, 28, 45, 40, 45, 20, 19, 59, 41, 49, 37, 31, 21, 27, 20, 51, 46, 48, 39, 50, 41, 21, 54, 40, 44, 50, 42, 34, 46, 35, 38, 37, 30, 22, 21, 40, 56, 48, 21, 52, 60, 45, 30, 31, 43, 36, 42, 28, 62, 65, 45, 47, 43, 24, 44, 25, 21, 29, 21, 41, 64, 55, 42, 63, 21, 28, 45, 21, 34, 53, 45, 47, 41, 35, 43, 53, 34, 46, 59, 53, 33, 45, 48, 42, 55, 46, 17, 23, 21, 53, 61, 58, 36, 45, 39, 39, 48, 41, 30, 50, 51, 23, 41, 47, 51, 60, 51, 20, 29, 42, 32, 19, 34, 48, 38, 38, 40, 39, 38, 26, 23, 20, 49, 23, 47, 43, 56, 60, 29, 25, 39, 37, 38, 47, 50, 64, 35, 25, 42, 51, 21, 53, 29, 34, 37, 20, 18, 57, 47, 27, 23, 36, 48, 40, 51, 59, 32, 39, 32, 38, 44, 51, 54, 22, 19, 31, 53, 28, 19, 40, 21, 39, 40, 36, 18, 32, 50, 18, 19, 53, 49, 40, 31, 47, 35, 47, 43, 51, 54, 76, 33, 19, 49, 26, 23, 49, 24, 48, 36, 33, 31, 53, 22, 43, 31, 53, 36, 47, 52, 39, 24, 17, 53, 21, 29, 34, 28, 61, 20, 47, 40, 26, 53, 26, 39, 17, 46, 34, 45, 23, 41, 26, 28, 18, 38, 66, 28, 49, 29, 43, 34, 30, 25, 27, 25, 22, 26, 49, 44, 37, 39, 33, 41, 65, 40, 26, 31, 28, 71, 25, 25, 19, 36, 56, 34, 39, 31, 22, 44, 60, 65, 44, 53, 28, 41, 40, 35, 41, 28, 53, 26, 28, 27, 27, 26, 53, 34, 50, 30, 47, 27, 55, 21, 35, 31, 39, 30, 39, 19, 25, 57, 21, 30, 25, 32, 26, 42, 39, 48, 24, 46, 53, 36, 34, 25, 33, 45, 31, 21, 33, 44, 34, 30, 32, 46, 29, 20, 35, 35, 29, 21, 43, 33, 61, 42, 33, 35, 32, 22, 61, 28, 24, 58, 42, 56, 26, 25, 59, 65, 57, 36, 42, 18, 18, 28, 20, 19, 28, 37, 23, 60, 17, 37, 29, 34, 36, 37, 47, 22, 61, 57, 33, 54, 40, 40, 40, 35, 32, 49, 31, 58, 26, 35, 25, 43, 32, 45, 48, 31, 20, 28, 50, 22, 19, 38, 23, 24, 26, 45, 88, 19, 24, 84, 48, 46, 31, 58, 35, 36, 36, 47, 57, 30, 30, 19, 30, 23, 41, 52, 27, 59, 21, 60, 69, 24, 19, 60, 64, 39, 51, 51, 25, 17, 37, 74, 26, 23, 57, 20, 37, 46, 39, 40, 50, 25, 44, 41, 30, 38, 54, 40, 37, 46, 38, 20, 71, 35, 62, 34, 56, 30, 17, 18, 35, 21, 46, 33, 27, 36, 42, 35, 25, 47, 44, 51, 53, 47, 20, 32, 28, 58, 52, 35, 30, 33, 56, 21, 56, 56, 45, 20, 62, 24, 52, 23, 36, 40, 30, 41, 59, 27, 19, 41, 27, 50, 29, 38, 30, 48, 39, 40, 27, 59, 31, 34, 47, 64, 29, 38, 42, 49, 53, 35, 70, 38, 36, 34, 24, 23, 21, 47, 59, 33, 63, 18, 32, 33, 19, 27, 40, 38, 20, 35, 22, 40, 34, 23, 26, 53, 34, 39, 25, 18, 65, 49, 64, 68, 56, 68, 35, 50, 45, 43, 39, 37, 45, 20, 36, 26, 36, 47, 34, 53, 32, 44, 25, 35, 34, 42, 47, 23, 56, 50, 38, 46, 47, 38, 42, 48, 27, 40, 53, 75, 39, 46, 51, 34, 23, 32, 45, 21, 36, 40, 20, 68, 40, 20, 48, 27, 57, 54, 47, 30, 17, 35, 56, 20, 38, 19, 24, 31, 44, 21, 41, 18, 23, 37, 46, 43, 31, 40, 27, 64, 31, 48, 37, 28, 42, 60, 17, 21, 46, 18, 22, 44, 36, 65, 26, 21, 41, 28, 27, 19, 40, 19, 40, 37, 33, 34, 44, 33, 72, 19, 59, 40, 35, 22, 41, 59, 50, 21, 40, 22, 43, 17, 42, 37, 19, 43, 46, 18, 50, 47, 31, 33, 27, 59, 39, 29, 26, 24, 25, 30, 52, 51, 30, 19, 23, 37, 25, 49, 30, 40, 20, 35, 24, 32, 35, 20, 29, 25, 50, 24, 49, 30, 37, 50, 20, 49, 55, 61, 22, 23, 43, 39, 21, 52, 30, 42, 50, 51, 47, 49, 45, 48, 36, 20, 61, 72, 19, 54, 47, 39, 24, 44, 19, 20, 23, 58, 20, 62, 25, 67, 37, 36, 38, 23, 28, 25, 41, 63, 60, 31, 29, 37, 51, 45, 27, 18, 34, 66, 19, 36, 35, 27, 29, 25, 22, 41, 20, 52, 34, 44, 50, 49, 36, 71, 20, 20, 59, 41, 38, 55, 17, 45, 43, 19, 33, 63, 39, 34, 31, 21, 23, 20, 45, 59, 58, 66, 37, 29, 32, 39, 57, 57, 50, 44, 56, 26, 22, 26, 33, 36, 45, 56, 23, 23, 35, 39, 45, 21, 56, 31, 44, 21, 31, 20, 17, 44, 19, 71, 23, 21, 29, 33, 46, 21, 36, 70, 19, 29, 34, 26, 30, 34, 24, 28, 24, 76, 19, 45, 24, 37, 24, 24, 40, 20, 23, 25, 64, 53, 28, 52, 28, 31, 77, 22, 17, 38, 45, 25, 50, 53, 24, 33, 57, 39, 30, 31, 39, 30, 48, 62, 39, 34, 45, 32, 40, 17, 19, 25, 42, 66, 63, 43, 26, 23, 39, 47, 36, 35, 25, 21, 46, 43, 31, 41, 50, 36, 31, 45, 50, 52, 40, 55, 64, 51, 30, 56, 35, 55, 30, 34, 36, 43, 24, 30, 33, 32, 29, 50, 18, 20, 45, 22, 45, 41, 42, 24, 22, 28, 23, 21, 23, 45, 41, 49, 38, 37, 70, 24, 28, 24, 83, 69, 37, 28, 54, 71, 56, 27, 42, 21, 29, 27, 32, 31, 38, 55, 48, 58, 23, 28, 49, 51, 23, 40, 59, 47, 37, 39, 43, 40, 48, 29, 48, 37, 19, 28, 56, 45, 60, 53, 37, 31, 29, 31, 39, 31, 42, 17, 19, 24, 62, 34, 47, 58, 23, 33, 76, 40, 28, 41, 46, 22, 29, 26, 31, 22, 41, 29, 60, 72, 41, 19, 25, 24, 59, 17, 59, 69, 25, 50, 29, 43, 29, 29, 32, 17, 50, 43, 41, 32, 54, 65, 18, 70, 53, 37, 21, 20, 47, 26, 31, 45, 35, 55, 49, 33, 30, 19, 48, 41, 29, 47, 53, 19, 71, 29, 43, 23, 52, 56, 28, 44, 37, 30, 31, 20, 64, 45, 40, 19, 54, 61, 18, 62, 33, 51, 54, 24, 54, 42, 51, 41, 47, 31, 21, 52, 20, 18, 58, 23, 21, 43, 53, 18, 44, 31, 48, 55, 24, 41, 35, 47, 54, 40, 56, 23, 34, 27, 32, 72, 43, 32, 53, 18, 38, 41, 29, 70, 46, 47, 56, 28, 55, 50, 47, 42, 22, 42, 42, 46, 45, 55, 57, 25, 43, 34, 27, 25, 37, 48, 38, 17, 70, 37, 45, 56, 64, 35, 23, 42, 23, 26, 56, 41, 47, 21, 29, 32, 72, 69, 19, 25, 56, 42, 58, 24, 41, 40, 37, 25, 43, 36, 41, 31, 22, 36, 42, 29, 25, 23, 34, 27, 32, 33, 38, 21, 39, 50, 32, 40, 58, 20, 63, 56, 30, 39, 25, 33, 41, 50, 44, 27, 27, 58, 25, 31, 39, 55, 22, 44, 41, 44, 61, 32, 35, 28, 57, 32, 46, 26, 43, 62, 60, 67, 19, 55, 48, 19, 19, 43, 31, 65, 32, 36, 41, 33, 72, 34, 27, 75, 39, 51, 47, 31, 40, 48, 36, 18, 33, 34, 51, 39, 20, 67, 42, 59, 31, 19, 47, 68, 30, 34, 49, 38, 60, 51, 24, 18, 24, 21, 48, 52, 48, 40, 29, 20, 31, 30, 30, 51, 29, 56, 46, 41, 29, 48, 34, 42, 25, 47, 23, 43, 19, 22, 40, 41, 71, 42, 31, 69, 58, 36, 31, 20, 41, 35, 62, 28, 27, 17, 35, 45, 44, 27, 51, 49, 64, 54, 43, 32, 36, 26, 37, 38, 26, 50, 47, 42, 66, 43, 37, 37, 25, 29, 63, 50, 42, 32, 42, 41, 25, 65, 32, 32, 24, 51, 67, 28, 32, 77, 47, 26, 20, 57, 40, 50, 19, 54, 51, 51, 38, 68, 17, 32, 29, 42, 40, 27, 22, 58, 61, 42, 49, 41, 43, 46, 28, 39, 50, 57, 47, 79, 40, 35, 35, 30, 26, 49, 48, 22, 22, 50, 47, 36, 44, 73, 24, 34, 37, 28, 46, 20, 29, 24, 29, 18, 41, 59, 27, 50, 25, 42, 36, 62, 71, 38, 44, 19, 49, 60, 28, 26, 18, 43, 65, 23, 21, 20, 19, 77, 32, 37, 29, 59, 22, 30, 31, 59, 29, 56, 19, 37, 58, 41, 29, 38, 36, 19, 46, 42, 29, 42, 29, 47, 37, 39, 46, 28, 36, 39, 44, 70, 60, 35, 59, 21, 22, 31, 40, 33, 41, 31, 62, 43, 28, 43, 55, 42, 23, 55, 44, 19, 23, 41, 63, 51, 49, 29, 25, 44, 17, 42, 21, 26, 24, 30, 58, 19, 65, 52, 17, 27, 53, 24, 37, 34, 40, 49, 86, 49, 43, 47, 32, 30, 25, 42, 60, 46, 39, 28, 26, 27, 59, 64, 90, 51, 36, 39, 30, 47, 32, 24, 23, 62, 48, 62, 35, 27, 49, 18, 19, 63, 54, 24, 61, 50, 36, 21, 38, 30, 22, 33, 39, 42, 47, 42, 29, 35, 23, 28, 34, 40, 67, 49, 34, 60, 26, 25, 40, 56, 35, 54, 25, 28, 31, 45, 52, 39, 19, 23, 23, 52, 33, 35, 58, 61, 39, 38, 59, 41, 27, 19, 22, 59, 37, 31, 18, 20, 28, 28, 49, 38, 19, 43, 28, 41, 50, 46, 20, 45, 28, 37, 46, 66, 35, 38, 25, 55, 48, 56, 47, 55, 41, 47, 40, 41, 21, 52, 62, 46, 21, 35, 29, 21, 35, 46, 70, 35, 34, 21, 29, 41, 51, 22, 47, 42, 19, 46, 23, 49, 58, 41, 50, 77, 27, 29, 54, 39, 32, 36, 24, 50, 32, 26, 49, 38, 47, 31, 38, 50, 18, 59, 56, 34, 32, 29, 29, 42, 37, 36, 59, 32, 52, 29, 39, 39, 69, 43, 39, 19, 60, 51, 75, 37, 24, 20, 58, 21, 28, 40, 45, 40, 41, 58, 44, 26, 29, 45, 23, 39, 45, 51, 22, 67, 28, 23, 33, 37, 19, 90, 35, 62, 20, 25, 21, 32, 32, 70, 65, 57, 31, 37, 34, 26, 44, 66, 39, 31, 53, 36, 61, 43, 38, 26, 27, 58, 37, 56, 36, 29, 28, 61, 66, 31, 21, 67, 26, 21, 33, 33, 59, 82, 34, 29, 19, 57, 41, 55, 27, 33, 43, 72, 68, 29, 41, 30, 31, 54, 32, 56, 39, 41, 48, 66, 25, 31, 32, 51, 42, 22, 31, 43, 42, 20, 40, 65, 53, 56, 51, 38, 44, 49, 47, 22, 53, 41, 19, 27, 48, 52, 61, 45, 29, 41, 38, 32, 37, 55, 62, 48, 43, 25, 47, 49, 55, 42, 40, 34, 38, 39, 45, 43, 32, 18, 27, 57, 38, 67, 26, 21, 17, 31, 51, 29, 28, 32, 31, 26, 28, 40, 51, 62, 54, 55, 24, 27, 18, 22, 21, 44, 50, 38, 39, 27, 35, 51, 60, 62, 38, 37, 43, 49, 17, 32, 23, 28, 83, 25, 42, 46, 29, 44, 60, 30, 24, 17, 76, 31, 52, 39, 73, 38, 48, 60, 26, 20, 54, 50, 20, 23, 35, 43, 50, 58, 55, 29, 47, 39, 63, 27, 62, 38, 25, 31, 40, 42, 52, 46, 31, 33, 58, 52, 25, 54, 46, 28, 24, 39, 60, 22, 27, 35, 47, 33, 40, 38, 26, 54, 39, 60, 32, 19, 49, 31, 29, 39, 42, 26, 27, 52, 50, 51, 51, 40, 61, 35, 41, 31, 43, 20, 35, 30, 42, 51, 39, 69, 63, 29, 21, 24, 55, 42, 48, 31, 52, 30, 29, 32, 47, 56, 40, 20, 36, 45, 38, 60, 70, 21, 57, 33, 37, 50, 68, 28, 32, 30, 18, 45, 22, 38, 37, 23, 59, 48, 53, 18, 30, 50, 63, 38, 48, 60, 46, 30, 39, 55, 17, 46, 50, 32, 18, 34, 50, 32, 23, 26, 46, 43, 32, 37, 48, 28, 43, 51, 30, 34, 34, 41, 53, 42, 18, 81, 40, 26, 20, 41, 49, 22, 46, 56, 35, 34, 29, 28, 35, 29, 21, 52, 45, 43, 35, 21, 35, 30, 45, 29, 29, 33, 17, 52, 24, 32, 40, 32, 33, 30, 28, 62, 46, 32, 26, 41, 57, 52, 35, 19, 49, 47, 38, 30, 32, 32, 46, 22, 47, 57, 47, 41, 36, 32, 37, 36, 64, 17, 23, 40, 42, 37, 35, 56, 52, 37, 48, 24, 33, 39, 34, 24, 23, 49, 18, 20, 54, 19, 53, 34, 28, 35, 20, 31, 37, 64, 42, 27, 36, 36, 23, 46, 24, 22, 62, 19, 20, 47, 57, 19, 37, 35, 17, 36, 50, 35, 27, 28, 29, 25, 47, 29, 19, 49, 31, 43, 56, 42, 45, 40, 44, 17, 36, 19, 53, 72, 46, 33, 22, 17, 41, 46, 33, 32, 52, 27, 48, 22, 25, 60, 38, 37, 49, 40, 48, 31, 40, 25, 68, 18, 19, 35, 28, 53, 33, 20, 45, 36, 34, 32, 19, 24, 34, 32, 37, 31, 21, 47, 23, 33, 60, 17, 67, 57, 17, 53, 25, 24, 48, 51, 38, 51, 18, 23, 42, 34, 39, 20, 39, 21, 26, 54, 34, 32, 59, 24, 54, 54, 67, 33, 37, 47, 51, 45, 19, 58, 27, 36, 20, 51, 45, 32, 59, 45, 43, 54, 20, 29, 28, 18, 49, 46, 58, 56, 41, 50, 74, 30, 62, 19, 29, 20, 36, 36, 51, 37, 29, 17, 18, 40, 26, 42, 32, 33, 28, 43, 47, 37, 73, 37, 50, 19, 32, 44, 20, 29, 21, 34, 44, 18, 22, 39, 26, 29, 21, 49, 24, 26, 25, 30, 52, 30, 20, 31, 44, 46, 31, 57, 58, 41, 23, 47, 72, 26, 45, 48, 23, 20, 18, 27, 25, 45, 53, 54, 41, 23, 17, 42, 61, 57, 21, 44, 48, 46, 37, 35, 27, 28, 19, 63, 69, 69, 36, 20, 28, 58, 48, 39, 38, 50, 35, 45, 38, 54, 42, 32, 55, 63, 21, 62, 46, 43, 35, 37, 51, 45, 19, 61, 61, 21, 45, 38, 28, 40, 32, 55, 64, 34, 19, 58, 46, 44, 31, 31, 39, 19, 56, 53, 68, 69, 62, 62, 39, 45, 29, 33, 34, 29, 65, 18, 22, 30, 44, 48, 52, 53, 48, 25, 41, 37, 55, 67, 36, 47, 36, 25, 19, 24, 30, 17, 28, 34, 35, 24, 43, 20, 55, 22, 20, 44, 38, 53, 26, 58, 23, 53, 35, 41, 31, 50, 57, 25, 23, 48, 34, 37, 43, 20, 40, 27, 35, 33, 57, 43, 40, 54, 28, 32, 49, 52, 41, 19, 33, 21, 25, 44, 20, 46, 39, 34, 19, 27, 53, 46, 23, 39, 30, 53, 33, 22, 44, 30, 66, 39, 37, 34, 17, 43, 46, 17, 60, 63, 42, 30, 33, 43, 20, 30, 37, 33, 49, 65, 35, 27, 33, 18, 41, 38, 45, 18, 33, 28, 47, 30, 36, 21, 44, 33, 61, 29, 45, 18, 29, 27, 21, 22, 49, 35, 47, 37, 37, 26, 18, 55, 47, 59, 70, 53, 39, 38, 33, 19, 28, 74, 40, 35, 35, 41, 38, 29, 46, 46, 40, 29, 39, 31, 50, 31, 27, 27, 28, 21, 57, 56, 22, 48, 36, 51, 39, 43, 38, 51, 43, 44, 35, 40, 39, 47, 23, 26, 25, 29, 30, 57, 37, 39, 37, 45, 21, 25, 61, 59, 28, 24, 47, 29, 49, 41, 33, 20, 40, 32, 48, 24, 50, 31, 23, 35, 50, 39, 52, 44, 43, 39, 33, 31, 40, 28, 19, 35, 44, 40, 20, 55, 30, 38, 75, 17, 79, 20, 25, 27, 19, 21, 19, 42, 28, 47, 59, 50, 48, 33, 27, 45, 24, 31, 24, 29, 24, 24, 28, 63, 36, 58, 51, 42, 39, 26, 39, 28, 51, 45, 45, 23, 38, 38, 39, 52, 17, 31, 22, 47, 28, 30, 41, 29, 42, 52, 35, 28, 46, 21, 45, 21, 74, 29, 39, 44, 23, 32, 61, 43, 23, 18, 38, 33, 19, 23, 37, 25, 36, 45, 32, 65, 18, 25, 29, 53, 33, 27, 24, 54, 23, 45, 36, 36, 44, 24, 39, 50, 38, 58, 44, 24, 52, 46, 50, 34, 40, 53, 24, 58, 26, 22, 36, 78, 36, 43, 60, 49, 50, 34, 46, 46, 69, 32, 40, 61, 56, 47, 29, 37, 51, 44, 33, 51, 24, 37, 40, 34, 61, 56, 69, 41, 40, 57, 26, 53, 90, 39, 51, 56, 51, 17, 42, 42, 29, 33, 47, 43, 64, 40, 21, 31, 60, 32, 66, 27, 40, 27, 18, 43, 52, 31, 28, 44, 37, 35, 42, 18, 46, 54, 54, 56, 27, 26, 41, 41, 32, 18, 36, 37, 31, 46, 24, 25, 55, 64, 30, 65, 46, 45, 18, 32, 38, 34, 35, 26, 24, 45, 36, 44, 23, 57, 41, 20, 32, 60, 38, 29, 22, 21, 40, 59, 45, 76, 45, 49, 36, 22, 47, 26, 21, 27, 26, 20, 31, 57, 39, 36, 66, 34, 80, 21, 40, 49, 48, 19, 42, 59, 63, 26, 41, 42, 19, 45, 47, 47, 34, 49, 36, 53, 21, 39, 52, 18, 59, 50, 31, 52, 41, 45, 54, 22, 50, 20, 43, 48, 24, 54, 26, 53, 51, 54, 32, 46, 60, 52, 44, 56, 57, 35, 30, 29, 49, 28, 30, 45, 17, 54, 31, 54, 55, 26, 37, 34, 38, 51, 41, 24, 56, 33, 31, 27, 32, 35, 43, 40, 19, 23, 40, 59, 43, 43, 53, 26, 68, 41, 32, 25, 20, 22, 33, 26, 29, 20, 68, 58, 32, 23, 30, 67, 26, 53, 27, 29, 38, 17, 48, 52, 28, 18, 28, 34, 62, 46, 30, 20, 29, 55, 20, 54, 27, 43, 17, 48, 34, 66, 29, 43, 37, 56, 58, 54, 45, 29, 26, 43, 33, 45, 25, 25, 69, 37, 20, 39, 17, 27, 26, 55, 39, 41, 34, 51, 67, 40, 58, 23, 45, 51, 43, 25, 36, 58, 33, 60, 24, 45, 49, 19, 34, 42, 25, 73, 59, 28, 19, 31, 58, 21, 47, 30, 41, 59, 43, 48, 44, 44, 24, 28, 50, 33, 23, 30, 46, 21, 52, 26, 27, 28, 50, 28, 61, 32, 30, 51, 40, 36, 28, 49, 61, 41, 55, 36, 21, 57, 49, 59, 41, 59, 20, 42, 41, 45, 36, 42, 25, 45, 58, 39, 34, 22, 44, 53, 22, 27, 60, 46, 56, 36, 56, 35, 29, 19, 38, 40, 52, 22, 32, 27, 25, 36, 22, 32, 37, 22, 29, 42, 45, 33, 39, 27, 53, 30, 52, 43, 47, 61, 39, 47, 19, 37, 31, 69, 22, 29, 24, 42, 30, 28, 27, 53, 35, 22, 25, 38, 27, 30, 27, 25, 36, 32, 70, 50, 40, 34, 57, 39, 49, 43, 26, 31, 60, 46, 26, 27, 31, 59, 19, 18, 22, 24, 53, 43, 24, 60, 24, 52, 32, 34, 35, 38, 34, 53, 23, 40, 40, 50, 17, 53, 32, 43, 18, 62, 45, 46, 73, 64, 26, 23, 30, 51, 35, 39, 35, 41, 31, 40, 46, 31, 41, 42, 23, 21, 49, 53, 38, 36, 32, 20, 17, 22, 25, 58, 24, 48, 20, 60, 20, 52, 36, 46, 22, 19, 37, 30, 44, 63, 22, 31, 17, 21, 67, 46, 29, 20, 27, 35, 19, 36, 43, 62, 28, 18, 59, 37, 27, 46, 29, 33, 62, 46, 28, 22, 46, 54, 34, 67, 62, 30, 19, 49, 27, 36, 52, 47, 57, 64, 56, 53, 53, 31, 41, 45, 63, 45, 23, 39, 31, 35, 54, 22, 40, 55, 31, 58, 31, 46, 38, 22, 17, 48, 73, 25, 53, 46, 45, 37, 30, 29, 29, 75, 58, 42, 62, 45, 38, 36, 70, 42, 47, 42, 43, 37, 36, 27, 54, 48, 29, 29, 37, 55, 67, 51, 35, 41, 64, 23, 49, 38, 62, 62, 70, 45, 20, 38, 50, 40, 30, 52, 28, 62, 31, 40, 34, 53, 42, 26, 51, 49, 43, 42, 31, 26, 50, 35, 37, 53, 20, 31, 28, 46, 21, 41, 30, 66, 37, 74, 46, 51, 33, 30, 47, 49, 53, 24, 54, 32, 45, 46, 37, 48, 25, 28, 44, 58, 27, 69, 38, 47, 30, 46, 27, 19, 18, 24, 32, 38, 19, 28, 39, 38, 42, 31, 46, 53, 21, 48, 46, 30, 24, 32, 30, 57, 56, 27, 38, 19, 61, 49, 28, 50, 43, 34, 28, 31, 55, 26, 51, 35, 42, 63, 31, 22, 67, 50, 63, 31, 17, 41, 53, 44, 24, 35, 61, 31, 44, 56, 66, 45, 69, 64, 38, 43, 37, 19, 59, 50, 33, 56, 28, 27, 42, 34, 19, 68, 28, 81, 22, 58, 31, 17, 25, 23, 25, 47, 39, 44, 46, 38, 23, 34, 33, 38, 52, 50, 38, 35, 67, 60, 44, 38, 29, 65, 33, 29, 18, 43, 42, 42, 32, 36, 41, 30, 21, 33, 19, 19, 35, 43, 45, 25, 39, 26, 60, 52, 22, 23, 46, 34, 39, 38, 56, 55, 40, 23, 39, 24, 51, 47, 25, 34, 18, 19, 32, 49, 40, 43, 71, 59, 32, 25, 32, 17, 57, 17, 27, 41, 62, 19, 41, 39, 30, 37, 18, 40, 38, 24, 33, 48, 51, 48, 43, 34, 41, 51, 25, 20, 61, 35, 42, 26, 46, 39, 18, 29, 39, 47, 40, 43, 32, 31, 19, 44, 46, 53, 43, 41, 35, 48, 28, 44, 20, 51, 28, 36, 28, 21, 34, 36, 25, 31, 44, 49, 40, 64, 54, 27, 18, 68, 64, 36, 21, 22, 61, 24, 49, 25, 66, 29, 22, 57, 41, 29, 20, 47, 22, 59, 42, 64, 29, 25, 62, 44, 21, 60, 46, 44, 21, 18, 39, 33, 63, 52, 17, 45, 31, 35, 31, 51, 48, 28, 42, 38, 43, 30, 24, 46, 50, 22, 18, 53, 40, 36, 32, 39, 46, 26, 19, 24, 28, 25, 44, 38, 42, 28, 36, 47, 46, 21, 19, 35, 26, 21, 67, 54, 26, 51, 27, 27, 39, 32, 52, 21, 27, 46, 42, 79, 50, 19, 43, 51, 45, 65, 66, 36, 26, 28, 39, 43, 20, 29, 32, 44, 18, 60, 34, 60, 38, 29, 41, 49, 37, 37, 43, 41, 37, 19, 29, 40, 53, 27, 44, 29, 25, 38, 48, 18, 50, 37, 48, 50, 74, 24, 34, 19, 39, 40, 30, 27, 23, 68, 46, 24, 18, 50, 42, 42, 37, 57, 27, 31, 34, 55, 56, 36, 35, 63, 28, 38, 30, 18, 35, 53, 46, 45, 35, 41, 60, 18, 19, 24, 56, 28, 29, 35, 27, 55, 18, 53, 26, 55, 45, 28, 32, 27, 35, 36, 41, 23, 22, 36, 65, 59, 25, 33, 31, 50, 49, 43, 23, 34, 23, 63, 33, 51, 31, 32, 48, 31, 47, 54, 23, 34, 46, 34, 31, 41, 41, 27, 30, 23, 24, 59, 56, 35, 59, 47, 51, 42, 29, 23, 43, 46, 30, 23, 29, 33, 33, 46, 35, 36, 25, 35, 30, 39, 37, 22, 25, 50, 64, 33, 59, 24, 41, 38, 60, 19, 23, 57, 36, 17, 24, 32, 27, 43, 21, 54, 22, 29, 22, 37, 38, 34, 45, 28, 68, 36, 20, 24, 38, 32, 34, 47, 28, 32, 58, 64, 38, 43, 28, 21, 23, 25, 41, 24, 41, 27, 54, 28, 29, 36, 41, 60, 34, 32, 27, 60, 39, 59, 34, 36, 54, 48, 19, 40, 25, 39, 34, 30, 43, 35, 36, 59, 33, 22, 48, 54, 49, 37, 45, 51, 46, 31, 29, 39, 35, 26, 51, 31, 39, 26, 25, 61, 49, 53, 52, 52, 74, 50, 29, 76, 19, 25, 45, 34, 20, 66, 63, 27, 41, 43, 23, 37, 31, 33, 28, 70, 35, 41, 17, 41, 44, 47, 24, 38, 34, 62, 27, 59, 35, 38, 28, 28, 59, 42, 54, 39, 41, 43, 32, 62, 58, 43, 67, 31, 51, 54, 20, 35, 22, 38, 42, 44, 28, 74, 34, 50, 50, 43, 29, 30, 61, 21, 27, 47, 39, 33, 22, 38, 45, 42, 35, 21, 49, 50, 36, 20, 49, 40, 22, 60, 67, 21, 60, 17, 43, 49, 41, 38, 38, 35, 40, 23, 19, 40, 46, 25, 36, 24, 62, 54, 23, 60, 18, 41, 45, 62, 28, 36, 31, 44, 33, 62, 27, 42, 55, 41, 26, 45, 67, 25, 30, 22, 19, 26, 61, 32, 43, 53, 53, 42, 36, 55, 35, 17, 55, 36, 40, 54, 45, 48, 40, 31, 26, 34, 19, 31, 34, 55, 38, 21, 27, 35, 20, 24, 27, 18, 28, 36, 66, 19, 59, 33, 28, 34, 41, 30, 59, 42, 47, 29, 60, 53, 41, 32, 73, 20, 49, 36, 33, 25, 34, 19, 40, 48, 51, 22, 34, 22, 34, 56, 49, 36, 18, 26, 48, 35, 29, 47, 21, 28, 53, 65, 26, 50, 26, 70, 55, 27, 31, 43, 24, 22, 26, 23, 41, 52, 41, 59, 56, 58, 65, 24, 51, 31, 84, 51, 24, 30, 22, 30, 47, 19, 43, 30, 46, 49, 37, 42, 57, 46, 23, 26, 64, 20, 31, 38, 44, 42, 23, 31, 35, 30, 35, 40, 18, 23, 63, 20, 50, 49, 53, 34, 54, 40, 22, 28, 24, 37, 62, 19, 51, 65, 31, 42, 42, 54, 23, 28, 42, 30, 33, 33, 39, 27, 20, 62, 17, 58, 69, 34, 34, 20, 64, 27, 41, 40, 53, 44, 37, 46, 17, 35, 36, 49, 48, 26, 26, 35, 29, 54, 36, 25, 18, 21, 22, 52, 36, 25, 49, 42, 37, 39, 32, 53, 30, 51, 62, 38, 23, 39, 37, 27, 41, 47, 25, 35, 47, 30, 29, 36, 42, 51, 34, 44, 27, 39, 46, 32, 31, 23, 47, 66, 26, 52, 53, 48, 37, 17, 50, 28, 17, 39, 28, 38, 53, 28, 42, 40, 20, 32, 23, 25, 41, 42, 18, 29, 36, 34, 34, 48, 36, 67, 53, 53, 45, 22, 25, 61, 31, 47, 31, 17, 38, 23, 42, 78, 54, 26, 30, 46, 26, 30, 66, 23, 25, 32, 25, 51, 24, 22, 28, 29, 26, 41, 19, 39, 51, 29, 27, 33, 19, 67, 41, 29, 28, 25, 21, 24, 48, 46, 35, 35, 50, 22, 45, 62, 39, 33, 22, 50, 25, 44, 26, 31, 56, 49, 49, 46, 33, 25, 40, 72, 32, 44, 48, 17, 50, 61, 22, 17, 49, 26, 19, 17, 38, 34, 51, 48, 63, 19, 73, 51, 38, 54, 41, 65, 36, 42, 38, 20, 37, 35, 51, 23, 27, 19, 37, 29, 33, 60, 22, 22, 60, 35, 45, 30, 43, 32, 35, 32, 61, 40, 20, 42, 21, 21, 26, 36, 24, 47, 51, 65, 61, 67, 49, 63, 58, 61, 36, 19, 29, 43, 60, 21, 22, 33, 36, 37, 35, 17, 37, 22, 28, 42, 57, 26, 45, 32, 63, 22, 36, 45, 34, 27, 20, 47, 46, 28, 45, 26, 32, 30, 59, 58, 45, 76, 38, 32, 38, 30, 53, 41, 27, 44, 35, 36, 51, 42, 40, 45, 55, 36, 57, 27, 62, 29, 38, 29, 18, 19, 32, 34, 25, 34, 37, 37, 18, 24, 17, 35, 31, 29, 49, 71, 35, 55, 37, 33, 49, 32, 63, 39, 42, 30, 30, 48, 57, 56, 28, 43, 46, 60, 50, 37, 41, 38, 23, 59, 72, 47, 29, 35, 24, 22, 24, 42, 55, 40, 61, 58, 39, 45, 25, 69, 36, 34, 45, 27, 28, 27, 41, 18, 44, 47, 36, 66, 33, 39, 30, 34, 62, 21, 50, 30, 43, 19, 26, 50, 45, 46, 17, 56, 25, 47, 23, 51, 19, 33, 32, 52, 36, 34, 26, 26, 35, 58, 32, 23, 28, 20, 17, 56, 26, 43, 45, 34, 54, 51, 58, 49, 21, 44, 26, 39, 19, 37, 49, 45, 60, 28, 23, 44, 19, 37, 41, 56, 44, 30, 23, 44, 38, 32, 57, 35, 48, 32, 76, 57, 58, 51, 50, 38, 53, 59, 23, 44, 23, 23, 44, 30, 28, 46, 34, 38, 26, 39, 28, 33, 43, 30, 59, 26, 28, 48, 49, 41, 20, 49, 37, 18, 61, 26, 66, 68, 27, 66, 28, 26, 25, 46, 62, 35, 26, 40, 31, 52, 61, 39, 25, 47, 33, 17, 26, 25, 39, 33, 46, 27, 48, 29, 40, 24, 55, 56, 25, 36, 26, 37, 36, 23, 25, 36, 37, 39, 35, 33, 31, 49, 34, 40, 27, 50, 38, 55, 49, 40, 42, 51, 27, 30, 70, 40, 43, 24, 22, 38, 52, 32, 33, 21, 27, 31, 29, 49, 61, 46, 37, 56, 37, 26, 31, 36, 52, 40, 21, 63, 30, 30, 50, 41, 39, 23, 52, 17, 46, 26, 30, 23, 21, 39, 37, 40, 38, 47, 20, 35, 62, 32, 42, 32, 27, 53, 28, 26, 47, 23, 70, 41, 43, 65, 40, 64, 40, 43, 50, 19, 32, 32, 42, 41, 30, 20, 45, 42, 24, 43, 28, 35, 36, 38, 32, 50, 34, 35, 30, 61, 27, 61, 18, 31, 31, 20, 35, 51, 43, 18, 55, 59, 21, 22, 60, 49, 21, 61, 64, 63, 27, 23, 41, 18, 40, 55, 17, 34, 24, 30, 29, 52, 57, 38, 43, 61, 38, 51, 40, 41, 19, 28, 59, 32, 31, 30, 54, 33, 20, 27, 32, 35, 27, 40, 34, 34, 38, 28, 65, 61, 18, 35, 45, 68, 25, 48, 19, 43, 54, 47, 25, 30, 67, 35, 26, 61, 54, 50, 32, 34, 36, 50, 28, 22, 58, 27, 62, 36, 21, 41, 28, 33, 54, 19, 76, 25, 34, 40, 22, 17, 55, 51, 26, 44, 65, 54, 34, 42, 26, 39, 46, 34, 59, 20, 34, 62, 51, 34, 47, 21, 30, 38, 45, 23, 34, 22, 31, 50, 19, 28, 23, 35, 28, 27, 46, 27, 25, 34, 33, 36, 30, 23, 36, 29, 50, 19, 20, 52, 20, 22, 34, 60, 19, 59, 36, 25, 49, 25, 50, 44, 28, 33, 40, 56, 56, 23, 38, 21, 67, 54, 50, 59, 51, 30, 42, 64, 61, 26, 47, 78, 24, 27, 51, 46, 55, 20, 30, 36, 19, 30, 31, 26, 59, 53, 24, 24, 36, 39, 53, 75, 36, 46, 56, 29, 60, 58, 67, 35, 39, 37, 54, 63, 47, 23, 55, 49, 39, 23, 19, 44, 25, 35, 23, 48, 48, 44, 55, 33, 18, 24, 32, 24, 44, 27, 39, 26, 34, 38, 41, 73, 32, 56, 59, 44, 35, 52, 30, 23, 43, 61, 38, 68, 41, 42, 48, 19, 31, 40, 32, 51, 44, 23, 30, 44, 24, 34, 17, 61, 30, 45, 22, 49, 39, 39, 34, 52, 24, 38, 19, 24, 25, 71, 27, 28, 54, 18, 49, 36, 57, 56, 17, 30, 41, 26, 61, 50, 24, 44, 38, 61, 25, 34, 38, 42, 17, 48, 38, 52, 55, 41, 45, 49, 19, 39, 43, 42, 17, 30, 61, 48, 60, 57, 27, 24, 41, 42, 34, 22, 47, 50, 30, 38, 31, 44, 34, 52, 34, 31, 68, 22, 42, 37, 36, 38, 72, 40, 19, 55, 30, 52, 29, 41, 29, 24, 46, 44, 17, 35, 38, 36, 24, 38, 19, 30, 36, 27, 27, 66, 65, 37, 17, 19, 57, 54, 21, 43, 50, 38, 31, 41, 20, 28, 62, 41, 37, 53, 28, 23, 46, 28, 50, 19, 84, 48, 61, 44, 32, 25, 47, 19, 37, 28, 34, 32, 24, 22, 30, 56, 30, 28, 40, 28, 39, 33, 23, 32, 37, 71, 30, 20, 53, 42, 44, 26, 21, 37, 36, 25, 54, 38, 29, 20, 32, 24, 59, 56, 40, 35, 38, 24, 30, 61, 36, 20, 36, 45, 22, 31, 34, 24, 42, 23, 46, 60, 40, 55, 47, 28, 55, 47, 54, 32, 38, 61, 62, 29, 26, 19, 40, 36, 42, 55, 55, 43, 39, 38, 28, 36, 49, 41, 60, 35, 36, 51, 23, 51, 17, 33, 50, 38, 18, 26, 21, 26, 30, 39, 33, 42, 28, 20, 66, 31, 36, 39, 52, 61, 27, 45, 21, 25, 24, 38, 51, 29, 45, 59, 41, 23, 43, 17, 32, 42, 68, 55, 43, 61, 28, 47, 21, 39, 39, 34, 57, 27, 21, 25, 51, 35, 19, 27, 34, 33, 36, 33, 60, 42, 47, 42, 34, 21, 30, 53, 34, 37, 27, 31, 30, 33, 41, 23, 65, 30, 51, 30, 49, 37, 42, 62, 67, 31, 50, 33, 27, 31, 35, 55, 54, 56, 21, 66, 46, 23, 43, 40, 52, 59, 36, 33, 20, 21, 59, 42, 62, 54, 35, 36, 21, 30, 46, 32, 21, 39, 54, 32, 23, 42, 62, 28, 28, 35, 19, 27, 54, 19, 27, 30, 35, 66, 19, 30, 32, 31, 26, 44, 52, 30, 32, 19, 33, 20, 61, 33, 24, 31, 20, 32, 29, 61, 48, 29, 61, 34, 22, 32, 38, 27, 26, 35, 62, 28, 50, 34, 37, 63, 35, 55, 40, 25, 51, 37, 54, 47, 26, 46, 24, 19, 35, 36, 18, 46, 30, 29, 65, 41, 34, 58, 39, 21, 31, 33, 28, 32, 32, 31, 41, 48, 27, 45, 34, 17, 25, 34, 33, 21, 22, 61, 40, 58, 30, 24, 52, 50, 49, 22, 32, 21, 25, 34, 48, 61, 21, 20, 30, 44, 44, 29, 31, 35, 27, 26, 37, 33, 44, 34, 28, 30, 41, 23, 19, 48, 26, 32, 26, 36, 49, 33, 54, 36, 41, 27, 68, 21, 56, 40, 46, 55, 43, 46, 51, 19, 67, 31, 27, 21, 36, 23, 62, 31, 52, 39, 24, 55, 38, 36, 17, 27, 30, 43, 51, 47, 49, 34, 19, 79, 28, 41, 21, 31, 45, 23, 32, 67, 32, 35, 29, 38, 38, 18, 47, 22, 21, 27, 28, 32, 40, 23, 25, 44, 43, 31, 33, 41, 29, 32, 78, 64, 58, 55, 20, 35, 27, 37, 42, 47, 29, 33, 44, 70, 25, 24, 42, 53, 39, 72, 43, 25, 37, 27, 39, 64, 29, 50, 33, 18, 20, 32, 45, 48, 20, 23, 34, 43, 44, 47, 47, 46, 19, 51, 41, 37, 31, 25, 46, 53, 47, 28, 60, 43, 45, 28, 19, 51, 19, 64, 44, 21, 33, 63, 44, 44, 28, 24, 20, 21, 20, 21, 34, 17, 35, 26, 35, 25, 27, 52, 43, 36, 37, 58, 25, 54, 42, 65, 44, 61, 29, 34, 24, 40, 42, 50, 18, 49, 38, 41, 70, 51, 24, 57, 22, 31, 28, 45, 39, 26, 38, 38, 19, 36, 30, 28, 53, 32, 57, 47, 36, 21, 25, 37, 31, 43, 61, 56, 35, 59, 44, 47, 44, 64, 26, 53, 50, 30, 46, 45, 44, 59, 65, 23, 33, 34, 38, 62, 21, 41, 35, 62, 53, 69, 19, 47, 52, 54, 58, 30, 39, 73, 64, 32, 51, 57, 25, 53, 39, 49, 23, 28, 46, 40, 24, 20, 50, 24, 35, 33, 54, 35, 31, 47, 48, 33, 35, 20, 36, 30, 39, 25, 40, 25, 33, 53, 18, 41, 25, 66, 53, 27, 28, 23, 29, 26, 26, 26, 38, 36, 34, 32, 57, 31, 38, 39, 35, 18, 75, 28, 41, 39, 31, 38, 43, 27, 42, 46, 20, 42, 69, 47, 49, 20, 52, 33, 18, 55, 61, 61, 39, 30, 61, 36, 34, 53, 38, 46, 27, 32, 60, 23, 29, 22, 25, 29, 90, 37, 32, 18, 35, 50, 54, 43, 28, 51, 31, 69, 57, 71, 28, 28, 25, 49, 52, 23, 32, 31, 30, 24, 45, 30, 52, 42, 49, 20, 25, 41, 55, 31, 25, 32, 29, 37, 52, 19, 21, 43, 39, 40, 54, 35, 34, 35, 57, 26, 55, 51, 35, 31, 17, 38, 39, 29, 39, 42, 46, 73, 47, 28, 27, 38, 19, 60, 24, 52, 36, 26, 28, 38, 47, 50, 27, 47, 36, 49, 68, 63, 36, 42, 28, 68, 38, 43, 49, 59, 25, 26, 59, 34, 34, 67, 45, 64, 51, 31, 42, 65, 23, 31, 33, 17, 63, 48, 45, 17, 26, 38, 29, 44, 43, 39, 43, 23, 19, 49, 52, 32, 38, 25, 44, 63, 42, 21, 32, 24, 50, 26, 35, 31, 24, 40, 31, 43, 44, 40, 75, 52, 51, 39, 40, 40, 33, 38, 29, 37, 37, 19, 46, 46, 37, 34, 38, 26, 34, 57, 33, 26, 20, 38, 61, 36, 61, 17, 40, 18, 21, 41, 46, 55, 27, 39, 36, 51, 27, 30, 42, 23, 56, 41, 49, 46, 54, 38, 45, 50, 22, 31, 27, 34, 46, 47, 36, 22, 51, 43, 54, 24, 29, 40, 18, 31, 57, 32, 48, 24, 24, 24, 49, 27, 37, 33, 19, 45, 54, 47, 22, 20, 36, 49, 58, 20, 34, 36, 28, 38, 32, 35, 51, 33, 71, 48, 53, 40, 64, 42, 25, 66, 59, 42, 24, 19, 26, 22, 33, 63, 20, 33, 57, 56, 70, 42, 25, 28, 80, 36, 37, 38, 52, 49, 30, 36, 70, 55, 25, 59, 22, 41, 62, 54, 38, 52, 36, 74, 35, 41, 30, 25, 35, 38, 34, 48, 50, 19, 51, 39, 20, 17, 42, 25, 36, 65, 32, 33, 32, 25, 41, 44, 72, 36, 67, 28, 24, 17, 32, 67, 21, 51, 32, 39, 18, 50, 24, 21, 24, 34, 68, 49, 50, 33, 18, 45, 20, 46, 64, 39, 35, 21, 29, 38, 45, 30, 41, 37, 45, 20, 45, 20, 63, 35, 24, 19, 23, 34, 23, 36, 46, 38, 55, 27, 48, 35, 53, 47, 50, 32, 40, 41, 47, 21, 45, 33, 68, 44, 49, 45, 41, 29, 46, 19, 56, 55, 29, 21, 38, 52, 22, 20, 31, 39, 55, 35, 67, 45, 61, 18, 51, 37, 21, 32, 48, 62, 21, 28, 19, 20, 41, 29, 47, 43, 20, 29, 51, 40, 26, 30, 36, 68, 42, 41, 43, 20, 43, 53, 45, 39, 31, 23, 69, 38, 37, 18, 41, 30, 23, 26, 37, 25, 33, 37, 31, 27, 35, 28, 59, 42, 50, 27, 32, 27, 43, 46, 41, 59, 20, 57, 46, 45, 29, 23, 46, 55, 58, 38, 29, 57, 26, 42, 61, 31, 43, 47, 51, 23, 24, 25, 22, 38, 48, 33, 46, 18, 52, 46, 28, 37, 23, 45, 17, 56, 38, 81, 22, 43, 31, 42, 66, 50, 31, 35, 21, 36, 33, 36, 37, 50, 55, 29, 24, 39, 34, 29, 60, 33, 58, 27, 46, 63, 38, 23, 29, 31, 49, 45, 42, 48, 33, 24, 49, 40, 57, 50, 46, 35, 19, 18, 20, 42, 46, 40, 54, 28, 38, 28, 22, 31, 27, 45, 38, 67, 75, 41, 37, 47, 34, 25, 52, 27, 35, 44, 27, 42, 26, 23, 28, 38, 28, 29, 56, 23, 30, 48, 45, 38, 24, 47, 22, 29, 45, 24, 40, 28, 46, 34, 32, 20, 23, 46, 21, 59, 31, 48, 34, 19, 20, 29, 54, 36, 43, 46, 33, 33, 36, 37, 37, 64, 45, 25, 28, 26, 23, 24, 58, 36, 46, 23, 32, 26, 52, 19, 33, 60, 39, 45, 37, 29, 53, 46, 20, 39, 48, 38, 36, 53, 29, 64, 34, 36, 41, 34, 68, 37, 22, 43, 19, 36, 47, 25, 33, 27, 33, 24, 47, 48, 28, 33, 44, 33, 26, 22, 34, 31, 73, 38, 51, 36, 29, 64, 24, 21, 28, 18, 41, 17, 58, 50, 30, 47, 24, 50, 29, 31, 40, 31, 53, 33, 43, 56, 38, 18, 50, 78, 27, 19, 24, 20, 25, 34, 30, 37, 38, 67, 35, 75, 47, 23, 51, 42, 32, 54, 20, 39, 41, 35, 45, 27, 61, 41, 22, 55, 66, 25, 20, 20, 67, 44, 42, 29, 48, 25, 31, 48, 40, 26, 28, 52, 41, 39, 48, 19, 27, 59, 19, 32, 39, 33, 38, 27, 23, 59, 25, 37, 31, 24, 53, 26, 25, 59, 39, 28, 22, 20, 25, 27, 20, 63, 43, 39, 27, 17, 40, 50, 23, 60, 30, 24, 23, 24, 45, 58, 22, 41, 48, 31, 33, 49, 30, 38, 58, 58, 61, 58, 32, 33, 28, 22, 35, 59, 45, 50, 56, 36, 18, 28, 42, 60, 41, 42, 36, 52, 42, 26, 31, 47, 35, 19, 59, 38, 36, 35, 39, 64, 17, 49, 19, 36, 26, 27, 66, 38, 25, 19, 30, 22, 60, 54, 46, 33, 51, 43, 18, 60, 39, 23, 39, 40, 48, 24, 32, 31, 39, 27, 45, 22, 31, 63, 45, 28, 23, 50, 33, 22, 27, 29, 36, 60, 46, 67, 20, 23, 38, 60, 20, 37, 37, 58, 25, 27, 33, 43, 21, 45, 44, 52, 53, 31, 18, 20, 32, 48, 46, 28, 27, 18, 69, 50, 43, 32, 43, 52, 50, 35, 20, 73, 46, 58, 46, 40, 25, 22, 33, 49, 62, 56, 24, 42, 41, 23, 22, 35, 42, 22, 32, 25, 66, 26, 46, 55, 24, 25, 22, 31, 30, 54, 19, 56, 47, 29, 53, 22, 50, 42, 21, 36, 60, 69, 31, 48, 23, 24, 47, 36, 39, 22, 27, 36, 54, 33, 42, 52, 23, 33, 30, 35, 35, 27, 24, 44, 62, 45, 75, 47, 64, 54, 41, 33, 20, 51, 45, 18, 43, 30, 32, 45, 61, 21, 56, 28, 23, 55, 27, 33, 22, 43, 32, 38, 18, 35, 67, 29, 19, 39, 29, 45, 44, 42, 34, 52, 18, 64, 30, 43, 49, 35, 37, 49, 19, 22, 31, 26, 44, 38, 33, 24, 18, 32, 27, 28, 30, 33, 74, 36, 36, 29, 39, 25, 21, 39, 38, 20, 51, 21, 36, 50, 38, 25, 23, 36, 41, 34, 19, 18, 36, 46, 29, 23, 35, 35, 40, 22, 19, 70, 38, 35, 25, 39, 50, 43, 37, 51, 29, 20, 21, 47, 44, 17, 25, 43, 46, 42, 46, 22, 49, 21, 53, 43, 22, 33, 28, 25, 40, 50, 48, 30, 33, 34, 42, 48, 38, 29, 33, 64, 44, 42, 49, 25, 33, 38, 20, 44, 45, 27, 18, 38, 45, 27, 64, 66, 34, 45, 42, 32, 26, 27, 21, 23, 47, 20, 46, 35, 55, 28, 21, 36, 29, 38, 37, 81, 41, 29, 30, 25, 42, 30, 65, 18, 26, 30, 61, 21, 53, 41, 43, 60, 47, 48, 32, 25, 31, 33, 27, 44, 44, 37, 34, 51, 51, 25, 25, 29, 36, 55, 39, 31, 31, 51, 30, 62, 37, 30, 27, 32, 23, 35, 46, 45, 27, 46, 22, 37, 42, 45, 33, 46, 24, 56, 34, 50, 48, 45, 55, 41, 66, 29, 37, 63, 61, 64, 35, 22, 58, 58, 24, 34, 33, 33, 17, 27, 20, 52, 30, 22, 20, 64, 49, 38, 46, 17, 45, 27, 45, 26, 23, 44, 36, 35, 24, 56, 47, 32, 29, 55, 35, 37, 36, 33, 34, 24, 25, 31, 38, 30, 35, 67, 34, 46, 19, 26, 28, 47, 49, 33, 44, 30, 18, 60, 44, 81, 43, 35, 58, 32, 40, 32, 21, 44, 33, 25, 42, 27, 62, 31, 33, 50, 50, 46, 22, 66, 52, 57, 41, 42, 19, 45, 51, 69, 45, 48, 60, 42, 19, 39, 46, 58, 36, 21, 41, 17, 29, 52, 33, 38, 42, 24, 59, 36, 54, 39, 42, 54, 26, 50, 42, 48, 31, 56, 28, 34, 28, 41, 44, 34, 26, 37, 64, 25, 48, 47, 17, 44, 46, 32, 42, 33, 61, 47, 35, 62, 62, 38, 19, 33, 39, 17, 58, 35, 34, 55, 32, 19, 41, 24, 52, 48, 32, 47, 38, 18, 25, 36, 40, 56, 50, 55, 32, 41, 28, 53, 26, 60, 21, 29, 73, 43, 45, 35, 36, 25, 38, 30, 49, 41, 27, 64, 41, 37, 51, 35, 61, 52, 40, 20, 52, 50, 49, 36, 33, 40, 45, 22, 47, 24, 19, 40, 40, 54, 43, 63, 18, 19, 49, 27, 49, 60, 71, 34, 29, 30, 46, 32, 25, 25, 22, 32, 40, 51, 20, 34, 40, 34, 19, 53, 42, 38, 41, 64, 30, 20, 19, 20, 35, 33, 43, 38, 50, 23, 30, 33, 48, 43, 33, 50, 23, 36, 50, 26, 55, 37, 43, 38, 42, 47, 44, 28, 51, 51, 49, 62, 31, 29, 23, 40, 66, 17, 37, 69, 23, 38, 37, 38, 38, 37, 37, 23, 17, 48, 17, 55, 35, 68, 35, 53, 43, 36, 24, 45, 34, 56, 36, 22, 19, 33, 48, 20, 64, 47, 20, 44, 45, 45, 45, 27, 41, 20, 54, 17, 50, 31, 49, 44, 49, 28, 43, 33, 35, 17, 25, 41, 30, 48, 61, 41, 36, 24, 27, 18, 20, 18, 23, 43, 38, 36, 28, 47, 51, 35, 38, 47, 26, 31, 46, 49, 19, 45, 46, 70, 19, 46, 20, 17, 43, 19, 37, 51, 40, 24, 31, 59, 27, 40, 30, 28, 25, 22, 21, 67, 35, 54, 36, 29, 32, 55, 56, 19, 36, 58, 35, 44, 40, 18, 71, 51, 27, 20, 33, 34, 28, 23, 36, 42, 31, 19, 38, 22, 36, 36, 17, 30, 35, 32, 42, 54, 36, 58, 39, 62, 51, 52, 25, 26, 26, 59, 21, 45, 26, 68, 60, 65, 41, 34, 28, 40, 57, 37, 34, 21, 57, 55, 51, 56, 32, 18, 48, 46, 41, 33, 33, 47, 46, 40, 42, 31, 51, 44, 37, 25, 63, 52, 42, 28, 30, 34, 37, 31, 35, 25, 38, 43, 22, 18, 26, 36, 43, 21, 62, 26, 54, 18, 65, 41, 31, 56, 50, 22, 36, 48, 30, 35, 37, 32, 56, 29, 41, 38, 34, 54, 36, 42, 33, 21, 69, 32, 50, 40, 45, 31, 24, 28, 21, 43, 22, 62, 21, 37, 34, 74, 43, 25, 43, 23, 33, 51, 64, 35, 69, 32, 20, 66, 31, 22, 32, 23, 21, 36, 62, 24, 45, 27, 38, 20, 30, 17, 17, 21, 56, 59, 37, 52, 50, 55, 55, 45, 72, 39, 28, 38, 44, 51, 20, 25, 52, 45, 17, 20, 64, 34, 74, 58, 44, 23, 23, 27, 34, 18, 33, 50, 47, 50, 18, 49, 34, 51, 56, 64, 23, 33, 43, 60, 57, 22, 23, 54, 39, 32, 20, 30, 56, 34, 32, 59, 19, 60, 29, 24, 41, 23, 32, 51, 18, 28, 33, 38, 29, 25, 31, 34, 34, 66, 41, 62, 58, 40, 24, 47, 19, 32, 51, 25, 60, 22, 29, 20, 42, 18, 23, 28, 55, 40, 52, 37, 21, 35, 45, 39, 54, 42, 26, 59, 42, 22, 33, 39, 18, 46, 24, 28, 21, 22, 61, 56, 21, 17, 29, 28, 62, 39, 30, 20, 18, 54, 49, 33, 50, 57, 50, 45, 41, 23, 23, 47, 39, 29, 46, 18, 68, 62, 43, 35, 21, 21, 41, 61, 24, 21, 36, 37, 69, 49, 55, 21, 18, 26, 20, 46, 23, 48, 36, 42, 41, 45, 52, 22, 20, 27, 36, 34, 38, 20, 47, 21, 33, 31, 61, 46, 55, 19, 39, 61, 28, 28, 61, 61, 54, 50, 37, 17, 56, 31, 32, 28, 34, 33, 42, 64, 29, 41, 45, 48, 44, 34, 40, 45, 23, 51, 24, 57, 23, 34, 36, 56, 42, 45, 38, 29, 54, 40, 29, 35, 47, 51, 28, 58, 24, 46, 57, 22, 28, 17, 23, 19, 66, 38, 39, 39, 47, 39, 22, 36, 32, 29, 43, 51, 42, 32, 34, 51, 38, 21, 33, 34, 32, 38, 27, 35, 29, 40, 46, 37, 38, 37, 44, 25, 38, 34, 50, 46, 20, 47, 46, 34, 61, 55, 36, 28, 22, 34, 33, 27, 43, 55, 39, 20, 38, 30, 21, 60, 43, 36, 23, 57, 25, 38, 22, 23, 61, 28, 17, 21, 49, 37, 23, 38, 34, 21, 27, 57, 22, 20, 49, 38, 40, 44, 37, 57, 56, 41, 50, 33, 29, 28, 35, 36, 45, 30, 35, 27, 22, 24, 47, 47, 18, 18, 31, 43, 45, 39, 30, 22, 30, 36, 20, 61, 27, 59, 60, 43, 35, 40, 20, 47, 40, 20, 75, 25, 47, 29, 34, 45, 23, 46, 41, 29, 17, 39, 37, 34, 20, 18, 52, 42, 22, 66, 35, 40, 21, 51, 34, 24, 43, 34, 42, 41, 21, 64, 24, 23, 29, 44, 28, 33, 35, 41, 42, 51, 32, 37, 18, 24, 29, 45, 52, 35, 46, 63, 32, 24, 72, 35, 29, 35, 70, 65, 36, 53, 49, 60, 18, 21, 42, 25, 32, 25, 47, 30, 21, 38, 34, 37, 56, 52, 23, 29, 45, 23, 45, 30, 45, 26, 23, 24, 43, 42, 72, 21, 57, 24, 27, 54, 42, 17, 48, 39, 41, 50, 57, 44, 20, 49, 44, 42, 18, 18, 51, 28, 39, 25, 41, 39, 42, 33, 63, 47, 27, 39, 20, 49, 70, 46, 47, 35, 24, 22, 43, 60, 53, 47, 34, 37, 35, 23, 27, 29, 23, 35, 18, 27, 56, 40, 31, 18, 25, 36, 38, 52, 23, 48, 34, 46, 26, 47, 21, 65, 31, 38, 68, 26, 42, 49, 42, 39, 31, 28, 36, 42, 62, 21, 21, 60, 22, 30, 64, 28, 41, 25, 27, 40, 58, 31, 50, 27, 31, 50, 45, 33, 62, 38, 19, 30, 38, 42, 49, 53, 45, 24, 29, 24, 38, 25, 51, 17, 19, 38, 60, 29, 66, 28, 36, 57, 24, 43, 29, 67, 20, 21, 20, 42, 20, 19, 34, 40, 33, 36, 43, 41, 34, 33, 47, 41, 40, 28, 62, 29, 46, 28, 57, 22, 28, 24, 54, 22, 36, 50, 18, 52, 41, 21, 36, 34, 21, 46, 57, 61, 22, 33, 36, 43, 25, 17, 45, 30, 40, 31, 42, 53, 37, 37, 54, 36, 46, 46, 32, 17, 42, 37, 29, 54, 38, 46, 24, 57, 59, 33, 35, 53, 26, 55, 46, 44, 30, 24, 32, 35, 67, 56, 26, 46, 52, 38, 31, 53, 27, 39, 22, 37, 20, 60, 51, 57, 59, 37, 35, 43, 17, 45, 18, 52, 18, 20, 34, 19, 18, 17, 30, 29, 30, 18, 50, 22, 21, 39, 20, 62, 32, 38, 32, 55, 38, 34, 28, 26, 41, 61, 75, 21, 50, 49, 26, 39, 24, 56, 17, 37, 23, 28, 57, 42, 25, 22, 39, 54, 41, 43, 19, 35, 34, 19, 33, 33, 49, 31, 69, 41, 24, 45, 63, 75, 34, 69, 33, 44, 45, 18, 48, 43, 26, 35, 34, 48, 54, 57, 21, 28, 51, 34, 41, 21, 26, 66, 40, 48, 44, 41, 41, 55, 43, 51, 31, 30, 51, 31, 18, 60, 29, 33, 34, 40, 57, 55, 40, 26, 29, 30, 29, 43, 65, 19, 35, 28, 34, 61, 63, 47, 31, 18, 29, 59, 50, 49, 60, 35, 48, 21, 41, 56, 26, 53, 23, 58, 61, 29, 22, 43, 31, 90, 53, 22, 55, 34, 28, 22, 36, 33, 17, 38, 60, 34, 72, 57, 39, 50, 45, 37, 27, 43, 41, 20, 44, 50, 54, 31, 33, 22, 46, 30, 22, 59, 57, 26, 52, 55, 47, 19, 26, 44, 42, 39, 17, 26, 26, 50, 46, 18, 26, 32, 50, 47, 52, 25, 42, 53, 30, 23, 23, 32, 29, 40, 45, 17, 55, 59, 56, 23, 37, 25, 22, 27, 21, 47, 38, 62, 27, 25, 27, 40, 22, 33, 28, 28, 29, 46, 30, 65, 28, 39, 35, 50, 25, 63, 56, 46, 19, 56, 31, 51, 17, 27, 30, 18, 41, 40, 35, 29, 27, 25, 53, 53, 26, 41, 28, 48, 22, 40, 52, 53, 40, 57, 53, 23, 31, 20, 25, 37, 50, 34, 37, 52, 29, 49, 31, 47, 51, 27, 23, 55, 28, 31, 30, 23, 19, 22, 50, 64, 21, 75, 29, 47, 20, 33, 45, 41, 23, 38, 67, 21, 38, 31, 19, 59, 36, 50, 60, 30, 28, 36, 21, 27, 18, 44, 56, 51, 48, 31, 35, 35, 36, 47, 23, 21, 49, 51, 46, 31, 27, 56, 48, 30, 29, 50, 18, 37, 51, 31, 29, 39, 48, 44, 39, 22, 27, 42, 18, 27, 51, 27, 60, 36, 45, 40, 63, 41, 53, 34, 34, 31, 22, 49, 43, 36, 27, 52, 29, 27, 42, 49, 24, 22, 30, 28, 20, 45, 39, 43, 18, 53, 74, 39, 33, 49, 21, 65, 39, 49, 20, 47, 46, 29, 61, 48, 44, 37, 45, 51, 33, 55, 25, 26, 54, 38, 39, 20, 30, 29, 42, 24, 36, 60, 42, 42, 46, 52, 27, 43, 43, 27, 52, 31, 38, 44, 27, 50, 23, 22, 36, 24, 59, 39, 23, 37, 25, 40, 38, 31, 28, 24, 46, 29, 38, 53, 33, 39, 44, 46, 30, 24, 29, 62, 25, 45, 21, 64, 39, 60, 35, 27, 41, 32, 24, 32, 42, 58, 23, 31, 28, 41, 37, 19, 27, 61, 39, 25, 58, 27, 51, 43, 46, 67, 50, 20, 35, 57, 40, 42, 67, 47, 30, 26, 51, 38, 24, 48, 34, 70, 35, 61, 26, 64, 26, 39, 55, 23, 33, 31, 28, 50, 26, 44, 51, 34, 37, 29, 24, 27, 44, 24, 54, 47, 30, 35, 39, 42, 70, 41, 36, 44, 42, 20, 29, 27, 30, 59, 31, 21, 26, 57, 59, 87, 25, 39, 53, 32, 38, 21, 17, 44, 23, 57, 30, 51, 54, 44, 60, 28, 39, 55, 36, 55, 33, 30, 40, 30, 40, 24, 18, 18, 49, 42, 28, 32, 48, 24, 73, 35, 23, 19, 27, 27, 41, 31, 64, 45, 47, 22, 24, 69, 40, 45, 47, 31, 48, 41, 34, 48, 28, 20, 39, 40, 28, 20, 41, 49, 24, 35, 22, 23, 23, 42, 34, 32, 54, 48, 38, 48, 22, 42, 31, 27, 57, 46, 44, 42, 24, 38, 29, 38, 33, 61, 49, 51, 42, 21, 29, 54, 23, 52, 28, 23, 43, 45, 51, 29, 23, 73, 47, 61, 26, 40, 32, 39, 33, 35, 42, 46, 33, 47, 33, 26, 42, 28, 29, 18, 33, 36, 33, 40, 57, 40, 36, 45, 52, 24, 30, 28, 46, 36, 48, 36, 48, 21, 18, 38, 66, 36, 38, 55, 27, 22, 29, 40, 21, 49, 29, 28, 26, 33, 72, 59, 37, 56, 27, 39, 49, 30, 28, 25, 18, 25, 36, 52, 20, 25, 26, 50, 48, 39, 30, 26, 36, 49, 34, 28, 28, 42, 26, 54, 66, 61, 45, 29, 74, 61, 27, 53, 57, 41, 45, 43, 26, 29, 65, 41, 31, 50, 52, 18, 26, 57, 63, 29, 32, 34, 43, 39, 28, 32, 28, 40, 25, 54, 48, 51, 67, 17, 24, 25, 68, 54, 32, 61, 41, 53, 45, 64, 51, 41, 40, 27, 41, 51, 36, 34, 43, 61, 31, 34, 43, 52, 18, 37, 60, 47, 21, 34, 28, 40, 26, 48, 46, 58, 32, 90, 38, 20, 43, 17, 36, 36, 30, 45, 51, 29, 35, 19, 56, 64, 62, 24, 33, 43, 22, 44, 37, 34, 43, 39, 30, 58, 31, 45, 30, 42, 50, 25, 34, 50, 72, 60, 41, 71, 40, 22, 52, 30, 36, 40, 31, 44, 38, 46, 57, 39, 42, 43, 22, 44, 22, 37, 20, 26, 35, 28, 34, 17, 18, 25, 44, 61, 39, 61, 19, 22, 19, 45, 20, 37, 26, 17, 26, 58, 61, 64, 19, 36, 47, 62, 36, 38, 54, 31, 26, 50, 41, 52, 28, 23, 46, 32, 59, 30, 58, 31, 36, 31, 52, 29, 48, 30, 58, 20, 19, 63, 51, 53, 54, 35, 26, 47, 44, 80, 55, 43, 49, 27, 42, 39, 36, 32, 58, 63, 49, 36, 46, 47, 39, 33, 43, 30, 39, 44, 27, 80, 31, 34, 28, 55, 36, 42, 67, 53, 43, 29, 19, 51, 21, 54, 42, 41, 28, 31, 82, 23, 40, 35, 24, 21, 27, 29, 40, 17, 27, 34, 18, 39, 34, 44, 18, 39, 28, 30, 60, 58, 49, 61, 27, 59, 52, 42, 27, 41, 35, 50, 36, 21, 38, 46, 59, 55, 49, 59, 38, 22, 28, 47, 59, 37, 33, 34, 35, 55, 18, 48, 48, 70, 27, 43, 32, 62, 18, 50, 38, 18, 46, 26, 44, 32, 25, 55, 55, 25, 40, 29, 60, 45, 43, 24, 34, 47, 24, 47, 18, 50, 22, 49, 41, 68, 62, 25, 24, 44, 32, 49, 68, 25, 47, 35, 37, 57, 37, 34, 46, 21, 26, 45, 17, 36, 33, 27, 32, 24, 37, 53, 44, 43, 29, 47, 37, 43, 23, 50, 25, 40, 39, 41, 32, 19, 26, 30, 34, 41, 42, 45, 22, 25, 22, 46, 48, 45, 19, 31, 30, 40, 60, 52, 22, 25, 51, 32, 51, 42, 39, 24, 52, 58, 43, 31, 28, 31, 61, 26, 46, 62, 50, 18, 23, 21, 46, 38, 53, 53, 50, 26, 46, 44, 36, 20, 35, 40, 70, 57, 40, 18, 45, 52, 24, 42, 28, 32, 56, 44, 20, 32, 59, 21, 32, 49, 22, 51, 33, 23, 34, 53, 44, 60, 55, 57, 59, 26, 30, 49, 45, 29, 37, 21, 36, 18, 37, 19, 65, 30, 27, 59, 32, 25, 40, 21, 49, 49, 51, 48, 42, 53, 29, 21, 54, 66, 34, 39, 62, 30, 27, 25, 26, 45, 55, 26, 45, 61, 34, 26, 44, 45, 55, 20, 48, 33, 44, 32, 54, 40, 29, 56, 47, 22, 46, 24, 37, 49, 44, 21, 55, 23, 50, 44, 48, 38, 48, 39, 52, 63, 48, 37, 26, 50, 47, 19, 34, 29, 33, 53, 24, 39, 50, 31, 22, 42, 23, 22, 65, 34, 23, 38, 45, 19, 46, 23, 37, 59, 36, 35, 51, 42, 35, 20, 25, 41, 40, 36, 34, 44, 31, 41, 43, 43, 58, 19, 43, 50, 44, 46, 52, 52, 18, 29, 19, 34, 23, 64, 68, 42, 41, 41, 22, 67, 27, 25, 49, 28, 51, 23, 19, 72, 33, 53, 35, 31, 18, 45, 28, 22, 55, 45, 22, 37, 28, 19, 37, 50, 31, 40, 51, 56, 21, 40, 22, 47, 44, 30, 44, 41, 47, 26, 34, 47, 36, 21, 45, 33, 33, 58, 31, 32, 44, 24, 35, 43, 49, 37, 42, 31, 19, 41, 51, 51, 63, 39, 30, 62, 27, 32, 37, 47, 31, 42, 37, 60, 42, 38, 24, 34, 20, 29, 90, 55, 36, 49, 50, 17, 54, 26, 44, 28, 54, 21, 24, 31, 59, 41, 29, 50, 32, 42, 47, 44, 49, 61, 65, 45, 59, 30, 34, 24, 42, 37, 36, 21, 54, 34, 41, 18, 26, 23, 63, 34, 54, 37, 54, 22, 32, 42, 31, 48, 28, 31, 28, 24, 50, 26, 37, 24, 18, 32, 23, 33, 49, 75, 74, 26, 66, 34, 18, 33, 36, 44, 36, 53, 60, 28, 36, 35, 45, 23, 35, 34, 46, 58, 55, 63, 41, 45, 41, 42, 90, 41, 22, 53, 49, 51, 22, 23, 59, 40, 28, 39, 25, 48, 62, 44, 28, 62, 32, 59, 20, 40, 47, 60, 55, 18, 43, 31, 22, 56, 41, 24, 42, 53, 52, 42, 48, 35, 33, 20, 33, 30, 31, 29, 26, 61, 45, 29, 57, 33, 20, 26, 36, 23, 31, 47, 61, 35, 23, 20, 41, 39, 51, 61, 51, 36, 41, 34, 25, 37, 19, 66, 23, 30, 53, 25, 18, 51, 61, 53, 17, 61, 44, 38, 40, 32, 46, 50, 36, 30, 33, 36, 85, 62, 24, 48, 58, 45, 66, 37, 55, 39, 58, 50, 28, 34, 41, 36, 22, 35, 49, 21, 64, 41, 52, 32, 27, 48, 28, 24, 51, 32, 61, 60, 33, 42, 24, 82, 26, 18, 34, 57, 25, 34, 71, 35, 47, 50, 33, 38, 50, 45, 32, 39, 25, 20, 46, 40, 66, 30, 36, 57, 46, 27, 33, 58, 30, 26, 81, 32, 22, 31, 29, 35, 30, 34, 54, 37, 22, 34, 30, 38, 71, 45, 41, 72, 45, 31, 39, 37, 43, 65, 43, 43, 32, 43, 32, 53, 22, 27, 40, 58, 22, 52], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Age\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('144b5122-3e60-4919-a8b6-19c8dd40c126');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxxlyH57TAaR",
        "colab_type": "code",
        "outputId": "303bf45d-1eab-49ca-84e8-174bc912a334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# Education Num\n",
        "fig = px.box(df_3_adult, y=\"Education-Num\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8e132a7b-3dfd-4fcc-8188-f21173a90f6d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8e132a7b-3dfd-4fcc-8188-f21173a90f6d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8e132a7b-3dfd-4fcc-8188-f21173a90f6d',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Education-Num=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x0\": \" \", \"xaxis\": \"x\", \"y\": [13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11, 4, 9, 9, 7, 14, 16, 9, 5, 7, 9, 13, 9, 10, 9, 9, 12, 10, 13, 10, 10, 7, 10, 9, 10, 12, 5, 13, 13, 9, 9, 13, 9, 14, 11, 11, 10, 9, 15, 13, 9, 10, 3, 11, 9, 9, 13, 4, 9, 16, 10, 9, 10, 9, 10, 10, 10, 13, 13, 10, 10, 9, 12, 6, 7, 4, 9, 13, 9, 9, 9, 9, 9, 14, 5, 16, 11, 10, 10, 9, 13, 10, 16, 10, 12, 9, 14, 13, 14, 10, 9, 9, 6, 7, 9, 9, 13, 15, 9, 10, 9, 5, 10, 11, 10, 9, 10, 13, 13, 13, 10, 13, 10, 12, 9, 10, 12, 9, 9, 13, 12, 14, 9, 11, 10, 10, 9, 10, 10, 11, 10, 9, 10, 9, 7, 10, 10, 9, 13, 9, 9, 9, 13, 10, 9, 13, 2, 9, 14, 9, 14, 10, 12, 14, 7, 11, 9, 9, 10, 13, 13, 9, 9, 9, 9, 13, 10, 10, 10, 4, 10, 9, 13, 13, 14, 16, 10, 10, 13, 10, 13, 3, 10, 14, 14, 9, 14, 9, 13, 10, 9, 7, 13, 9, 9, 7, 9, 9, 10, 9, 4, 13, 12, 13, 13, 6, 10, 2, 9, 9, 1, 9, 6, 9, 11, 9, 7, 9, 10, 10, 15, 9, 10, 13, 14, 9, 13, 9, 12, 10, 10, 9, 9, 9, 12, 9, 15, 13, 6, 10, 10, 9, 9, 9, 11, 4, 13, 9, 7, 10, 7, 10, 10, 9, 13, 9, 9, 5, 14, 14, 9, 9, 10, 10, 10, 10, 13, 10, 12, 13, 9, 14, 15, 10, 13, 4, 13, 9, 13, 10, 13, 9, 13, 14, 9, 9, 13, 10, 9, 9, 10, 9, 12, 12, 12, 10, 13, 14, 10, 13, 10, 10, 10, 3, 10, 9, 9, 10, 9, 7, 14, 9, 11, 10, 9, 9, 10, 13, 11, 6, 9, 3, 9, 14, 9, 9, 12, 11, 9, 9, 10, 13, 7, 11, 12, 13, 13, 9, 13, 7, 13, 9, 7, 11, 9, 5, 6, 14, 13, 13, 9, 9, 10, 12, 13, 9, 10, 7, 11, 9, 9, 10, 9, 10, 14, 9, 9, 3, 10, 10, 10, 9, 9, 10, 10, 9, 9, 13, 12, 13, 9, 10, 10, 9, 13, 13, 10, 10, 13, 11, 4, 14, 9, 9, 10, 10, 10, 9, 10, 16, 10, 8, 2, 7, 7, 9, 10, 7, 13, 9, 6, 9, 14, 9, 9, 14, 9, 7, 13, 12, 10, 14, 9, 10, 10, 10, 13, 9, 9, 10, 5, 10, 9, 9, 9, 7, 9, 10, 9, 12, 9, 11, 9, 10, 9, 10, 12, 9, 13, 2, 13, 9, 9, 10, 14, 13, 10, 10, 9, 9, 11, 13, 5, 10, 6, 9, 12, 10, 13, 9, 8, 10, 2, 9, 13, 13, 9, 9, 12, 9, 13, 9, 10, 3, 10, 9, 9, 9, 10, 10, 13, 13, 10, 10, 9, 9, 11, 3, 11, 10, 9, 11, 10, 10, 10, 13, 13, 9, 10, 10, 10, 10, 13, 10, 9, 14, 7, 9, 9, 12, 15, 8, 2, 11, 13, 9, 10, 16, 10, 9, 10, 9, 6, 10, 9, 10, 10, 9, 7, 9, 11, 7, 9, 9, 9, 7, 13, 13, 10, 9, 13, 9, 14, 4, 9, 14, 9, 9, 7, 11, 16, 13, 8, 12, 13, 10, 10, 9, 9, 9, 13, 10, 10, 7, 6, 13, 9, 9, 4, 13, 13, 9, 10, 15, 10, 13, 13, 10, 4, 5, 12, 9, 14, 9, 9, 6, 14, 9, 9, 13, 13, 9, 9, 9, 13, 15, 13, 7, 5, 11, 5, 9, 9, 9, 10, 13, 10, 6, 10, 13, 9, 9, 13, 13, 3, 16, 9, 9, 10, 10, 10, 16, 9, 9, 9, 13, 10, 9, 16, 10, 9, 10, 5, 9, 4, 9, 8, 14, 10, 14, 10, 12, 10, 9, 7, 5, 10, 7, 7, 13, 9, 9, 8, 10, 9, 9, 9, 11, 9, 10, 7, 9, 4, 13, 14, 9, 10, 10, 10, 10, 13, 12, 14, 14, 13, 9, 13, 13, 10, 10, 16, 13, 8, 10, 9, 7, 13, 10, 13, 11, 13, 9, 9, 9, 10, 13, 9, 9, 13, 9, 13, 13, 9, 10, 9, 7, 10, 10, 11, 11, 10, 10, 10, 13, 9, 13, 10, 9, 14, 11, 8, 13, 9, 10, 7, 13, 13, 13, 6, 9, 9, 11, 9, 13, 9, 9, 9, 9, 9, 16, 13, 6, 10, 10, 13, 9, 10, 3, 9, 11, 9, 11, 10, 14, 13, 10, 10, 11, 11, 10, 13, 10, 9, 8, 9, 9, 11, 13, 10, 9, 9, 10, 9, 9, 14, 13, 12, 9, 7, 9, 7, 9, 14, 13, 13, 9, 9, 10, 10, 10, 6, 9, 11, 10, 9, 9, 11, 14, 11, 9, 9, 9, 9, 9, 10, 13, 7, 10, 10, 9, 7, 9, 9, 13, 13, 9, 7, 10, 10, 13, 9, 10, 9, 13, 13, 9, 14, 11, 9, 13, 16, 9, 9, 9, 13, 13, 7, 9, 7, 10, 9, 10, 6, 7, 10, 13, 13, 10, 14, 13, 3, 13, 9, 11, 9, 9, 5, 13, 9, 13, 10, 10, 13, 10, 9, 11, 10, 13, 11, 13, 15, 14, 10, 13, 13, 9, 4, 5, 9, 12, 9, 9, 10, 13, 9, 10, 9, 10, 6, 10, 9, 13, 14, 9, 9, 10, 10, 10, 3, 10, 9, 9, 9, 14, 10, 2, 9, 10, 1, 9, 7, 9, 14, 15, 9, 13, 12, 9, 9, 9, 9, 10, 10, 13, 10, 10, 9, 14, 4, 9, 10, 6, 10, 6, 13, 12, 10, 9, 13, 6, 9, 9, 10, 9, 13, 11, 9, 13, 13, 9, 9, 10, 10, 14, 9, 9, 9, 12, 9, 9, 9, 11, 9, 9, 9, 10, 13, 11, 9, 9, 9, 9, 14, 13, 10, 13, 10, 16, 9, 13, 9, 13, 9, 9, 9, 10, 9, 11, 9, 10, 14, 4, 11, 9, 9, 9, 10, 14, 9, 10, 9, 9, 13, 9, 6, 14, 11, 12, 10, 9, 4, 9, 5, 9, 9, 7, 9, 10, 10, 13, 10, 15, 12, 10, 10, 13, 9, 13, 13, 10, 10, 13, 8, 11, 14, 9, 9, 9, 9, 12, 10, 10, 15, 8, 13, 12, 13, 9, 9, 16, 7, 10, 13, 13, 9, 11, 13, 9, 9, 10, 10, 11, 9, 13, 10, 9, 12, 13, 16, 9, 10, 13, 6, 9, 9, 10, 4, 13, 5, 13, 12, 9, 13, 9, 13, 11, 14, 13, 13, 9, 9, 11, 10, 9, 13, 13, 10, 9, 10, 13, 10, 11, 9, 9, 10, 13, 10, 9, 9, 11, 10, 10, 14, 9, 9, 2, 10, 10, 9, 13, 10, 14, 10, 9, 9, 9, 9, 10, 10, 9, 9, 10, 9, 14, 13, 9, 11, 9, 13, 12, 13, 9, 13, 10, 15, 9, 9, 7, 14, 11, 9, 10, 11, 9, 9, 7, 9, 13, 13, 9, 9, 14, 10, 9, 13, 13, 11, 10, 9, 13, 10, 13, 12, 14, 9, 9, 9, 13, 13, 13, 10, 14, 9, 13, 10, 9, 10, 10, 10, 16, 10, 13, 10, 9, 13, 15, 9, 9, 10, 9, 14, 9, 13, 10, 9, 15, 13, 9, 13, 9, 7, 10, 9, 9, 9, 10, 14, 9, 9, 9, 10, 10, 15, 9, 9, 9, 14, 10, 9, 9, 10, 9, 13, 9, 13, 10, 9, 9, 11, 9, 10, 9, 13, 10, 10, 13, 13, 10, 12, 9, 10, 13, 13, 13, 9, 9, 9, 12, 9, 10, 9, 10, 13, 6, 13, 9, 8, 2, 9, 14, 9, 3, 10, 12, 10, 9, 10, 9, 10, 15, 11, 13, 11, 3, 9, 9, 13, 13, 10, 9, 10, 9, 16, 12, 13, 13, 9, 10, 9, 10, 9, 6, 9, 9, 9, 7, 9, 10, 9, 15, 11, 10, 13, 9, 13, 10, 15, 14, 10, 9, 9, 13, 13, 7, 13, 4, 10, 13, 9, 10, 3, 9, 6, 13, 9, 10, 9, 9, 9, 10, 11, 9, 9, 10, 9, 6, 12, 9, 14, 13, 11, 9, 9, 9, 14, 13, 10, 13, 13, 10, 9, 10, 9, 10, 10, 7, 13, 4, 9, 9, 6, 6, 9, 14, 6, 9, 9, 12, 9, 9, 9, 9, 7, 12, 9, 9, 9, 9, 13, 10, 6, 10, 9, 13, 13, 10, 10, 11, 12, 7, 9, 7, 13, 5, 9, 9, 11, 9, 11, 10, 10, 6, 13, 12, 14, 9, 9, 9, 9, 11, 9, 11, 10, 9, 10, 10, 9, 7, 6, 14, 9, 9, 9, 10, 9, 15, 9, 6, 10, 9, 6, 9, 10, 14, 11, 12, 15, 13, 13, 14, 10, 9, 9, 11, 7, 10, 9, 11, 9, 9, 13, 6, 9, 9, 9, 14, 10, 9, 9, 9, 9, 7, 7, 13, 10, 10, 13, 10, 10, 13, 14, 12, 9, 9, 10, 10, 10, 11, 13, 11, 13, 13, 9, 13, 13, 5, 13, 5, 10, 10, 13, 10, 9, 9, 13, 9, 9, 6, 14, 6, 10, 12, 10, 9, 6, 9, 9, 15, 9, 10, 3, 13, 6, 13, 10, 6, 9, 6, 4, 10, 16, 8, 13, 9, 9, 10, 8, 10, 13, 4, 9, 10, 10, 7, 13, 10, 13, 12, 10, 10, 6, 11, 9, 10, 13, 9, 9, 13, 13, 9, 9, 10, 10, 9, 9, 13, 10, 13, 10, 14, 13, 9, 9, 13, 7, 13, 7, 9, 10, 9, 9, 9, 9, 9, 6, 10, 8, 12, 14, 9, 13, 9, 15, 10, 7, 9, 10, 9, 9, 10, 9, 15, 9, 13, 9, 12, 7, 9, 12, 3, 2, 10, 6, 9, 13, 9, 9, 13, 14, 14, 15, 9, 10, 9, 13, 6, 10, 5, 9, 9, 9, 13, 9, 9, 10, 6, 9, 13, 13, 4, 9, 9, 10, 7, 9, 15, 12, 10, 4, 10, 12, 14, 9, 10, 10, 9, 9, 9, 13, 9, 9, 9, 9, 9, 5, 16, 9, 7, 9, 9, 10, 9, 9, 10, 9, 14, 10, 10, 10, 9, 10, 9, 9, 4, 13, 14, 9, 10, 8, 13, 9, 10, 13, 14, 13, 9, 9, 9, 13, 10, 9, 10, 9, 9, 14, 11, 14, 10, 9, 13, 11, 13, 13, 9, 13, 9, 13, 7, 4, 13, 10, 10, 13, 9, 9, 7, 9, 9, 9, 6, 9, 13, 13, 9, 9, 10, 10, 9, 13, 6, 14, 13, 9, 7, 9, 10, 9, 10, 11, 6, 9, 6, 7, 9, 6, 10, 13, 9, 6, 13, 12, 13, 14, 14, 12, 3, 5, 12, 5, 14, 10, 9, 13, 12, 9, 4, 13, 10, 9, 15, 9, 14, 7, 9, 4, 9, 9, 11, 13, 7, 9, 9, 9, 9, 9, 13, 4, 4, 10, 10, 9, 13, 9, 9, 10, 9, 10, 9, 10, 14, 9, 9, 14, 9, 10, 13, 10, 10, 13, 11, 4, 9, 10, 10, 13, 9, 9, 14, 5, 13, 9, 13, 14, 9, 9, 12, 9, 10, 13, 9, 9, 13, 10, 9, 5, 13, 6, 9, 10, 13, 10, 10, 10, 13, 13, 10, 9, 13, 9, 13, 10, 9, 9, 10, 9, 10, 9, 10, 9, 14, 10, 13, 13, 7, 9, 13, 10, 4, 9, 9, 10, 9, 14, 14, 6, 5, 7, 9, 15, 2, 10, 14, 13, 11, 13, 13, 9, 14, 10, 10, 13, 13, 13, 9, 9, 14, 10, 7, 4, 9, 9, 13, 13, 13, 13, 9, 10, 6, 9, 9, 7, 14, 14, 9, 13, 14, 13, 9, 13, 9, 6, 7, 9, 9, 13, 10, 9, 14, 14, 13, 13, 9, 10, 9, 6, 9, 10, 7, 14, 9, 9, 13, 13, 10, 7, 6, 15, 9, 13, 9, 10, 9, 10, 12, 9, 10, 9, 9, 10, 9, 13, 10, 9, 14, 13, 9, 9, 10, 9, 5, 13, 9, 10, 4, 9, 10, 13, 10, 10, 13, 13, 15, 14, 9, 14, 7, 12, 16, 10, 12, 13, 13, 11, 9, 10, 14, 12, 10, 11, 9, 7, 9, 10, 6, 10, 10, 13, 4, 10, 13, 13, 6, 9, 10, 9, 13, 10, 13, 7, 10, 9, 4, 11, 9, 9, 8, 9, 10, 13, 14, 10, 9, 9, 9, 13, 10, 13, 7, 9, 10, 10, 9, 9, 10, 9, 10, 9, 13, 10, 9, 9, 14, 12, 9, 9, 9, 12, 13, 10, 13, 10, 10, 13, 9, 10, 14, 13, 11, 9, 10, 10, 8, 9, 10, 9, 10, 9, 10, 10, 9, 13, 10, 9, 14, 16, 9, 9, 9, 9, 13, 3, 9, 10, 9, 9, 10, 7, 14, 9, 14, 11, 9, 10, 10, 9, 13, 14, 9, 10, 14, 9, 9, 7, 9, 13, 10, 7, 10, 10, 14, 6, 7, 10, 13, 14, 3, 9, 10, 7, 13, 9, 10, 9, 13, 11, 6, 9, 9, 9, 5, 9, 13, 10, 9, 12, 13, 12, 13, 14, 4, 14, 9, 14, 13, 10, 10, 7, 10, 8, 5, 13, 9, 10, 13, 13, 9, 11, 12, 13, 13, 9, 9, 9, 9, 13, 9, 10, 9, 9, 3, 9, 10, 10, 14, 9, 9, 6, 9, 10, 4, 7, 13, 9, 13, 12, 4, 13, 13, 9, 11, 4, 11, 9, 13, 10, 10, 15, 4, 11, 12, 10, 10, 14, 9, 10, 13, 13, 10, 16, 10, 9, 11, 13, 9, 14, 13, 9, 13, 11, 10, 11, 10, 8, 9, 9, 13, 9, 11, 10, 10, 9, 10, 12, 9, 6, 15, 10, 7, 9, 13, 16, 13, 14, 13, 10, 9, 9, 9, 7, 10, 9, 11, 9, 13, 9, 10, 13, 9, 10, 10, 13, 13, 10, 10, 13, 9, 4, 10, 13, 12, 13, 10, 13, 9, 14, 13, 10, 10, 9, 10, 14, 13, 10, 13, 7, 10, 10, 11, 7, 7, 10, 10, 7, 6, 9, 14, 10, 11, 9, 7, 14, 13, 11, 5, 8, 10, 12, 13, 13, 13, 10, 12, 10, 9, 7, 6, 13, 15, 10, 9, 9, 10, 11, 14, 9, 5, 9, 13, 5, 10, 16, 9, 9, 9, 10, 10, 10, 7, 9, 4, 10, 13, 13, 13, 4, 13, 9, 13, 10, 9, 10, 8, 14, 10, 9, 9, 10, 10, 4, 13, 10, 11, 10, 7, 9, 10, 6, 10, 10, 10, 13, 14, 5, 10, 9, 8, 13, 9, 7, 13, 5, 9, 15, 10, 9, 9, 9, 9, 15, 13, 13, 10, 14, 9, 10, 10, 9, 5, 10, 12, 9, 10, 3, 9, 9, 6, 13, 11, 6, 9, 9, 9, 9, 9, 10, 11, 11, 12, 13, 11, 9, 9, 9, 9, 13, 9, 14, 13, 11, 13, 9, 7, 11, 6, 13, 9, 10, 13, 7, 13, 16, 9, 5, 9, 4, 10, 9, 10, 9, 7, 9, 13, 10, 10, 13, 9, 10, 13, 3, 9, 4, 9, 10, 15, 9, 12, 9, 10, 13, 9, 10, 7, 8, 13, 9, 13, 10, 14, 11, 11, 4, 11, 9, 13, 13, 14, 10, 10, 8, 10, 9, 10, 9, 10, 9, 9, 11, 10, 10, 9, 10, 9, 10, 10, 11, 16, 10, 9, 10, 9, 9, 10, 13, 10, 16, 10, 11, 5, 9, 9, 10, 9, 9, 14, 9, 10, 9, 10, 5, 7, 13, 6, 9, 9, 14, 9, 9, 2, 6, 13, 2, 9, 7, 13, 10, 13, 9, 10, 9, 14, 9, 12, 6, 11, 9, 12, 13, 9, 11, 9, 6, 9, 10, 13, 10, 10, 7, 9, 9, 9, 13, 15, 3, 9, 11, 9, 9, 14, 9, 13, 12, 6, 9, 11, 9, 9, 7, 13, 10, 13, 7, 14, 10, 13, 11, 3, 7, 13, 10, 11, 9, 10, 9, 4, 10, 9, 9, 14, 13, 10, 10, 13, 9, 10, 9, 15, 9, 9, 9, 11, 13, 13, 9, 3, 10, 10, 9, 2, 15, 7, 4, 9, 10, 10, 10, 10, 13, 10, 13, 9, 13, 10, 9, 14, 13, 9, 5, 13, 9, 13, 10, 9, 10, 9, 9, 14, 6, 9, 13, 7, 10, 9, 9, 15, 9, 9, 10, 10, 9, 10, 7, 11, 14, 13, 13, 9, 10, 10, 7, 9, 13, 14, 13, 9, 10, 9, 9, 15, 10, 10, 9, 10, 9, 9, 10, 9, 13, 9, 9, 9, 10, 9, 10, 7, 10, 5, 11, 9, 4, 13, 4, 10, 14, 9, 10, 9, 9, 7, 9, 9, 9, 10, 9, 13, 9, 10, 10, 13, 10, 9, 13, 16, 13, 9, 9, 9, 10, 10, 9, 14, 3, 12, 10, 10, 14, 10, 10, 9, 9, 9, 13, 9, 15, 13, 7, 13, 13, 7, 10, 14, 12, 6, 14, 9, 9, 13, 10, 9, 9, 10, 13, 3, 7, 2, 14, 9, 13, 10, 6, 8, 13, 9, 9, 10, 13, 14, 9, 10, 10, 15, 13, 7, 9, 10, 6, 5, 15, 15, 13, 10, 9, 10, 13, 9, 10, 10, 12, 10, 9, 9, 10, 11, 9, 7, 6, 15, 14, 2, 8, 13, 10, 9, 10, 4, 13, 9, 5, 9, 13, 14, 12, 9, 3, 7, 4, 7, 9, 10, 13, 14, 1, 9, 9, 9, 13, 9, 9, 10, 15, 9, 13, 10, 9, 6, 14, 9, 9, 13, 9, 10, 10, 7, 5, 9, 8, 15, 9, 10, 9, 14, 7, 14, 10, 9, 13, 9, 9, 4, 9, 11, 9, 12, 9, 9, 13, 6, 10, 9, 11, 15, 13, 10, 10, 9, 7, 10, 9, 10, 9, 13, 7, 9, 1, 9, 13, 9, 13, 10, 9, 13, 9, 10, 13, 14, 10, 6, 13, 4, 11, 9, 9, 10, 9, 9, 13, 10, 13, 13, 9, 9, 7, 10, 9, 10, 9, 10, 14, 5, 10, 9, 10, 13, 7, 10, 10, 14, 10, 10, 11, 6, 13, 7, 13, 9, 9, 9, 4, 4, 10, 10, 12, 10, 11, 10, 9, 9, 9, 14, 13, 16, 9, 9, 13, 13, 9, 10, 16, 15, 13, 6, 9, 9, 9, 15, 10, 14, 9, 9, 10, 9, 9, 14, 13, 9, 8, 10, 15, 13, 11, 10, 9, 3, 10, 13, 10, 12, 15, 13, 10, 9, 9, 9, 9, 6, 10, 9, 10, 12, 10, 9, 9, 9, 10, 13, 10, 7, 9, 10, 9, 9, 10, 10, 13, 14, 10, 13, 15, 9, 10, 9, 10, 10, 9, 9, 10, 13, 10, 15, 5, 13, 13, 13, 9, 13, 10, 8, 9, 9, 10, 10, 10, 15, 13, 16, 10, 10, 13, 16, 9, 13, 10, 10, 13, 12, 3, 13, 10, 10, 9, 11, 10, 14, 9, 13, 10, 12, 7, 10, 10, 9, 13, 9, 10, 6, 9, 10, 9, 9, 9, 14, 13, 9, 6, 13, 10, 9, 9, 13, 16, 13, 9, 9, 15, 10, 9, 11, 14, 9, 9, 9, 15, 6, 10, 14, 13, 9, 10, 14, 10, 10, 10, 15, 13, 9, 9, 8, 9, 9, 12, 9, 13, 13, 13, 13, 9, 10, 6, 4, 10, 8, 9, 9, 3, 9, 9, 10, 9, 5, 9, 13, 13, 4, 13, 13, 6, 9, 12, 4, 13, 9, 10, 10, 13, 13, 13, 7, 13, 9, 10, 13, 15, 13, 6, 13, 9, 4, 13, 10, 2, 9, 9, 4, 9, 10, 13, 10, 10, 10, 9, 5, 13, 10, 11, 9, 13, 13, 7, 10, 13, 9, 9, 11, 13, 9, 9, 12, 13, 9, 9, 10, 12, 13, 13, 9, 13, 9, 6, 14, 6, 9, 13, 10, 14, 9, 9, 13, 10, 16, 10, 9, 15, 9, 3, 10, 12, 7, 7, 4, 13, 9, 9, 10, 9, 9, 10, 10, 12, 7, 9, 13, 9, 13, 10, 9, 9, 12, 10, 13, 9, 13, 7, 8, 10, 10, 10, 9, 10, 12, 10, 10, 14, 9, 10, 10, 13, 13, 8, 9, 9, 9, 9, 9, 9, 13, 9, 9, 9, 9, 9, 11, 10, 7, 11, 3, 9, 13, 9, 10, 9, 9, 7, 13, 11, 10, 9, 14, 13, 7, 6, 10, 10, 13, 7, 13, 15, 9, 15, 10, 9, 9, 6, 10, 6, 13, 9, 9, 5, 13, 11, 11, 9, 6, 9, 9, 13, 16, 16, 10, 13, 11, 9, 15, 13, 9, 13, 9, 10, 9, 9, 7, 3, 10, 11, 10, 12, 5, 14, 10, 9, 13, 10, 10, 9, 10, 13, 10, 10, 9, 11, 13, 3, 12, 12, 14, 10, 10, 13, 6, 9, 9, 9, 9, 9, 9, 9, 13, 10, 9, 10, 6, 14, 13, 1, 8, 9, 13, 11, 13, 12, 9, 13, 10, 10, 10, 13, 9, 9, 4, 9, 7, 13, 13, 14, 4, 13, 10, 6, 9, 13, 10, 13, 10, 7, 9, 13, 14, 11, 4, 9, 14, 9, 13, 9, 9, 13, 11, 5, 10, 11, 9, 13, 10, 7, 10, 9, 14, 13, 13, 15, 9, 13, 16, 10, 14, 10, 10, 9, 5, 9, 10, 9, 13, 10, 9, 13, 9, 9, 10, 6, 10, 9, 9, 10, 9, 6, 9, 10, 9, 6, 8, 10, 9, 16, 9, 12, 9, 14, 11, 13, 10, 13, 10, 9, 6, 9, 13, 10, 10, 14, 8, 11, 7, 9, 13, 13, 13, 9, 9, 9, 9, 10, 10, 13, 9, 9, 9, 4, 9, 9, 4, 16, 10, 7, 16, 16, 13, 9, 10, 9, 10, 15, 9, 9, 9, 10, 10, 10, 13, 1, 9, 9, 11, 10, 9, 9, 10, 7, 13, 16, 4, 9, 6, 9, 10, 10, 9, 11, 9, 13, 13, 11, 10, 13, 9, 9, 9, 9, 13, 9, 5, 9, 13, 9, 14, 13, 9, 13, 9, 10, 4, 9, 4, 8, 10, 9, 13, 13, 12, 6, 11, 5, 9, 9, 13, 13, 10, 9, 13, 14, 9, 9, 15, 9, 13, 10, 9, 9, 10, 9, 9, 13, 13, 13, 14, 3, 10, 9, 7, 14, 12, 13, 3, 9, 14, 13, 7, 13, 9, 10, 9, 9, 9, 9, 4, 10, 9, 9, 9, 9, 9, 10, 10, 5, 9, 10, 9, 10, 10, 9, 10, 14, 7, 10, 10, 9, 13, 9, 10, 11, 9, 13, 10, 13, 13, 10, 10, 9, 13, 10, 9, 14, 10, 9, 11, 5, 9, 11, 10, 9, 14, 9, 10, 11, 7, 9, 5, 9, 9, 10, 9, 9, 9, 9, 13, 10, 14, 10, 9, 9, 13, 13, 9, 13, 10, 9, 4, 13, 15, 10, 11, 10, 12, 9, 10, 13, 9, 13, 9, 11, 10, 10, 10, 10, 10, 10, 9, 10, 9, 9, 9, 9, 10, 12, 10, 9, 13, 13, 9, 12, 10, 10, 4, 9, 13, 6, 7, 6, 9, 10, 9, 7, 10, 15, 13, 13, 10, 13, 10, 9, 9, 13, 10, 14, 14, 10, 10, 13, 11, 9, 12, 10, 9, 6, 10, 10, 14, 9, 13, 7, 10, 13, 9, 15, 9, 10, 9, 11, 9, 10, 10, 13, 13, 10, 9, 10, 10, 11, 10, 10, 8, 9, 9, 10, 10, 13, 9, 13, 9, 11, 11, 9, 9, 10, 9, 12, 14, 6, 9, 9, 9, 10, 4, 4, 9, 11, 3, 10, 9, 10, 6, 14, 9, 12, 13, 9, 11, 10, 9, 9, 9, 9, 10, 10, 10, 9, 9, 13, 10, 10, 9, 7, 4, 13, 9, 13, 9, 9, 9, 13, 10, 4, 9, 13, 10, 9, 13, 10, 7, 9, 9, 12, 10, 10, 14, 9, 9, 9, 3, 9, 9, 14, 7, 11, 12, 10, 9, 13, 2, 13, 9, 10, 13, 9, 9, 9, 9, 9, 13, 10, 10, 9, 9, 14, 11, 10, 10, 10, 9, 11, 9, 13, 9, 14, 7, 7, 12, 13, 5, 9, 9, 10, 13, 9, 13, 13, 14, 12, 9, 13, 6, 5, 9, 9, 13, 9, 10, 9, 10, 9, 13, 9, 10, 9, 9, 6, 4, 13, 9, 13, 10, 15, 10, 13, 9, 9, 12, 10, 10, 4, 10, 9, 10, 9, 10, 16, 12, 9, 10, 11, 9, 6, 9, 10, 10, 9, 9, 9, 9, 11, 12, 10, 15, 13, 6, 9, 9, 13, 9, 13, 9, 10, 11, 9, 9, 10, 11, 13, 13, 9, 13, 13, 15, 10, 11, 12, 10, 13, 7, 9, 9, 9, 9, 9, 7, 12, 9, 7, 10, 9, 13, 9, 13, 9, 9, 10, 9, 10, 9, 4, 9, 10, 9, 6, 13, 13, 10, 7, 9, 14, 14, 6, 9, 9, 14, 7, 9, 13, 9, 5, 10, 10, 10, 13, 1, 13, 13, 9, 10, 10, 11, 13, 10, 11, 9, 13, 9, 10, 9, 10, 5, 13, 9, 9, 15, 13, 10, 3, 10, 10, 9, 9, 9, 13, 10, 10, 9, 9, 8, 10, 9, 9, 13, 9, 10, 11, 10, 9, 10, 10, 13, 16, 4, 10, 9, 9, 10, 9, 5, 9, 10, 11, 12, 7, 10, 10, 11, 6, 9, 14, 13, 7, 9, 9, 13, 9, 13, 9, 10, 9, 9, 13, 11, 7, 10, 9, 4, 8, 10, 9, 10, 8, 9, 5, 10, 9, 8, 7, 11, 9, 13, 10, 9, 9, 13, 14, 10, 12, 9, 4, 9, 9, 14, 10, 10, 6, 13, 9, 10, 10, 7, 10, 9, 9, 9, 11, 10, 10, 11, 5, 13, 10, 13, 16, 14, 9, 10, 8, 12, 10, 7, 12, 9, 13, 9, 7, 13, 9, 9, 10, 9, 13, 9, 9, 9, 12, 10, 10, 9, 10, 9, 13, 9, 13, 13, 9, 10, 9, 9, 10, 9, 13, 10, 13, 10, 13, 9, 11, 4, 10, 9, 9, 9, 9, 13, 6, 10, 9, 8, 11, 10, 15, 9, 9, 14, 9, 10, 10, 9, 5, 2, 10, 9, 9, 10, 9, 9, 10, 14, 15, 10, 9, 10, 10, 13, 9, 10, 10, 9, 13, 10, 11, 13, 2, 11, 9, 12, 9, 4, 16, 10, 10, 10, 10, 13, 15, 13, 10, 9, 10, 14, 9, 9, 9, 9, 10, 14, 9, 9, 9, 10, 16, 6, 10, 9, 12, 13, 10, 12, 10, 9, 11, 13, 14, 9, 10, 10, 10, 9, 10, 6, 13, 9, 10, 10, 5, 7, 13, 13, 9, 10, 9, 10, 10, 9, 9, 9, 13, 10, 9, 10, 9, 7, 8, 13, 10, 9, 7, 10, 13, 10, 10, 13, 10, 10, 9, 9, 9, 14, 12, 10, 9, 5, 10, 9, 10, 13, 7, 9, 10, 9, 10, 13, 9, 9, 13, 9, 9, 9, 12, 16, 11, 10, 9, 14, 16, 9, 15, 15, 13, 9, 7, 14, 9, 10, 10, 6, 10, 9, 13, 9, 9, 4, 14, 9, 9, 13, 10, 10, 10, 9, 6, 14, 10, 10, 11, 8, 6, 9, 9, 6, 15, 14, 9, 10, 13, 15, 13, 13, 10, 13, 9, 9, 6, 10, 11, 12, 13, 7, 13, 7, 10, 13, 10, 4, 11, 10, 7, 9, 13, 13, 13, 12, 9, 10, 10, 9, 10, 13, 13, 10, 13, 9, 9, 9, 9, 12, 9, 6, 10, 13, 2, 9, 4, 9, 8, 6, 10, 10, 9, 9, 9, 6, 13, 13, 9, 14, 11, 9, 11, 9, 6, 12, 13, 9, 12, 14, 10, 9, 9, 8, 10, 7, 9, 13, 10, 9, 10, 4, 9, 11, 10, 13, 10, 14, 9, 9, 9, 14, 7, 9, 9, 13, 9, 13, 11, 10, 9, 3, 12, 10, 10, 12, 9, 10, 10, 5, 13, 7, 11, 11, 12, 9, 10, 16, 14, 13, 9, 7, 9, 10, 11, 9, 9, 10, 9, 9, 9, 9, 15, 5, 9, 9, 13, 11, 13, 10, 14, 7, 13, 7, 9, 15, 10, 4, 9, 10, 8, 10, 6, 4, 10, 12, 12, 10, 6, 13, 10, 9, 4, 5, 10, 5, 9, 9, 10, 10, 9, 9, 9, 9, 9, 5, 12, 13, 10, 10, 10, 13, 11, 11, 9, 9, 9, 7, 9, 13, 10, 9, 13, 9, 10, 9, 7, 9, 16, 9, 13, 10, 11, 12, 12, 10, 14, 9, 10, 10, 9, 13, 10, 10, 10, 9, 9, 4, 5, 6, 8, 6, 9, 5, 11, 9, 9, 6, 10, 13, 10, 11, 10, 9, 10, 9, 9, 9, 2, 13, 9, 13, 9, 10, 13, 9, 10, 13, 13, 10, 9, 9, 4, 9, 11, 10, 9, 10, 10, 9, 13, 16, 14, 13, 8, 9, 13, 14, 12, 13, 10, 13, 9, 4, 9, 13, 13, 10, 9, 6, 13, 13, 10, 13, 15, 9, 9, 10, 14, 10, 7, 9, 10, 10, 12, 13, 10, 10, 4, 9, 14, 13, 13, 10, 12, 10, 9, 9, 14, 9, 10, 9, 9, 9, 9, 13, 11, 13, 9, 13, 8, 14, 9, 12, 9, 16, 9, 7, 13, 10, 13, 13, 9, 9, 9, 12, 13, 13, 10, 12, 9, 13, 13, 9, 9, 10, 10, 9, 7, 11, 3, 9, 9, 10, 10, 6, 10, 9, 6, 11, 9, 10, 9, 10, 4, 9, 12, 10, 13, 13, 9, 13, 10, 9, 13, 9, 9, 13, 10, 9, 13, 9, 9, 11, 9, 13, 9, 9, 10, 10, 15, 9, 9, 13, 10, 14, 13, 10, 10, 12, 7, 13, 6, 10, 10, 13, 9, 12, 13, 9, 10, 10, 12, 9, 14, 9, 9, 13, 15, 14, 9, 13, 10, 9, 6, 9, 13, 10, 11, 14, 9, 13, 10, 13, 9, 7, 11, 10, 9, 10, 9, 10, 10, 10, 9, 13, 10, 10, 14, 9, 5, 9, 10, 12, 9, 9, 7, 8, 10, 9, 9, 9, 6, 11, 10, 10, 13, 9, 10, 13, 11, 9, 10, 10, 9, 9, 13, 14, 10, 11, 7, 12, 12, 14, 10, 13, 9, 14, 9, 10, 9, 9, 16, 6, 15, 9, 14, 9, 13, 9, 5, 13, 10, 15, 9, 10, 8, 13, 10, 11, 12, 5, 9, 10, 13, 3, 9, 9, 9, 10, 15, 13, 9, 9, 9, 9, 7, 13, 13, 5, 11, 3, 14, 14, 10, 9, 10, 9, 10, 3, 10, 9, 9, 10, 9, 9, 13, 10, 9, 8, 9, 13, 10, 15, 9, 9, 12, 13, 11, 10, 10, 9, 9, 13, 14, 10, 9, 10, 4, 14, 9, 9, 9, 13, 9, 10, 10, 10, 11, 10, 10, 7, 9, 10, 10, 2, 9, 7, 10, 16, 10, 9, 4, 16, 9, 9, 9, 13, 9, 4, 10, 9, 13, 10, 9, 9, 9, 9, 9, 10, 2, 9, 10, 9, 9, 10, 9, 12, 15, 10, 4, 14, 10, 9, 10, 12, 10, 12, 10, 13, 10, 9, 14, 9, 9, 4, 5, 13, 13, 9, 14, 9, 9, 14, 8, 9, 9, 11, 10, 14, 6, 10, 13, 9, 7, 4, 12, 9, 10, 11, 13, 9, 13, 6, 9, 9, 10, 5, 5, 9, 9, 10, 10, 9, 13, 10, 9, 9, 14, 10, 13, 9, 9, 14, 9, 11, 9, 10, 9, 9, 5, 9, 9, 13, 13, 10, 9, 10, 9, 14, 13, 7, 3, 15, 16, 10, 13, 11, 7, 9, 9, 8, 13, 9, 9, 10, 16, 13, 10, 12, 13, 14, 15, 6, 9, 13, 10, 13, 9, 11, 10, 6, 11, 10, 13, 11, 13, 9, 13, 13, 13, 8, 10, 10, 13, 4, 9, 9, 9, 10, 10, 13, 4, 10, 13, 7, 10, 10, 13, 10, 12, 10, 10, 11, 9, 13, 10, 7, 9, 10, 9, 14, 10, 12, 9, 9, 13, 10, 9, 7, 10, 9, 10, 10, 10, 13, 9, 10, 13, 5, 9, 9, 5, 11, 9, 16, 14, 12, 7, 13, 10, 10, 10, 14, 10, 10, 5, 13, 9, 6, 10, 14, 10, 10, 14, 9, 9, 4, 9, 10, 9, 13, 10, 10, 13, 14, 10, 11, 10, 10, 10, 16, 9, 13, 9, 9, 12, 3, 9, 13, 9, 5, 9, 9, 9, 4, 9, 12, 13, 13, 13, 13, 9, 7, 9, 12, 8, 14, 12, 10, 3, 9, 9, 10, 10, 11, 10, 4, 13, 13, 9, 14, 9, 9, 13, 10, 9, 9, 9, 10, 9, 6, 9, 9, 9, 3, 13, 6, 13, 10, 14, 9, 11, 9, 10, 13, 11, 14, 4, 9, 13, 13, 10, 10, 9, 5, 9, 9, 13, 9, 8, 9, 13, 11, 13, 13, 7, 13, 5, 9, 13, 9, 14, 13, 11, 10, 14, 11, 10, 9, 11, 14, 11, 9, 10, 9, 9, 10, 10, 9, 9, 10, 13, 10, 13, 13, 9, 14, 9, 13, 11, 10, 15, 9, 13, 5, 13, 10, 10, 9, 9, 3, 8, 9, 13, 9, 6, 10, 9, 9, 9, 7, 13, 9, 11, 13, 9, 10, 14, 13, 9, 13, 13, 6, 9, 11, 9, 10, 10, 9, 9, 9, 10, 15, 9, 9, 10, 10, 13, 13, 10, 6, 13, 9, 4, 6, 10, 9, 11, 10, 14, 9, 9, 9, 13, 9, 9, 14, 13, 9, 13, 14, 10, 9, 13, 9, 10, 9, 7, 12, 2, 9, 10, 9, 13, 10, 9, 10, 13, 9, 9, 10, 12, 9, 6, 13, 10, 3, 13, 9, 14, 10, 16, 9, 10, 13, 10, 14, 14, 10, 3, 9, 12, 14, 12, 10, 13, 10, 6, 14, 9, 9, 9, 7, 6, 10, 9, 10, 9, 10, 10, 4, 10, 9, 13, 6, 9, 9, 10, 13, 10, 10, 10, 13, 13, 10, 13, 10, 12, 9, 14, 9, 10, 9, 13, 11, 11, 11, 15, 13, 9, 13, 10, 14, 9, 9, 13, 9, 9, 13, 9, 13, 13, 13, 7, 11, 9, 7, 4, 9, 11, 9, 16, 10, 14, 10, 9, 13, 9, 9, 10, 10, 9, 3, 12, 13, 10, 10, 10, 10, 9, 9, 14, 9, 14, 10, 3, 10, 13, 14, 8, 9, 7, 9, 6, 10, 10, 9, 14, 13, 9, 10, 9, 11, 13, 10, 9, 10, 12, 13, 9, 4, 5, 9, 9, 2, 9, 13, 16, 9, 13, 10, 10, 14, 10, 13, 6, 10, 14, 13, 13, 5, 16, 13, 9, 14, 10, 10, 9, 9, 7, 13, 9, 9, 11, 10, 10, 13, 7, 9, 4, 9, 9, 15, 9, 13, 10, 14, 15, 16, 10, 9, 10, 15, 11, 10, 3, 13, 10, 11, 13, 9, 12, 9, 10, 9, 9, 14, 15, 11, 10, 13, 10, 9, 2, 10, 9, 7, 9, 5, 10, 2, 9, 4, 13, 10, 5, 13, 10, 9, 9, 5, 5, 10, 13, 9, 11, 14, 10, 11, 10, 9, 10, 9, 16, 9, 10, 10, 10, 12, 10, 14, 9, 10, 12, 10, 9, 10, 10, 10, 10, 12, 11, 10, 13, 10, 6, 9, 10, 4, 9, 9, 9, 9, 9, 13, 13, 10, 11, 9, 6, 9, 12, 10, 10, 10, 9, 8, 6, 10, 5, 11, 9, 11, 10, 10, 9, 14, 10, 10, 10, 12, 14, 9, 10, 6, 9, 10, 6, 8, 9, 13, 14, 10, 2, 9, 14, 13, 10, 13, 10, 14, 2, 6, 9, 10, 9, 9, 13, 10, 6, 9, 10, 10, 10, 9, 10, 12, 6, 9, 9, 6, 14, 14, 10, 13, 9, 13, 7, 9, 4, 13, 4, 10, 10, 10, 14, 2, 9, 9, 10, 6, 14, 6, 9, 9, 9, 9, 5, 13, 13, 9, 9, 10, 12, 13, 10, 10, 13, 10, 9, 9, 9, 9, 9, 10, 9, 13, 11, 9, 10, 10, 9, 7, 10, 13, 7, 9, 14, 9, 6, 9, 9, 9, 13, 10, 14, 10, 10, 13, 10, 10, 9, 10, 10, 13, 10, 10, 9, 9, 10, 9, 6, 13, 13, 5, 10, 13, 13, 10, 7, 13, 13, 9, 10, 10, 9, 11, 2, 9, 9, 9, 10, 9, 9, 13, 8, 11, 16, 9, 13, 3, 9, 9, 9, 3, 13, 16, 9, 6, 13, 9, 9, 10, 13, 9, 13, 6, 10, 9, 13, 5, 9, 9, 9, 10, 7, 9, 9, 9, 7, 6, 14, 9, 12, 13, 3, 13, 11, 9, 13, 10, 13, 11, 9, 9, 9, 9, 10, 12, 6, 10, 9, 9, 9, 9, 9, 10, 10, 13, 9, 13, 12, 13, 11, 13, 9, 4, 9, 9, 11, 10, 9, 10, 10, 9, 9, 13, 11, 9, 9, 9, 12, 6, 9, 9, 9, 9, 5, 6, 12, 13, 13, 13, 13, 9, 10, 10, 13, 9, 9, 6, 10, 5, 7, 10, 13, 14, 9, 8, 11, 10, 13, 10, 9, 5, 9, 10, 10, 10, 5, 9, 5, 10, 9, 9, 9, 9, 9, 5, 9, 13, 7, 10, 6, 9, 9, 9, 9, 12, 9, 14, 9, 15, 9, 10, 10, 9, 9, 8, 14, 14, 9, 9, 9, 13, 9, 10, 10, 10, 13, 3, 9, 10, 13, 13, 10, 11, 9, 10, 11, 9, 5, 14, 9, 9, 10, 10, 10, 12, 10, 9, 5, 14, 13, 10, 10, 9, 12, 9, 13, 13, 9, 10, 9, 9, 13, 9, 10, 9, 4, 9, 10, 16, 10, 9, 13, 13, 9, 9, 15, 15, 10, 6, 9, 12, 13, 6, 9, 9, 10, 9, 10, 10, 7, 10, 10, 9, 7, 10, 13, 13, 11, 9, 7, 14, 10, 10, 14, 9, 9, 14, 10, 9, 13, 9, 13, 9, 10, 12, 10, 10, 14, 10, 14, 10, 9, 13, 9, 9, 9, 10, 13, 14, 4, 10, 13, 4, 9, 10, 9, 12, 10, 11, 7, 9, 10, 6, 9, 13, 13, 13, 10, 9, 9, 6, 9, 9, 13, 10, 9, 9, 10, 14, 9, 4, 9, 7, 10, 13, 10, 5, 10, 10, 9, 10, 10, 9, 9, 15, 6, 16, 10, 8, 9, 10, 10, 9, 10, 9, 9, 9, 9, 9, 10, 14, 9, 10, 9, 14, 10, 9, 10, 10, 14, 10, 9, 7, 10, 10, 13, 10, 7, 13, 9, 9, 9, 13, 13, 9, 11, 10, 10, 10, 14, 12, 9, 13, 10, 10, 10, 10, 13, 10, 13, 9, 14, 9, 9, 10, 10, 13, 10, 10, 9, 4, 11, 13, 9, 14, 10, 7, 12, 9, 11, 10, 13, 9, 9, 16, 9, 12, 13, 9, 12, 9, 9, 9, 9, 10, 13, 11, 9, 9, 13, 9, 13, 7, 15, 13, 9, 9, 10, 14, 10, 10, 8, 14, 9, 7, 7, 13, 10, 5, 10, 13, 10, 13, 11, 9, 13, 13, 10, 13, 9, 5, 9, 10, 9, 10, 7, 9, 8, 11, 13, 10, 9, 9, 7, 9, 13, 15, 9, 6, 9, 9, 10, 9, 9, 14, 14, 10, 10, 1, 10, 9, 10, 7, 10, 4, 9, 13, 9, 10, 9, 9, 9, 9, 10, 9, 10, 10, 9, 14, 10, 14, 9, 9, 6, 8, 9, 10, 10, 6, 14, 15, 9, 6, 14, 10, 13, 9, 9, 13, 9, 6, 14, 9, 11, 13, 13, 14, 13, 10, 9, 3, 14, 13, 13, 9, 9, 9, 15, 9, 9, 13, 9, 9, 9, 13, 14, 6, 9, 9, 10, 13, 9, 12, 9, 7, 9, 9, 13, 13, 13, 10, 10, 9, 4, 14, 9, 9, 10, 10, 14, 10, 6, 12, 13, 9, 9, 4, 10, 14, 13, 11, 9, 13, 10, 9, 10, 7, 7, 9, 5, 9, 12, 10, 14, 13, 9, 10, 9, 13, 9, 6, 16, 12, 10, 9, 15, 13, 10, 9, 9, 9, 9, 7, 9, 11, 10, 13, 9, 13, 10, 9, 13, 9, 7, 11, 14, 10, 10, 7, 14, 10, 10, 9, 10, 10, 4, 11, 9, 10, 9, 14, 10, 9, 9, 13, 13, 9, 4, 10, 9, 10, 9, 14, 14, 14, 9, 9, 9, 5, 13, 14, 9, 10, 9, 10, 3, 14, 9, 12, 7, 10, 9, 9, 7, 9, 10, 13, 12, 13, 13, 11, 9, 16, 11, 15, 10, 9, 12, 13, 9, 9, 10, 8, 14, 11, 11, 9, 9, 9, 7, 10, 10, 10, 9, 9, 13, 9, 10, 10, 13, 9, 7, 14, 9, 13, 7, 11, 11, 12, 10, 9, 13, 13, 9, 9, 9, 9, 11, 6, 9, 10, 9, 13, 10, 9, 6, 9, 9, 13, 9, 9, 10, 9, 13, 9, 14, 4, 9, 13, 7, 9, 12, 5, 16, 9, 13, 9, 9, 13, 13, 8, 9, 9, 7, 6, 10, 5, 9, 10, 10, 9, 13, 15, 11, 9, 9, 13, 9, 10, 9, 14, 10, 9, 12, 14, 10, 9, 10, 10, 9, 10, 13, 10, 10, 9, 13, 15, 9, 13, 9, 10, 9, 13, 13, 10, 4, 9, 9, 14, 9, 10, 10, 9, 10, 13, 10, 9, 13, 7, 9, 9, 10, 10, 10, 9, 10, 9, 10, 7, 9, 10, 12, 13, 7, 13, 14, 7, 12, 13, 9, 9, 4, 10, 10, 10, 10, 9, 6, 9, 9, 10, 10, 9, 13, 10, 9, 9, 13, 9, 10, 9, 13, 9, 9, 10, 6, 10, 12, 13, 9, 9, 7, 10, 10, 5, 9, 14, 10, 14, 10, 14, 10, 8, 10, 13, 9, 9, 9, 13, 14, 11, 9, 9, 9, 10, 10, 9, 9, 10, 10, 9, 7, 13, 9, 9, 9, 12, 11, 10, 1, 10, 9, 4, 9, 13, 13, 14, 11, 8, 6, 13, 14, 9, 9, 14, 12, 10, 9, 7, 11, 13, 4, 13, 10, 9, 15, 10, 10, 8, 11, 10, 10, 13, 6, 7, 13, 13, 9, 13, 9, 9, 9, 13, 10, 12, 13, 10, 9, 12, 5, 11, 13, 13, 10, 13, 12, 9, 9, 10, 13, 15, 13, 10, 4, 13, 10, 9, 10, 10, 13, 9, 13, 6, 9, 9, 9, 14, 10, 13, 9, 10, 10, 9, 10, 13, 13, 11, 11, 9, 14, 10, 10, 12, 9, 9, 9, 9, 9, 14, 16, 15, 6, 4, 9, 12, 9, 10, 4, 13, 9, 10, 12, 9, 4, 10, 14, 10, 13, 9, 6, 13, 9, 6, 13, 9, 10, 14, 12, 10, 9, 10, 10, 9, 7, 9, 16, 13, 10, 10, 9, 13, 9, 10, 13, 9, 9, 13, 13, 9, 11, 5, 10, 13, 10, 12, 9, 14, 13, 15, 9, 10, 9, 14, 10, 10, 13, 13, 4, 10, 13, 13, 10, 10, 11, 9, 7, 8, 10, 16, 7, 16, 10, 6, 7, 11, 6, 14, 13, 10, 4, 9, 13, 10, 10, 9, 13, 9, 13, 9, 14, 13, 9, 9, 9, 9, 1, 13, 9, 9, 10, 9, 13, 9, 11, 9, 7, 6, 13, 13, 14, 13, 9, 14, 10, 5, 13, 10, 6, 13, 9, 9, 13, 11, 10, 15, 13, 9, 10, 10, 13, 14, 9, 14, 9, 9, 10, 9, 13, 9, 9, 12, 9, 9, 11, 14, 16, 10, 5, 9, 3, 11, 10, 12, 5, 13, 9, 13, 10, 4, 13, 9, 12, 9, 11, 9, 9, 11, 9, 16, 7, 9, 10, 14, 10, 11, 9, 10, 10, 10, 9, 14, 9, 6, 9, 10, 12, 10, 9, 15, 10, 10, 9, 7, 9, 13, 13, 13, 10, 1, 13, 9, 9, 10, 9, 9, 13, 10, 12, 10, 13, 14, 9, 10, 10, 9, 9, 9, 13, 7, 13, 10, 9, 9, 9, 9, 4, 13, 13, 15, 10, 10, 4, 13, 10, 10, 9, 10, 7, 9, 13, 10, 9, 13, 9, 10, 16, 9, 9, 9, 11, 9, 10, 9, 9, 13, 9, 10, 9, 10, 9, 13, 6, 10, 13, 11, 9, 9, 13, 13, 9, 10, 13, 9, 11, 10, 10, 7, 13, 13, 7, 9, 9, 9, 15, 9, 10, 12, 9, 12, 9, 10, 13, 6, 10, 15, 10, 10, 13, 9, 9, 16, 9, 9, 16, 9, 9, 9, 9, 13, 13, 14, 9, 9, 9, 10, 13, 9, 13, 7, 9, 12, 16, 9, 10, 9, 2, 9, 9, 13, 13, 11, 13, 13, 4, 9, 9, 9, 7, 13, 9, 9, 10, 14, 9, 2, 16, 4, 10, 13, 9, 10, 12, 16, 4, 9, 14, 9, 9, 10, 6, 11, 10, 15, 10, 10, 10, 10, 10, 6, 10, 10, 9, 13, 10, 13, 10, 9, 10, 14, 12, 10, 12, 6, 13, 14, 7, 8, 10, 6, 10, 7, 4, 10, 10, 10, 13, 15, 10, 10, 9, 9, 9, 10, 10, 9, 10, 10, 10, 9, 9, 13, 10, 9, 10, 13, 9, 11, 10, 12, 9, 6, 10, 14, 10, 14, 9, 10, 12, 10, 9, 12, 9, 9, 10, 13, 10, 13, 10, 14, 10, 10, 9, 13, 14, 10, 10, 13, 9, 13, 5, 12, 10, 10, 9, 14, 6, 9, 10, 10, 13, 9, 10, 9, 3, 13, 7, 9, 12, 12, 9, 9, 9, 9, 9, 10, 9, 7, 11, 10, 7, 3, 13, 9, 10, 9, 14, 13, 9, 10, 13, 10, 9, 14, 9, 10, 11, 6, 9, 13, 10, 9, 8, 10, 9, 10, 6, 9, 9, 5, 9, 14, 13, 11, 14, 13, 9, 9, 9, 15, 10, 13, 9, 13, 9, 10, 10, 13, 10, 16, 6, 12, 9, 10, 9, 10, 9, 9, 9, 13, 7, 3, 13, 16, 12, 9, 9, 9, 9, 9, 9, 7, 6, 10, 9, 9, 10, 13, 14, 9, 12, 13, 9, 10, 9, 8, 12, 9, 7, 14, 10, 10, 10, 7, 9, 9, 9, 10, 13, 10, 9, 14, 13, 9, 15, 13, 10, 13, 9, 10, 9, 9, 11, 6, 13, 7, 9, 13, 9, 9, 10, 11, 13, 10, 11, 13, 9, 9, 9, 10, 9, 9, 7, 9, 13, 13, 4, 9, 10, 13, 7, 10, 10, 10, 9, 14, 9, 10, 9, 13, 10, 9, 13, 9, 10, 10, 9, 10, 5, 8, 9, 9, 7, 9, 9, 9, 9, 14, 9, 9, 13, 10, 4, 10, 9, 15, 6, 9, 10, 5, 14, 6, 10, 13, 9, 10, 4, 10, 9, 10, 5, 9, 7, 10, 9, 10, 7, 10, 7, 9, 13, 12, 10, 9, 9, 13, 10, 8, 13, 7, 9, 11, 14, 6, 10, 9, 9, 14, 9, 13, 12, 10, 10, 9, 13, 11, 11, 13, 10, 9, 10, 9, 9, 13, 13, 9, 16, 10, 14, 9, 10, 10, 10, 4, 10, 13, 9, 14, 9, 10, 9, 3, 9, 9, 13, 13, 13, 10, 9, 9, 14, 9, 9, 9, 10, 10, 14, 13, 8, 10, 13, 9, 10, 10, 10, 8, 2, 9, 15, 10, 10, 9, 9, 10, 10, 2, 13, 4, 5, 9, 10, 9, 9, 10, 10, 13, 9, 5, 13, 9, 7, 7, 7, 10, 6, 9, 9, 12, 9, 4, 5, 13, 9, 9, 3, 6, 10, 10, 9, 4, 9, 10, 9, 16, 13, 7, 13, 9, 13, 11, 13, 9, 14, 10, 7, 15, 9, 9, 11, 10, 10, 12, 9, 13, 9, 9, 13, 13, 10, 9, 12, 13, 9, 10, 10, 9, 12, 13, 10, 7, 13, 9, 10, 9, 13, 9, 10, 10, 9, 12, 14, 10, 9, 10, 14, 10, 9, 10, 10, 9, 9, 13, 6, 10, 12, 4, 13, 9, 11, 9, 14, 10, 9, 10, 13, 10, 2, 9, 9, 10, 10, 10, 12, 14, 7, 10, 14, 9, 10, 10, 9, 9, 9, 9, 10, 6, 13, 4, 9, 10, 10, 9, 9, 10, 10, 13, 9, 10, 12, 10, 9, 9, 10, 13, 14, 12, 5, 10, 13, 9, 1, 9, 14, 9, 9, 10, 9, 7, 13, 9, 9, 4, 13, 10, 9, 9, 13, 10, 13, 9, 14, 4, 9, 13, 9, 9, 4, 9, 9, 9, 16, 9, 10, 11, 10, 10, 9, 13, 10, 10, 10, 10, 13, 10, 9, 11, 7, 11, 10, 10, 10, 9, 10, 6, 9, 13, 10, 10, 13, 10, 11, 9, 10, 4, 13, 9, 10, 15, 8, 10, 6, 9, 10, 7, 10, 9, 6, 5, 13, 13, 9, 16, 14, 15, 9, 14, 13, 10, 9, 9, 13, 13, 10, 10, 9, 11, 4, 13, 11, 9, 9, 15, 10, 13, 9, 10, 14, 10, 10, 9, 7, 10, 9, 10, 9, 13, 9, 10, 9, 10, 4, 9, 10, 10, 9, 10, 6, 10, 7, 10, 9, 9, 13, 13, 9, 14, 13, 13, 9, 10, 10, 7, 13, 10, 13, 11, 16, 10, 9, 10, 11, 9, 14, 13, 9, 9, 9, 13, 4, 9, 12, 12, 5, 9, 7, 9, 9, 5, 10, 4, 13, 10, 9, 13, 11, 3, 9, 7, 12, 11, 9, 14, 9, 9, 10, 9, 1, 15, 9, 10, 14, 9, 11, 11, 1, 13, 9, 14, 9, 9, 6, 9, 1, 14, 8, 10, 10, 9, 9, 10, 14, 15, 10, 10, 7, 8, 9, 9, 4, 9, 11, 9, 13, 15, 13, 9, 6, 13, 10, 7, 15, 13, 16, 13, 9, 7, 9, 9, 6, 12, 7, 10, 9, 11, 9, 13, 9, 10, 13, 9, 13, 9, 9, 12, 13, 6, 15, 9, 13, 13, 10, 9, 9, 10, 9, 13, 13, 12, 9, 9, 4, 9, 9, 14, 10, 9, 9, 13, 13, 13, 15, 9, 10, 9, 8, 4, 8, 12, 9, 9, 9, 14, 9, 11, 10, 10, 9, 14, 10, 11, 10, 13, 10, 9, 10, 10, 9, 10, 13, 10, 7, 13, 13, 9, 14, 13, 10, 2, 10, 14, 9, 9, 11, 9, 9, 9, 10, 10, 12, 13, 15, 16, 13, 10, 9, 11, 9, 9, 13, 9, 16, 6, 10, 14, 4, 11, 10, 10, 13, 8, 6, 10, 9, 10, 13, 15, 13, 14, 9, 10, 9, 13, 10, 10, 13, 9, 10, 9, 13, 10, 10, 13, 13, 13, 13, 13, 13, 10, 9, 3, 9, 9, 9, 9, 13, 9, 16, 13, 3, 11, 9, 9, 14, 3, 7, 9, 9, 7, 4, 9, 10, 10, 10, 10, 10, 9, 9, 13, 6, 10, 9, 5, 10, 9, 7, 9, 2, 9, 9, 9, 9, 10, 9, 9, 9, 5, 9, 10, 10, 10, 9, 15, 9, 15, 13, 9, 10, 9, 10, 13, 9, 10, 9, 9, 9, 9, 10, 9, 9, 10, 9, 9, 13, 10, 9, 9, 10, 10, 7, 13, 6, 7, 8, 9, 9, 13, 9, 13, 7, 15, 9, 9, 13, 10, 13, 9, 4, 10, 10, 10, 10, 11, 9, 13, 11, 4, 10, 9, 9, 12, 10, 9, 12, 9, 13, 9, 14, 8, 12, 10, 9, 9, 9, 13, 9, 9, 13, 13, 13, 10, 13, 9, 10, 13, 13, 13, 10, 9, 12, 10, 10, 13, 13, 9, 13, 10, 13, 10, 9, 9, 14, 9, 9, 11, 5, 9, 12, 4, 9, 10, 9, 10, 9, 13, 12, 11, 9, 9, 14, 9, 13, 10, 9, 11, 13, 9, 9, 9, 9, 9, 13, 10, 9, 13, 13, 15, 13, 13, 13, 9, 9, 9, 3, 14, 11, 10, 12, 13, 13, 10, 9, 9, 11, 10, 14, 10, 12, 10, 9, 9, 13, 8, 10, 10, 11, 9, 13, 9, 11, 9, 5, 16, 10, 13, 2, 10, 14, 10, 9, 15, 13, 11, 10, 10, 14, 10, 15, 13, 9, 9, 12, 8, 5, 9, 9, 9, 9, 6, 9, 12, 13, 11, 9, 10, 10, 14, 12, 11, 10, 13, 9, 13, 10, 9, 13, 10, 9, 13, 9, 9, 9, 10, 10, 9, 9, 13, 13, 9, 10, 11, 14, 7, 9, 9, 10, 13, 10, 13, 9, 4, 9, 10, 9, 9, 5, 9, 9, 9, 13, 13, 13, 11, 15, 9, 9, 10, 11, 9, 9, 9, 12, 10, 10, 9, 13, 13, 10, 9, 9, 9, 13, 9, 13, 12, 9, 13, 10, 9, 9, 4, 10, 10, 9, 9, 6, 14, 4, 9, 7, 9, 15, 6, 13, 15, 10, 13, 12, 10, 10, 13, 13, 9, 10, 9, 10, 14, 10, 9, 9, 7, 6, 9, 9, 10, 9, 9, 10, 13, 9, 10, 10, 7, 9, 11, 10, 9, 11, 10, 13, 11, 9, 6, 13, 6, 10, 9, 9, 10, 6, 10, 13, 10, 9, 13, 9, 10, 10, 14, 6, 11, 11, 10, 13, 9, 9, 14, 13, 10, 13, 9, 9, 10, 6, 13, 11, 10, 10, 9, 9, 9, 15, 9, 14, 10, 14, 14, 9, 9, 13, 10, 10, 14, 11, 13, 13, 10, 15, 6, 9, 11, 9, 9, 9, 5, 13, 10, 9, 16, 13, 8, 6, 13, 9, 11, 9, 9, 9, 10, 10, 14, 10, 12, 10, 12, 10, 9, 13, 13, 14, 14, 9, 9, 10, 9, 10, 10, 10, 9, 10, 2, 9, 13, 13, 10, 9, 9, 13, 11, 9, 10, 4, 10, 14, 9, 13, 10, 14, 9, 9, 9, 9, 1, 9, 5, 14, 10, 16, 13, 12, 12, 11, 9, 9, 10, 10, 9, 9, 6, 9, 9, 9, 4, 13, 9, 10, 12, 11, 9, 10, 13, 9, 7, 15, 13, 13, 16, 9, 6, 12, 9, 9, 10, 9, 10, 4, 9, 15, 4, 9, 9, 9, 9, 13, 9, 10, 9, 13, 13, 14, 13, 13, 10, 13, 13, 7, 5, 9, 10, 9, 12, 10, 10, 10, 13, 9, 13, 14, 10, 9, 9, 9, 6, 9, 13, 13, 10, 6, 10, 14, 9, 13, 9, 12, 7, 9, 9, 9, 8, 9, 9, 9, 10, 9, 9, 9, 13, 13, 4, 14, 15, 3, 14, 2, 2, 4, 10, 9, 10, 10, 6, 14, 13, 13, 10, 10, 6, 9, 10, 9, 13, 10, 9, 9, 10, 13, 16, 13, 10, 13, 9, 10, 9, 10, 9, 10, 13, 9, 13, 13, 13, 9, 9, 9, 11, 13, 13, 9, 10, 10, 4, 10, 10, 10, 10, 10, 12, 10, 11, 9, 9, 9, 9, 9, 4, 10, 11, 13, 10, 9, 13, 11, 10, 9, 10, 9, 9, 10, 13, 15, 9, 13, 9, 11, 13, 5, 10, 10, 11, 9, 13, 9, 9, 6, 10, 3, 15, 7, 14, 9, 3, 10, 11, 10, 9, 15, 13, 9, 9, 10, 11, 10, 3, 10, 7, 9, 9, 2, 9, 9, 12, 10, 13, 6, 9, 13, 16, 10, 9, 10, 15, 9, 9, 13, 10, 2, 4, 10, 13, 9, 10, 9, 7, 11, 10, 13, 10, 9, 9, 9, 10, 9, 6, 9, 10, 9, 9, 13, 13, 9, 10, 11, 13, 3, 11, 8, 9, 9, 13, 9, 9, 9, 13, 5, 13, 10, 15, 10, 9, 15, 13, 15, 9, 11, 5, 9, 8, 13, 9, 13, 10, 11, 9, 11, 10, 12, 13, 6, 11, 9, 9, 13, 11, 10, 9, 9, 10, 6, 14, 9, 10, 6, 10, 7, 14, 13, 6, 9, 12, 10, 10, 13, 14, 4, 9, 14, 13, 13, 10, 10, 10, 10, 13, 9, 10, 6, 14, 9, 7, 9, 13, 9, 10, 16, 6, 9, 10, 14, 14, 10, 10, 9, 3, 10, 7, 13, 11, 14, 6, 9, 4, 13, 15, 14, 6, 9, 9, 9, 9, 9, 9, 9, 9, 10, 9, 7, 9, 9, 10, 13, 14, 13, 9, 13, 11, 10, 10, 10, 10, 10, 10, 10, 7, 10, 11, 13, 13, 9, 14, 10, 7, 13, 11, 9, 10, 9, 13, 10, 9, 10, 10, 16, 13, 9, 13, 13, 15, 4, 13, 9, 12, 13, 9, 4, 9, 9, 9, 14, 11, 9, 9, 13, 13, 9, 13, 13, 9, 9, 7, 9, 9, 7, 12, 9, 14, 13, 9, 6, 4, 13, 9, 13, 10, 10, 13, 9, 8, 9, 9, 13, 10, 12, 10, 9, 9, 10, 9, 9, 13, 3, 9, 10, 13, 9, 9, 7, 6, 9, 6, 9, 14, 8, 9, 9, 7, 10, 9, 9, 9, 10, 13, 13, 10, 14, 13, 13, 9, 10, 4, 9, 6, 10, 3, 9, 13, 9, 12, 7, 9, 4, 10, 9, 13, 12, 10, 13, 13, 9, 10, 13, 13, 9, 9, 14, 10, 13, 9, 13, 10, 9, 10, 12, 10, 6, 9, 14, 9, 9, 10, 10, 13, 13, 13, 10, 11, 10, 16, 10, 13, 5, 9, 4, 11, 9, 10, 12, 10, 9, 10, 9, 10, 13, 14, 10, 10, 13, 10, 11, 7, 16, 8, 14, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 6, 9, 13, 8, 9, 13, 13, 10, 7, 9, 10, 10, 9, 10, 9, 9, 13, 12, 9, 9, 9, 10, 9, 3, 10, 14, 9, 7, 9, 9, 10, 9, 13, 4, 14, 9, 9, 10, 5, 10, 9, 9, 10, 9, 10, 10, 12, 10, 15, 13, 9, 16, 13, 9, 9, 13, 9, 10, 13, 4, 9, 9, 13, 9, 10, 4, 14, 9, 10, 9, 9, 13, 6, 13, 9, 14, 9, 7, 9, 3, 9, 9, 10, 9, 9, 10, 13, 14, 9, 11, 10, 9, 10, 10, 10, 10, 10, 10, 9, 9, 10, 14, 13, 10, 9, 10, 13, 10, 10, 10, 10, 9, 9, 13, 10, 7, 10, 9, 9, 13, 13, 10, 9, 10, 13, 9, 10, 4, 10, 7, 9, 9, 10, 9, 10, 13, 13, 6, 10, 9, 9, 10, 9, 7, 10, 10, 10, 14, 10, 9, 13, 14, 7, 13, 9, 9, 14, 9, 9, 12, 11, 9, 14, 10, 9, 10, 14, 10, 10, 10, 9, 9, 10, 9, 10, 10, 9, 10, 9, 10, 12, 9, 13, 9, 9, 14, 9, 11, 10, 14, 6, 10, 16, 10, 13, 13, 13, 10, 13, 7, 9, 4, 7, 13, 10, 7, 5, 7, 6, 9, 9, 9, 14, 13, 10, 13, 12, 10, 6, 10, 9, 14, 14, 9, 10, 9, 9, 10, 3, 9, 9, 10, 14, 10, 10, 9, 9, 9, 13, 10, 7, 9, 11, 13, 10, 6, 13, 13, 9, 9, 10, 10, 9, 10, 10, 13, 11, 11, 13, 15, 3, 9, 14, 9, 9, 13, 13, 4, 9, 13, 9, 6, 13, 16, 10, 9, 10, 10, 3, 4, 12, 13, 9, 13, 13, 12, 13, 10, 6, 13, 10, 2, 9, 10, 11, 10, 15, 8, 9, 9, 14, 7, 9, 9, 10, 13, 15, 9, 11, 13, 6, 9, 9, 13, 7, 9, 13, 10, 10, 10, 5, 8, 6, 13, 10, 11, 10, 9, 10, 9, 10, 9, 14, 11, 13, 12, 14, 9, 13, 13, 9, 9, 9, 10, 13, 7, 13, 9, 13, 13, 9, 15, 9, 14, 14, 10, 12, 9, 9, 6, 9, 9, 9, 13, 10, 13, 9, 13, 14, 10, 13, 10, 13, 6, 9, 9, 9, 8, 10, 7, 13, 9, 15, 13, 15, 14, 12, 3, 9, 5, 2, 12, 9, 9, 9, 13, 9, 15, 10, 13, 13, 10, 13, 4, 10, 10, 12, 9, 10, 10, 10, 5, 10, 11, 11, 10, 9, 10, 3, 13, 10, 9, 9, 10, 9, 13, 14, 13, 9, 9, 10, 10, 10, 10, 10, 10, 6, 9, 10, 9, 13, 9, 9, 9, 15, 14, 11, 14, 9, 11, 15, 9, 16, 12, 6, 11, 13, 14, 10, 13, 14, 16, 10, 6, 14, 9, 13, 9, 10, 16, 9, 13, 2, 9, 9, 13, 8, 9, 10, 10, 10, 6, 10, 9, 7, 9, 3, 9, 14, 9, 15, 9, 13, 10, 10, 13, 9, 10, 10, 14, 10, 13, 10, 10, 14, 9, 14, 9, 13, 10, 13, 10, 4, 9, 10, 10, 9, 14, 9, 7, 9, 7, 10, 10, 10, 9, 9, 13, 8, 9, 9, 11, 10, 9, 13, 13, 12, 9, 10, 13, 9, 13, 10, 10, 11, 10, 10, 9, 10, 9, 16, 9, 10, 13, 9, 9, 9, 5, 9, 7, 7, 9, 10, 13, 10, 10, 9, 9, 9, 9, 9, 10, 15, 13, 9, 9, 10, 12, 13, 13, 9, 9, 9, 10, 9, 14, 9, 9, 9, 9, 14, 13, 9, 9, 9, 13, 10, 9, 10, 13, 9, 7, 9, 13, 4, 14, 9, 10, 10, 9, 10, 9, 14, 13, 9, 13, 9, 6, 13, 14, 4, 13, 5, 8, 10, 9, 9, 11, 10, 9, 10, 2, 10, 10, 13, 10, 14, 9, 12, 9, 13, 9, 13, 9, 16, 9, 5, 10, 12, 10, 14, 9, 9, 9, 7, 7, 13, 11, 13, 9, 10, 13, 10, 7, 9, 10, 9, 10, 9, 10, 9, 6, 10, 9, 5, 16, 9, 4, 10, 10, 10, 9, 11, 10, 16, 9, 7, 9, 13, 7, 10, 13, 13, 9, 12, 10, 9, 12, 15, 9, 9, 10, 9, 9, 10, 10, 4, 8, 6, 10, 5, 9, 13, 13, 9, 13, 4, 12, 10, 9, 13, 9, 11, 13, 14, 10, 9, 10, 10, 13, 10, 9, 6, 5, 7, 9, 10, 10, 9, 9, 13, 10, 13, 10, 7, 9, 10, 16, 9, 9, 7, 10, 10, 9, 13, 14, 13, 4, 9, 10, 9, 7, 9, 10, 8, 9, 9, 9, 14, 13, 9, 10, 3, 9, 7, 9, 6, 13, 13, 9, 9, 9, 13, 10, 9, 9, 10, 10, 12, 9, 10, 6, 13, 9, 10, 10, 9, 9, 14, 9, 3, 13, 10, 13, 14, 9, 9, 4, 16, 8, 14, 10, 16, 7, 10, 10, 10, 16, 10, 13, 12, 12, 9, 4, 12, 10, 9, 9, 9, 13, 12, 9, 10, 10, 9, 10, 10, 11, 9, 10, 13, 13, 13, 10, 13, 9, 8, 10, 13, 9, 10, 10, 13, 5, 10, 10, 9, 9, 9, 10, 13, 10, 9, 10, 9, 9, 10, 11, 9, 6, 9, 9, 13, 9, 13, 10, 10, 10, 5, 13, 9, 11, 9, 11, 9, 9, 6, 8, 9, 6, 9, 10, 10, 9, 14, 9, 10, 2, 13, 5, 9, 13, 15, 9, 11, 9, 9, 10, 9, 13, 10, 13, 10, 9, 9, 9, 11, 10, 9, 16, 4, 13, 9, 14, 6, 9, 9, 9, 7, 9, 10, 9, 9, 9, 11, 9, 12, 13, 9, 9, 3, 10, 11, 1, 9, 10, 9, 13, 13, 14, 14, 10, 9, 9, 10, 10, 9, 10, 15, 14, 15, 14, 9, 9, 13, 5, 13, 9, 13, 7, 9, 9, 13, 5, 10, 13, 13, 3, 2, 9, 11, 13, 13, 13, 13, 11, 13, 13, 13, 9, 10, 10, 9, 14, 10, 9, 9, 10, 13, 13, 10, 10, 9, 10, 10, 10, 10, 5, 9, 9, 9, 13, 12, 10, 10, 13, 10, 12, 9, 10, 10, 9, 14, 9, 10, 9, 15, 14, 14, 13, 9, 9, 13, 10, 4, 13, 7, 3, 10, 13, 9, 9, 13, 7, 9, 9, 13, 9, 9, 13, 9, 11, 10, 9, 10, 12, 13, 10, 13, 10, 10, 6, 13, 9, 13, 9, 9, 15, 9, 13, 9, 13, 9, 9, 8, 3, 6, 13, 13, 13, 13, 10, 6, 10, 8, 9, 13, 10, 15, 11, 11, 10, 14, 9, 13, 9, 10, 5, 7, 13, 10, 14, 9, 10, 9, 10, 9, 9, 10, 10, 4, 10, 14, 9, 9, 13, 9, 7, 7, 10, 9, 13, 14, 9, 10, 13, 14, 10, 5, 9, 9, 9, 3, 13, 9, 13, 9, 13, 9, 9, 9, 9, 9, 10, 9, 9, 9, 9, 9, 9, 3, 9, 9, 9, 9, 13, 9, 13, 13, 13, 13, 13, 13, 12, 9, 9, 14, 10, 13, 9, 10, 9, 10, 14, 10, 10, 9, 13, 9, 4, 9, 15, 10, 14, 9, 13, 13, 4, 9, 10, 9, 10, 2, 9, 8, 9, 10, 9, 9, 10, 13, 14, 13, 10, 9, 9, 9, 10, 13, 14, 5, 10, 10, 13, 9, 14, 13, 9, 13, 10, 10, 10, 10, 15, 9, 9, 13, 10, 10, 13, 14, 9, 10, 13, 9, 14, 7, 9, 9, 10, 9, 9, 13, 11, 10, 13, 9, 7, 9, 11, 9, 13, 13, 6, 9, 10, 10, 11, 11, 11, 9, 9, 13, 9, 10, 13, 10, 9, 13, 13, 13, 10, 10, 9, 9, 10, 11, 4, 9, 13, 16, 9, 9, 9, 9, 12, 10, 13, 10, 9, 15, 10, 14, 10, 15, 9, 12, 3, 9, 12, 14, 13, 9, 15, 8, 10, 13, 16, 9, 9, 13, 9, 7, 9, 6, 13, 13, 7, 13, 13, 10, 11, 5, 14, 9, 9, 13, 9, 13, 8, 9, 9, 10, 16, 13, 12, 10, 13, 9, 9, 13, 10, 14, 6, 10, 9, 12, 2, 9, 9, 11, 12, 10, 9, 13, 13, 9, 13, 14, 13, 14, 9, 10, 12, 11, 9, 10, 10, 11, 15, 10, 10, 9, 13, 11, 9, 9, 9, 9, 10, 10, 12, 13, 9, 9, 13, 3, 9, 10, 6, 14, 9, 15, 9, 13, 13, 10, 10, 9, 10, 9, 10, 13, 10, 13, 13, 10, 13, 9, 9, 9, 10, 10, 9, 5, 9, 6, 10, 10, 10, 9, 9, 10, 10, 9, 13, 10, 9, 10, 8, 9, 10, 16, 10, 9, 10, 9, 10, 10, 9, 10, 14, 5, 12, 10, 9, 9, 13, 10, 10, 10, 9, 10, 13, 10, 13, 10, 10, 7, 9, 10, 4, 11, 9, 16, 9, 9, 13, 13, 7, 9, 3, 9, 10, 9, 11, 13, 13, 4, 9, 9, 9, 13, 4, 9, 10, 13, 13, 10, 10, 14, 9, 9, 10, 9, 9, 10, 9, 13, 10, 12, 10, 7, 9, 9, 9, 9, 10, 15, 13, 9, 9, 13, 9, 9, 9, 9, 9, 10, 12, 13, 9, 10, 9, 11, 10, 13, 9, 13, 13, 7, 9, 10, 11, 10, 13, 9, 9, 3, 10, 9, 10, 9, 13, 9, 9, 11, 9, 9, 9, 10, 8, 14, 13, 9, 9, 9, 10, 11, 13, 10, 9, 9, 6, 6, 8, 11, 11, 10, 9, 13, 13, 14, 10, 3, 9, 11, 9, 10, 11, 16, 6, 5, 10, 13, 14, 13, 6, 9, 9, 13, 16, 9, 9, 9, 10, 10, 16, 13, 13, 9, 10, 9, 9, 2, 9, 10, 9, 9, 9, 4, 12, 15, 9, 9, 9, 10, 9, 9, 9, 10, 10, 13, 10, 13, 4, 9, 10, 10, 12, 4, 9, 13, 9, 10, 6, 13, 7, 13, 9, 10, 11, 9, 13, 13, 12, 9, 13, 9, 7, 10, 11, 10, 10, 16, 10, 9, 12, 13, 13, 15, 10, 10, 9, 13, 3, 14, 9, 7, 9, 6, 13, 14, 15, 9, 9, 11, 13, 9, 9, 10, 9, 13, 9, 11, 9, 13, 10, 7, 11, 10, 9, 13, 14, 9, 7, 10, 10, 5, 10, 9, 9, 13, 10, 6, 9, 13, 9, 6, 7, 13, 13, 10, 13, 10, 13, 12, 6, 10, 13, 5, 11, 13, 14, 10, 13, 11, 10, 8, 13, 7, 9, 9, 9, 9, 9, 14, 13, 4, 9, 13, 7, 7, 10, 13, 7, 9, 10, 9, 14, 10, 3, 13, 9, 10, 4, 13, 13, 9, 9, 13, 14, 6, 9, 11, 9, 13, 1, 14, 9, 9, 13, 9, 7, 9, 9, 10, 13, 6, 11, 9, 10, 9, 10, 9, 12, 14, 10, 7, 13, 9, 8, 10, 10, 9, 9, 9, 6, 10, 13, 11, 12, 13, 7, 9, 10, 9, 9, 10, 9, 11, 9, 9, 9, 9, 10, 10, 13, 13, 9, 10, 9, 9, 10, 14, 9, 9, 7, 9, 7, 6, 9, 12, 9, 9, 9, 14, 14, 9, 10, 10, 12, 9, 16, 9, 13, 9, 9, 9, 14, 12, 10, 10, 9, 9, 9, 10, 9, 13, 10, 9, 10, 13, 9, 10, 9, 13, 7, 9, 9, 7, 10, 9, 11, 9, 10, 7, 9, 13, 10, 9, 13, 9, 10, 4, 10, 15, 11, 10, 10, 14, 9, 12, 14, 4, 10, 9, 10, 9, 9, 9, 10, 9, 12, 10, 10, 9, 9, 9, 11, 11, 6, 13, 10, 7, 9, 14, 13, 9, 13, 9, 10, 13, 10, 11, 7, 13, 2, 9, 7, 2, 9, 8, 8, 9, 12, 4, 10, 13, 10, 10, 15, 9, 13, 9, 13, 10, 16, 13, 10, 11, 9, 9, 9, 10, 10, 10, 12, 9, 10, 14, 10, 9, 10, 13, 9, 9, 14, 13, 13, 9, 9, 9, 3, 9, 13, 9, 7, 13, 9, 10, 7, 10, 13, 9, 10, 9, 13, 9, 10, 9, 9, 13, 1, 9, 10, 10, 10, 7, 9, 12, 9, 6, 10, 9, 10, 13, 4, 10, 9, 9, 9, 6, 10, 13, 10, 9, 15, 10, 10, 9, 5, 9, 10, 10, 12, 9, 9, 10, 14, 9, 9, 10, 9, 12, 9, 7, 16, 6, 3, 10, 13, 14, 13, 9, 6, 7, 9, 9, 9, 10, 13, 13, 10, 12, 10, 9, 10, 8, 13, 10, 10, 10, 5, 9, 10, 9, 9, 9, 13, 8, 11, 3, 9, 14, 12, 9, 10, 9, 10, 9, 13, 10, 16, 9, 7, 4, 12, 10, 9, 9, 13, 7, 10, 10, 9, 9, 5, 9, 13, 9, 10, 9, 13, 10, 13, 9, 9, 10, 13, 6, 3, 9, 13, 9, 11, 10, 9, 15, 9, 10, 12, 10, 10, 9, 9, 10, 13, 10, 10, 10, 13, 4, 10, 13, 10, 9, 9, 13, 10, 10, 10, 6, 13, 9, 9, 13, 13, 13, 8, 13, 9, 9, 10, 13, 10, 7, 13, 14, 10, 9, 9, 9, 13, 9, 15, 11, 10, 6, 4, 9, 15, 9, 10, 13, 13, 9, 6, 13, 9, 9, 13, 10, 9, 9, 9, 9, 13, 13, 9, 7, 7, 9, 10, 16, 13, 11, 16, 12, 4, 3, 9, 9, 7, 6, 10, 9, 9, 9, 13, 13, 11, 10, 9, 10, 10, 10, 13, 14, 10, 10, 10, 11, 9, 9, 6, 14, 10, 10, 13, 14, 10, 10, 13, 3, 7, 9, 10, 13, 11, 9, 9, 10, 9, 13, 9, 10, 13, 16, 10, 11, 13, 12, 13, 7, 6, 5, 14, 9, 9, 6, 13, 5, 10, 10, 9, 10, 9, 9, 13, 9, 10, 9, 13, 3, 9, 9, 10, 10, 13, 7, 13, 10, 6, 13, 14, 12, 10, 9, 9, 9, 10, 9, 10, 14, 13, 7, 10, 15, 7, 9, 9, 9, 13, 11, 11, 9, 14, 11, 6, 14, 9, 11, 14, 10, 9, 10, 9, 7, 13, 13, 9, 9, 13, 5, 10, 9, 13, 10, 4, 7, 7, 10, 13, 9, 9, 10, 9, 9, 9, 10, 9, 10, 14, 13, 9, 9, 9, 7, 9, 13, 13, 9, 9, 14, 9, 10, 14, 11, 12, 7, 9, 9, 16, 9, 3, 9, 1, 10, 13, 9, 12, 9, 9, 13, 10, 9, 15, 13, 10, 13, 9, 13, 9, 10, 10, 14, 10, 9, 9, 6, 13, 9, 14, 13, 7, 10, 14, 13, 9, 9, 9, 10, 9, 6, 9, 9, 8, 9, 11, 14, 9, 10, 14, 14, 15, 9, 14, 9, 9, 13, 5, 13, 9, 9, 9, 9, 13, 9, 10, 9, 13, 9, 9, 10, 10, 9, 13, 10, 9, 10, 4, 10, 9, 9, 13, 10, 13, 10, 10, 9, 14, 5, 9, 9, 14, 9, 10, 10, 16, 10, 9, 13, 13, 10, 9, 13, 10, 13, 13, 2, 6, 10, 9, 14, 14, 9, 9, 9, 6, 14, 14, 12, 13, 8, 14, 10, 14, 10, 11, 8, 9, 13, 9, 10, 9, 10, 9, 10, 13, 9, 6, 10, 10, 9, 9, 14, 2, 9, 9, 11, 10, 6, 14, 9, 11, 4, 9, 12, 10, 10, 10, 13, 10, 10, 9, 10, 10, 9, 12, 10, 9, 14, 8, 9, 9, 9, 14, 3, 14, 9, 11, 10, 10, 13, 9, 9, 9, 13, 10, 11, 9, 16, 16, 9, 9, 10, 6, 6, 10, 9, 13, 14, 10, 12, 9, 13, 9, 13, 9, 3, 9, 9, 10, 9, 10, 14, 9, 9, 7, 9, 13, 9, 9, 10, 9, 9, 9, 9, 9, 10, 9, 7, 13, 14, 13, 9, 13, 10, 3, 9, 10, 10, 9, 9, 10, 11, 13, 14, 13, 10, 12, 9, 11, 10, 2, 9, 10, 14, 9, 13, 9, 9, 9, 15, 9, 13, 9, 13, 13, 13, 10, 14, 11, 9, 9, 10, 8, 14, 11, 14, 13, 5, 10, 10, 9, 6, 13, 14, 9, 9, 16, 10, 10, 15, 13, 13, 10, 9, 6, 11, 10, 4, 10, 13, 4, 9, 10, 4, 10, 13, 10, 9, 9, 11, 9, 9, 13, 13, 7, 5, 9, 14, 13, 12, 13, 10, 14, 9, 7, 10, 15, 9, 9, 9, 14, 3, 9, 10, 9, 9, 10, 9, 9, 9, 6, 9, 16, 11, 9, 9, 9, 12, 9, 9, 9, 9, 11, 8, 9, 9, 9, 10, 11, 10, 9, 10, 10, 9, 4, 10, 9, 9, 9, 6, 9, 10, 9, 9, 10, 12, 14, 13, 14, 10, 4, 9, 15, 13, 10, 12, 2, 10, 9, 9, 10, 10, 10, 10, 6, 13, 9, 9, 3, 5, 9, 9, 9, 10, 9, 9, 9, 14, 9, 15, 10, 10, 10, 9, 10, 6, 9, 8, 10, 9, 13, 9, 9, 9, 10, 13, 9, 13, 10, 10, 9, 11, 9, 10, 9, 9, 10, 13, 10, 12, 13, 9, 14, 10, 9, 9, 6, 9, 13, 9, 5, 14, 13, 13, 10, 9, 8, 13, 9, 10, 9, 9, 9, 13, 10, 10, 7, 13, 10, 10, 14, 10, 10, 9, 9, 14, 12, 4, 9, 10, 13, 10, 4, 9, 13, 9, 10, 9, 14, 10, 7, 10, 14, 12, 9, 12, 10, 10, 9, 10, 10, 10, 9, 3, 9, 9, 5, 9, 9, 7, 13, 9, 10, 13, 9, 9, 14, 14, 9, 15, 13, 10, 3, 13, 7, 9, 9, 4, 9, 13, 9, 9, 10, 9, 13, 9, 14, 9, 3, 9, 9, 12, 9, 10, 11, 13, 9, 13, 10, 9, 9, 9, 9, 10, 13, 9, 16, 9, 9, 7, 7, 9, 13, 13, 12, 9, 9, 9, 10, 13, 9, 16, 10, 10, 4, 9, 9, 10, 10, 10, 4, 15, 13, 13, 9, 14, 7, 13, 10, 10, 10, 13, 9, 12, 10, 12, 9, 10, 4, 10, 4, 6, 9, 9, 14, 13, 16, 9, 12, 9, 9, 12, 11, 6, 9, 9, 13, 4, 9, 10, 5, 12, 14, 13, 13, 9, 10, 13, 10, 7, 14, 11, 9, 9, 6, 9, 10, 13, 13, 9, 9, 6, 4, 4, 13, 9, 9, 13, 10, 14, 10, 14, 13, 13, 13, 3, 9, 10, 10, 14, 9, 13, 16, 10, 7, 11, 14, 9, 10, 9, 10, 9, 9, 9, 9, 9, 9, 7, 10, 13, 8, 13, 9, 9, 11, 10, 14, 13, 9, 10, 7, 9, 2, 6, 8, 14, 9, 15, 13, 9, 10, 13, 10, 9, 9, 9, 13, 10, 13, 13, 9, 9, 9, 14, 13, 9, 13, 9, 10, 10, 14, 9, 14, 4, 13, 9, 9, 10, 10, 14, 13, 10, 9, 9, 9, 9, 13, 12, 13, 10, 9, 10, 11, 9, 3, 10, 9, 11, 9, 9, 11, 13, 9, 9, 10, 7, 9, 10, 10, 10, 10, 13, 11, 10, 9, 13, 13, 9, 10, 11, 10, 9, 10, 10, 5, 10, 10, 9, 13, 5, 10, 13, 10, 10, 9, 9, 15, 10, 9, 11, 13, 9, 7, 4, 10, 9, 9, 10, 9, 13, 6, 9, 13, 9, 10, 9, 10, 9, 9, 15, 9, 13, 9, 9, 13, 13, 10, 10, 12, 13, 9, 10, 13, 13, 11, 11, 9, 10, 13, 9, 9, 14, 4, 14, 10, 9, 14, 9, 7, 11, 9, 16, 9, 10, 9, 13, 9, 13, 13, 9, 13, 9, 14, 9, 10, 13, 9, 9, 10, 10, 9, 6, 10, 14, 9, 13, 10, 13, 16, 13, 13, 9, 13, 13, 13, 7, 5, 6, 14, 10, 9, 10, 10, 6, 3, 10, 10, 2, 13, 8, 15, 9, 14, 16, 9, 10, 9, 10, 9, 10, 9, 14, 9, 13, 9, 13, 7, 13, 11, 10, 13, 7, 13, 9, 9, 13, 9, 9, 9, 9, 7, 10, 11, 9, 10, 9, 10, 13, 12, 10, 12, 9, 9, 10, 13, 8, 13, 14, 4, 9, 13, 12, 11, 10, 10, 16, 8, 13, 10, 8, 9, 10, 13, 15, 7, 9, 11, 12, 10, 9, 13, 10, 13, 10, 10, 9, 13, 10, 10, 9, 9, 9, 14, 9, 9, 9, 10, 12, 11, 10, 15, 9, 16, 16, 10, 9, 10, 10, 9, 10, 8, 15, 9, 13, 14, 10, 10, 13, 15, 9, 14, 9, 10, 13, 11, 9, 10, 9, 10, 9, 9, 9, 14, 10, 12, 10, 9, 9, 12, 13, 9, 9, 9, 8, 12, 9, 14, 14, 10, 7, 13, 10, 14, 9, 10, 10, 10, 14, 16, 9, 10, 10, 14, 9, 13, 10, 9, 9, 10, 10, 14, 2, 10, 9, 10, 7, 16, 6, 12, 10, 4, 13, 13, 10, 9, 13, 13, 9, 13, 13, 6, 10, 9, 9, 13, 13, 10, 10, 7, 9, 10, 7, 13, 10, 13, 12, 10, 14, 9, 9, 9, 14, 13, 11, 10, 12, 10, 10, 14, 14, 11, 10, 8, 7, 9, 16, 8, 11, 9, 12, 12, 13, 6, 9, 4, 9, 10, 10, 9, 9, 10, 15, 13, 13, 10, 4, 9, 9, 9, 10, 13, 9, 10, 14, 9, 13, 10, 10, 3, 11, 9, 9, 14, 13, 10, 12, 10, 13, 9, 6, 14, 9, 8, 10, 9, 9, 11, 11, 10, 7, 12, 14, 9, 14, 13, 10, 9, 13, 9, 9, 13, 13, 9, 7, 13, 6, 7, 9, 9, 6, 14, 13, 9, 9, 10, 10, 12, 7, 10, 9, 10, 9, 13, 7, 6, 9, 9, 13, 9, 9, 9, 10, 9, 9, 9, 13, 9, 10, 9, 7, 11, 10, 9, 16, 10, 10, 14, 9, 9, 6, 9, 14, 9, 10, 7, 10, 9, 10, 11, 13, 9, 10, 16, 9, 13, 9, 9, 10, 10, 7, 10, 9, 9, 6, 9, 9, 13, 11, 9, 6, 13, 13, 9, 9, 6, 10, 9, 12, 9, 6, 10, 11, 10, 10, 9, 10, 15, 13, 16, 9, 13, 9, 10, 15, 9, 9, 9, 6, 14, 13, 11, 12, 9, 15, 15, 10, 10, 9, 6, 9, 4, 8, 7, 3, 12, 10, 9, 9, 9, 13, 10, 13, 9, 8, 10, 4, 5, 9, 9, 9, 10, 5, 9, 9, 10, 10, 13, 8, 10, 9, 13, 14, 7, 9, 14, 5, 9, 9, 5, 5, 13, 10, 9, 10, 9, 9, 4, 12, 13, 10, 8, 13, 13, 9, 9, 9, 4, 9, 9, 9, 10, 13, 2, 9, 10, 9, 11, 4, 11, 7, 13, 10, 10, 14, 13, 13, 9, 10, 10, 4, 9, 9, 7, 9, 9, 14, 9, 13, 10, 16, 13, 13, 13, 9, 14, 9, 10, 10, 14, 14, 3, 14, 9, 12, 6, 10, 13, 6, 13, 9, 14, 9, 14, 10, 10, 12, 10, 13, 11, 8, 3, 13, 7, 10, 13, 11, 9, 9, 9, 9, 9, 9, 9, 6, 7, 9, 12, 10, 13, 10, 13, 15, 11, 11, 14, 7, 9, 4, 14, 10, 13, 10, 9, 9, 9, 10, 14, 10, 10, 11, 9, 10, 9, 9, 13, 9, 9, 10, 11, 13, 13, 10, 11, 12, 13, 13, 9, 9, 15, 5, 13, 9, 13, 11, 14, 13, 13, 13, 13, 9, 9, 13, 11, 13, 13, 10, 6, 9, 10, 10, 13, 3, 13, 10, 9, 6, 13, 13, 9, 13, 5, 14, 10, 10, 9, 6, 13, 9, 8, 13, 10, 10, 9, 14, 7, 10, 1, 9, 9, 9, 13, 9, 9, 5, 9, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 10, 9, 13, 14, 13, 13, 13, 9, 13, 13, 9, 9, 10, 9, 10, 13, 10, 10, 9, 3, 15, 9, 13, 13, 13, 14, 13, 13, 2, 9, 9, 10, 9, 12, 10, 3, 10, 13, 12, 13, 9, 10, 9, 13, 7, 9, 13, 9, 9, 12, 10, 13, 6, 10, 9, 6, 9, 10, 12, 10, 9, 10, 10, 2, 10, 9, 13, 10, 7, 10, 9, 12, 13, 10, 9, 13, 13, 8, 9, 9, 9, 10, 9, 11, 12, 13, 9, 10, 10, 9, 10, 10, 6, 13, 13, 9, 9, 9, 10, 9, 13, 13, 9, 14, 10, 9, 10, 10, 13, 14, 13, 14, 13, 11, 13, 10, 13, 13, 13, 9, 9, 10, 10, 13, 10, 6, 13, 10, 13, 6, 14, 9, 9, 9, 9, 13, 10, 9, 9, 13, 10, 12, 10, 11, 13, 9, 14, 13, 9, 10, 7, 11, 13, 9, 10, 13, 13, 10, 9, 11, 13, 9, 10, 8, 9, 9, 10, 13, 9, 5, 9, 13, 10, 10, 9, 10, 15, 9, 9, 13, 10, 9, 5, 10, 16, 10, 13, 14, 10, 4, 9, 4, 10, 14, 10, 9, 13, 10, 13, 16, 9, 12, 10, 4, 10, 7, 9, 9, 10, 12, 12, 10, 14, 7, 10, 9, 14, 9, 9, 10, 14, 11, 9, 9, 9, 13, 9, 10, 13, 13, 13, 15, 13, 9, 13, 11, 13, 3, 14, 13, 9, 10, 9, 10, 11, 9, 10, 13, 7, 9, 12, 9, 9, 9, 13, 10, 15, 9, 10, 13, 9, 13, 9, 9, 9, 11, 14, 9, 9, 6, 9, 6, 13, 11, 4, 2, 10, 12, 10, 10, 14, 11, 15, 9, 16, 9, 9, 9, 12, 10, 13, 15, 13, 13, 13, 9, 10, 10, 13, 9, 11, 9, 8, 14, 9, 9, 9, 10, 10, 9, 13, 10, 10, 9, 10, 9, 13, 9, 9, 7, 5, 11, 10, 16, 13, 10, 13, 10, 12, 9, 13, 10, 13, 2, 5, 5, 9, 9, 7, 7, 10, 5, 6, 9, 10, 9, 13, 13, 7, 12, 6, 11, 10, 9, 9, 13, 9, 13, 15, 6, 9, 13, 10, 10, 14, 9, 9, 8, 13, 7, 11, 9, 4, 14, 10, 10, 9, 13, 10, 9, 9, 9, 10, 13, 13, 9, 10, 14, 13, 9, 10, 5, 9, 14, 13, 9, 10, 2, 13, 9, 13, 5, 9, 9, 10, 9, 10, 9, 12, 9, 10, 13, 10, 13, 9, 2, 9, 11, 13, 9, 12, 9, 9, 13, 13, 13, 9, 3, 14, 9, 9, 10, 9, 13, 9, 8, 10, 7, 13, 7, 11, 10, 9, 9, 10, 9, 12, 10, 9, 14, 9, 10, 10, 9, 9, 13, 10, 13, 9, 10, 9, 9, 13, 9, 10, 10, 9, 9, 9, 9, 9, 15, 10, 7, 4, 7, 9, 7, 7, 10, 14, 14, 9, 13, 9, 9, 10, 10, 10, 9, 10, 9, 2, 8, 9, 13, 9, 9, 6, 6, 10, 9, 9, 12, 9, 13, 10, 9, 10, 9, 13, 13, 13, 13, 13, 13, 10, 15, 13, 10, 13, 16, 12, 13, 9, 10, 10, 9, 10, 9, 9, 10, 13, 9, 13, 6, 9, 12, 13, 11, 7, 10, 9, 13, 9, 6, 13, 10, 10, 13, 3, 9, 9, 9, 6, 11, 4, 9, 6, 13, 13, 11, 9, 9, 9, 9, 10, 3, 11, 13, 9, 10, 9, 12, 15, 11, 9, 9, 13, 13, 13, 9, 7, 3, 9, 12, 10, 9, 10, 10, 10, 16, 9, 12, 9, 13, 10, 11, 9, 9, 9, 11, 9, 10, 10, 13, 9, 14, 9, 9, 10, 10, 6, 10, 9, 9, 14, 9, 9, 9, 12, 7, 9, 9, 10, 13, 7, 11, 9, 9, 14, 9, 9, 12, 14, 9, 10, 13, 10, 13, 7, 9, 9, 9, 4, 10, 12, 10, 9, 6, 9, 13, 10, 10, 10, 11, 9, 16, 4, 9, 10, 13, 9, 9, 14, 10, 9, 10, 9, 4, 9, 10, 9, 14, 13, 10, 10, 9, 11, 10, 13, 10, 10, 10, 11, 7, 8, 11, 9, 3, 9, 10, 9, 13, 9, 10, 13, 9, 13, 14, 9, 7, 10, 7, 13, 13, 12, 13, 14, 9, 10, 11, 9, 10, 9, 13, 14, 9, 9, 9, 10, 12, 9, 9, 15, 10, 9, 9, 13, 9, 14, 13, 7, 9, 12, 9, 10, 14, 9, 11, 10, 13, 9, 13, 10, 10, 9, 10, 14, 10, 13, 8, 9, 10, 13, 13, 13, 10, 9, 9, 10, 13, 10, 10, 9, 9, 11, 9, 11, 9, 13, 9, 12, 9, 15, 14, 9, 9, 9, 9, 9, 9, 13, 13, 7, 7, 7, 9, 9, 10, 9, 14, 9, 10, 13, 10, 13, 9, 10, 9, 9, 9, 13, 10, 10, 10, 10, 9, 8, 9, 10, 9, 9, 9, 9, 9, 13, 2, 8, 10, 10, 3, 6, 9, 13, 9, 10, 6, 10, 14, 3, 9, 9, 10, 13, 10, 10, 15, 9, 6, 13, 9, 9, 10, 10, 10, 13, 9, 10, 9, 15, 10, 9, 15, 9, 10, 3, 9, 8, 9, 9, 12, 9, 13, 9, 9, 9, 9, 10, 9, 9, 9, 9, 13, 6, 13, 13, 12, 13, 5, 13, 13, 9, 9, 10, 10, 10, 9, 6, 10, 9, 9, 12, 10, 9, 12, 9, 10, 7, 9, 10, 13, 16, 10, 9, 13, 9, 12, 9, 9, 8, 9, 13, 14, 14, 9, 9, 10, 9, 4, 10, 9, 9, 4, 7, 16, 5, 9, 9, 10, 13, 9, 10, 9, 9, 11, 10, 9, 9, 13, 13, 13, 14, 13, 11, 6, 7, 9, 13, 13, 13, 9, 10, 10, 10, 4, 9, 13, 9, 13, 7, 4, 14, 9, 10, 10, 10, 9, 13, 11, 10, 15, 9, 10, 9, 9, 9, 9, 10, 2, 13, 13, 9, 16, 3, 13, 11, 10, 9, 10, 11, 8, 15, 12, 9, 11, 10, 5, 7, 13, 9, 10, 9, 10, 7, 13, 14, 13, 9, 4, 14, 9, 7, 10, 5, 13, 10, 6, 15, 9, 6, 9, 6, 13, 9, 10, 9, 14, 9, 10, 13, 5, 10, 13, 9, 15, 10, 6, 14, 10, 12, 11, 9, 5, 9, 10, 11, 13, 3, 10, 10, 13, 7, 14, 10, 9, 11, 9, 13, 13, 2, 13, 13, 16, 9, 10, 10, 9, 9, 9, 9, 9, 10, 8, 9, 13, 13, 10, 10, 14, 13, 10, 10, 10, 13, 13, 9, 9, 6, 9, 13, 9, 10, 9, 14, 9, 11, 10, 10, 15, 13, 9, 9, 12, 10, 10, 9, 9, 9, 10, 12, 9, 14, 13, 9, 9, 13, 13, 9, 13, 12, 9, 13, 11, 9, 9, 9, 14, 10, 13, 9, 10, 13, 9, 9, 9, 13, 7, 9, 13, 9, 11, 13, 13, 10, 9, 9, 9, 14, 6, 13, 9, 11, 16, 9, 9, 9, 9, 9, 6, 9, 13, 8, 7, 9, 10, 11, 9, 9, 13, 11, 9, 7, 5, 9, 10, 10, 9, 13, 9, 14, 9, 10, 9, 10, 12, 10, 7, 10, 10, 10, 13, 3, 13, 10, 9, 5, 13, 9, 13, 9, 13, 14, 10, 9, 9, 12, 11, 7, 13, 9, 16, 9, 10, 9, 9, 13, 9, 3, 9, 9, 13, 11, 10, 10, 9, 11, 9, 10, 6, 9, 5, 13, 9, 9, 3, 10, 9, 10, 9, 10, 9, 9, 9, 9, 7, 13, 6, 7, 9, 9, 10, 14, 5, 14, 11, 13, 9, 9, 9, 12, 7, 13, 9, 4, 6, 10, 9, 10, 9, 10, 9, 10, 9, 9, 10, 9, 5, 3, 14, 9, 9, 6, 10, 13, 14, 7, 13, 7, 5, 10, 16, 13, 9, 9, 11, 12, 15, 9, 10, 9, 13, 9, 13, 13, 13, 15, 4, 10, 13, 9, 13, 14, 6, 9, 3, 15, 14, 10, 13, 10, 14, 13, 9, 10, 7, 13, 9, 9, 10, 13, 9, 9, 9, 13, 9, 13, 10, 7, 10, 10, 9, 10, 13, 9, 7, 9, 13, 13, 9, 13, 4, 9, 12, 10, 10, 9, 13, 9, 10, 12, 10, 10, 13, 11, 11, 9, 10, 14, 9, 13, 12, 13, 12, 9, 4, 9, 10, 13, 9, 11, 10, 9, 14, 9, 13, 12, 9, 9, 14, 10, 13, 9, 9, 14, 7, 10, 10, 13, 9, 13, 13, 10, 9, 9, 5, 14, 10, 10, 9, 5, 9, 9, 9, 9, 10, 13, 9, 14, 9, 12, 7, 11, 9, 4, 9, 10, 10, 10, 6, 10, 14, 9, 13, 9, 9, 10, 9, 6, 9, 16, 9, 9, 9, 11, 13, 14, 13, 11, 10, 9, 9, 15, 7, 3, 8, 4, 5, 13, 13, 9, 15, 10, 9, 10, 10, 10, 9, 15, 10, 9, 13, 13, 10, 9, 10, 9, 14, 9, 13, 9, 10, 13, 16, 11, 11, 9, 9, 10, 9, 13, 10, 11, 13, 13, 9, 9, 11, 7, 13, 9, 11, 7, 10, 9, 6, 9, 14, 9, 7, 10, 9, 14, 9, 14, 9, 11, 13, 7, 3, 11, 13, 12, 6, 9, 9, 13, 13, 9, 13, 9, 9, 9, 12, 9, 11, 14, 11, 10, 9, 9, 13, 9, 11, 10, 6, 9, 6, 15, 13, 10, 13, 9, 10, 14, 14, 7, 10, 11, 10, 13, 9, 11, 9, 14, 7, 9, 9, 15, 10, 10, 9, 14, 9, 10, 13, 10, 10, 10, 13, 14, 9, 10, 9, 5, 10, 9, 6, 16, 13, 9, 9, 9, 11, 10, 9, 11, 10, 10, 13, 9, 9, 13, 13, 9, 11, 12, 10, 10, 9, 2, 13, 4, 10, 13, 13, 13, 4, 13, 10, 7, 7, 10, 9, 14, 13, 3, 13, 3, 7, 10, 9, 9, 4, 10, 12, 9, 13, 7, 14, 13, 7, 3, 11, 4, 9, 3, 13, 11, 13, 16, 9, 13, 9, 10, 9, 4, 10, 10, 10, 10, 13, 10, 10, 16, 13, 13, 11, 3, 6, 9, 16, 10, 10, 5, 9, 9, 9, 7, 10, 10, 14, 15, 6, 7, 10, 13, 12, 8, 9, 10, 14, 9, 14, 13, 9, 15, 13, 11, 9, 9, 9, 7, 9, 9, 3, 11, 9, 10, 9, 9, 9, 9, 3, 13, 10, 10, 13, 14, 9, 9, 9, 13, 16, 9, 13, 13, 13, 10, 10, 14, 9, 13, 10, 14, 11, 12, 11, 13, 13, 13, 2, 13, 9, 10, 3, 9, 10, 13, 9, 9, 13, 13, 3, 13, 10, 4, 10, 7, 7, 9, 4, 10, 9, 10, 10, 10, 16, 8, 10, 13, 10, 9, 9, 13, 9, 9, 10, 10, 7, 9, 10, 14, 9, 10, 9, 9, 14, 9, 10, 9, 9, 1, 9, 10, 11, 10, 10, 10, 6, 10, 13, 10, 13, 9, 13, 13, 13, 14, 9, 12, 13, 10, 9, 9, 11, 13, 13, 13, 9, 13, 9, 13, 11, 9, 10, 13, 7, 9, 10, 13, 13, 11, 13, 12, 10, 9, 9, 10, 4, 13, 14, 7, 11, 10, 9, 6, 9, 10, 11, 10, 13, 13, 10, 9, 16, 5, 6, 10, 13, 10, 3, 11, 6, 13, 13, 7, 9, 9, 13, 9, 9, 10, 10, 16, 9, 10, 11, 9, 13, 9, 7, 14, 10, 12, 13, 13, 9, 10, 10, 11, 9, 9, 13, 13, 4, 10, 9, 9, 9, 9, 13, 6, 10, 3, 13, 9, 13, 13, 2, 9, 9, 13, 7, 5, 10, 11, 9, 9, 14, 10, 14, 10, 12, 14, 13, 9, 9, 10, 11, 4, 6, 10, 6, 10, 11, 9, 16, 11, 10, 9, 10, 13, 10, 11, 13, 13, 9, 13, 9, 9, 9, 13, 14, 10, 9, 12, 9, 13, 11, 15, 10, 10, 9, 13, 9, 9, 9, 13, 9, 10, 13, 10, 10, 8, 11, 9, 10, 10, 5, 9, 13, 9, 13, 2, 14, 14, 9, 9, 9, 14, 10, 13, 5, 13, 13, 5, 10, 9, 7, 10, 10, 9, 9, 10, 9, 13, 11, 13, 9, 9, 10, 16, 12, 9, 7, 13, 10, 10, 11, 12, 9, 12, 13, 4, 9, 7, 9, 9, 12, 9, 9, 9, 9, 10, 10, 9, 5, 9, 14, 9, 10, 9, 7, 10, 7, 9, 9, 16, 10, 10, 13, 10, 16, 9, 12, 14, 10, 10, 14, 13, 9, 9, 10, 13, 13, 9, 15, 13, 6, 9, 10, 13, 13, 14, 3, 14, 7, 14, 11, 14, 5, 13, 6, 9, 13, 9, 9, 10, 9, 7, 9, 9, 10, 9, 13, 3, 11, 13, 9, 10, 15, 9, 9, 9, 10, 7, 10, 9, 9, 9, 10, 9, 5, 9, 14, 13, 9, 9, 10, 14, 13, 9, 10, 9, 14, 9, 13, 9, 7, 11, 10, 10, 10, 9, 9, 10, 9, 9, 9, 11, 14, 11, 10, 12, 11, 9, 9, 13, 9, 12, 13, 13, 10, 11, 9, 6, 5, 13, 14, 10, 9, 9, 10, 10, 10, 13, 12, 13, 16, 9, 9, 11, 6, 13, 9, 9, 9, 13, 11, 9, 15, 9, 13, 9, 9, 9, 10, 9, 10, 9, 5, 10, 13, 13, 12, 9, 7, 10, 10, 13, 8, 9, 11, 6, 9, 7, 9, 9, 9, 10, 13, 5, 9, 10, 9, 9, 9, 15, 9, 13, 10, 13, 9, 9, 9, 9, 15, 13, 9, 16, 4, 12, 9, 6, 10, 9, 13, 9, 10, 9, 13, 9, 10, 10, 10, 9, 10, 13, 10, 13, 7, 10, 9, 13, 9, 9, 10, 13, 9, 7, 10, 9, 13, 10, 9, 9, 9, 10, 10, 10, 9, 9, 6, 10, 6, 13, 10, 9, 14, 3, 8, 9, 13, 14, 13, 15, 10, 9, 14, 5, 13, 3, 9, 13, 13, 13, 10, 9, 10, 7, 9, 9, 9, 10, 9, 4, 2, 10, 15, 13, 7, 13, 7, 13, 10, 12, 11, 10, 12, 9, 9, 14, 13, 14, 10, 5, 14, 14, 9, 12, 9, 9, 13, 12, 13, 13, 13, 6, 9, 9, 13, 9, 12, 9, 10, 11, 10, 13, 9, 10, 15, 13, 10, 9, 9, 10, 13, 9, 15, 13, 6, 13, 13, 9, 11, 14, 10, 9, 10, 10, 15, 10, 9, 15, 9, 10, 14, 10, 9, 11, 10, 11, 10, 13, 9, 9, 13, 11, 13, 9, 10, 13, 10, 13, 9, 10, 15, 9, 13, 9, 14, 13, 13, 9, 9, 13, 11, 10, 9, 9, 13, 14, 10, 9, 9, 10, 9, 10, 9, 9, 12, 11, 9, 14, 13, 9, 10, 9, 7, 9, 14, 9, 9, 10, 9, 6, 13, 10, 14, 9, 16, 9, 7, 9, 13, 13, 12, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 16, 10, 13, 14, 9, 10, 13, 10, 9, 14, 9, 13, 14, 9, 12, 9, 10, 11, 16, 13, 11, 9, 13, 9, 10, 11, 9, 9, 7, 9, 14, 13, 11, 14, 9, 9, 9, 5, 10, 7, 13, 13, 14, 9, 5, 4, 9, 10, 13, 13, 9, 7, 9, 12, 6, 10, 16, 10, 15, 9, 9, 4, 13, 14, 11, 13, 9, 10, 9, 9, 5, 9, 10, 9, 11, 10, 12, 9, 9, 12, 9, 9, 9, 9, 9, 13, 15, 9, 9, 12, 10, 10, 9, 13, 9, 12, 10, 13, 8, 10, 9, 4, 9, 9, 14, 16, 10, 14, 9, 13, 2, 9, 7, 6, 9, 9, 4, 9, 9, 4, 14, 7, 9, 7, 10, 10, 9, 10, 12, 13, 10, 9, 13, 4, 9, 9, 11, 9, 10, 9, 13, 9, 11, 11, 10, 13, 9, 10, 9, 13, 13, 15, 9, 13, 9, 9, 9, 9, 9, 10, 12, 7, 13, 10, 4, 13, 9, 9, 10, 9, 9, 8, 10, 13, 9, 12, 9, 6, 9, 6, 13, 9, 14, 10, 10, 9, 9, 12, 12, 10, 10, 10, 10, 13, 10, 8, 10, 6, 9, 13, 9, 14, 13, 10, 13, 13, 6, 9, 9, 10, 9, 9, 6, 16, 11, 6, 9, 13, 9, 9, 10, 16, 4, 10, 9, 13, 10, 15, 9, 10, 10, 10, 9, 9, 13, 9, 4, 13, 14, 9, 9, 10, 9, 9, 10, 10, 10, 13, 10, 9, 9, 10, 8, 10, 9, 9, 10, 9, 9, 10, 6, 14, 10, 5, 9, 9, 9, 13, 9, 9, 9, 10, 9, 10, 13, 13, 10, 9, 9, 13, 14, 13, 9, 12, 9, 13, 10, 4, 9, 9, 12, 9, 11, 4, 9, 9, 9, 10, 13, 9, 10, 10, 13, 9, 9, 10, 9, 9, 9, 14, 9, 7, 10, 9, 10, 13, 12, 9, 11, 9, 7, 4, 9, 10, 15, 9, 14, 9, 10, 10, 10, 9, 9, 13, 9, 10, 9, 13, 3, 13, 9, 9, 9, 9, 9, 10, 7, 10, 13, 2, 13, 13, 14, 10, 10, 13, 16, 10, 4, 8, 9, 10, 10, 6, 10, 13, 13, 13, 9, 6, 10, 10, 9, 13, 9, 10, 2, 9, 13, 12, 13, 5, 13, 10, 13, 10, 14, 4, 10, 9, 10, 10, 10, 13, 9, 9, 5, 9, 6, 9, 9, 9, 9, 9, 8, 12, 13, 14, 9, 7, 12, 4, 10, 9, 10, 3, 14, 9, 14, 10, 11, 9, 6, 10, 9, 14, 13, 9, 10, 10, 9, 7, 9, 9, 13, 5, 9, 10, 9, 9, 9, 13, 4, 13, 10, 9, 10, 13, 11, 15, 14, 9, 10, 10, 9, 9, 4, 13, 10, 9, 9, 13, 7, 13, 9, 13, 11, 14, 7, 10, 4, 9, 9, 10, 9, 10, 9, 13, 10, 10, 9, 13, 10, 5, 14, 13, 9, 13, 9, 9, 10, 10, 9, 13, 10, 9, 7, 10, 5, 7, 9, 9, 14, 9, 13, 10, 9, 9, 5, 11, 9, 10, 10, 11, 11, 9, 13, 9, 12, 15, 11, 9, 9, 5, 12, 9, 9, 10, 7, 6, 9, 9, 13, 16, 13, 10, 9, 12, 14, 9, 13, 15, 15, 4, 11, 10, 11, 9, 9, 9, 10, 13, 11, 9, 11, 9, 9, 12, 8, 12, 10, 13, 9, 9, 10, 6, 8, 9, 9, 9, 9, 10, 10, 9, 14, 9, 10, 9, 10, 9, 9, 10, 9, 13, 14, 7, 13, 9, 9, 13, 5, 9, 9, 10, 9, 10, 10, 9, 11, 13, 9, 13, 10, 13, 11, 11, 14, 16, 13, 14, 9, 9, 13, 10, 10, 10, 11, 10, 9, 4, 13, 9, 9, 14, 12, 13, 9, 7, 13, 9, 14, 9, 9, 13, 13, 9, 13, 3, 8, 11, 3, 9, 7, 14, 10, 10, 12, 9, 9, 13, 13, 9, 9, 13, 10, 9, 10, 9, 13, 9, 7, 9, 13, 12, 9, 14, 13, 13, 13, 9, 9, 4, 7, 10, 9, 9, 13, 16, 9, 11, 13, 10, 13, 8, 6, 10, 9, 9, 13, 9, 13, 13, 9, 9, 12, 15, 9, 9, 9, 7, 8, 13, 9, 9, 10, 7, 9, 9, 10, 13, 9, 11, 14, 6, 12, 9, 14, 9, 6, 13, 13, 14, 11, 9, 2, 9, 9, 9, 10, 9, 13, 9, 9, 13, 10, 14, 7, 4, 7, 11, 13, 10, 7, 9, 16, 12, 9, 15, 9, 5, 6, 4, 10, 10, 13, 16, 9, 13, 13, 13, 9, 13, 10, 9, 13, 13, 10, 3, 5, 14, 1, 10, 9, 16, 9, 16, 10, 13, 3, 10, 9, 9, 9, 9, 9, 9, 10, 7, 10, 10, 9, 9, 13, 14, 14, 10, 9, 9, 16, 13, 10, 9, 8, 4, 10, 7, 9, 9, 13, 9, 9, 10, 13, 9, 9, 14, 15, 10, 10, 11, 9, 13, 9, 12, 10, 9, 9, 5, 4, 9, 13, 12, 10, 9, 13, 1, 9, 10, 11, 13, 9, 9, 9, 9, 7, 9, 16, 3, 9, 10, 8, 14, 8, 10, 9, 13, 10, 10, 13, 9, 13, 13, 14, 12, 10, 9, 9, 4, 14, 9, 13, 7, 5, 10, 13, 9, 12, 4, 4, 7, 13, 13, 10, 9, 9, 13, 13, 16, 9, 9, 11, 10, 14, 13, 13, 10, 14, 9, 9, 10, 15, 10, 13, 13, 13, 9, 10, 9, 14, 9, 14, 13, 10, 12, 10, 9, 11, 9, 13, 9, 11, 7, 13, 13, 9, 9, 14, 10, 13, 10, 16, 13, 10, 13, 10, 10, 13, 10, 10, 7, 15, 12, 9, 12, 9, 6, 10, 10, 9, 9, 10, 10, 10, 10, 6, 13, 9, 11, 10, 9, 10, 10, 13, 10, 9, 9, 9, 9, 11, 14, 10, 13, 13, 14, 9, 14, 10, 11, 9, 13, 13, 15, 9, 7, 10, 10, 9, 9, 9, 14, 13, 10, 14, 10, 10, 11, 10, 9, 10, 9, 10, 9, 13, 6, 9, 13, 9, 5, 6, 9, 13, 10, 11, 16, 9, 13, 6, 13, 9, 10, 13, 10, 9, 10, 5, 14, 9, 13, 9, 6, 10, 6, 13, 10, 11, 9, 9, 7, 13, 9, 9, 9, 10, 7, 11, 10, 9, 9, 8, 9, 9, 14, 13, 10, 7, 13, 13, 4, 12, 9, 7, 16, 13, 13, 9, 10, 7, 10, 9, 11, 13, 9, 9, 14, 9, 13, 14, 9, 9, 9, 10, 13, 10, 13, 13, 9, 3, 11, 9, 13, 9, 9, 9, 10, 9, 10, 9, 10, 9, 9, 12, 8, 13, 9, 9, 13, 9, 13, 11, 10, 14, 10, 10, 9, 10, 6, 11, 13, 9, 9, 7, 13, 14, 10, 9, 10, 13, 8, 9, 4, 9, 10, 9, 9, 10, 14, 13, 13, 11, 9, 11, 12, 10, 13, 9, 9, 14, 10, 9, 10, 15, 10, 9, 10, 10, 9, 9, 13, 10, 10, 8, 14, 12, 10, 10, 9, 5, 13, 13, 7, 9, 13, 9, 7, 7, 9, 13, 6, 9, 12, 10, 13, 14, 16, 9, 10, 10, 13, 13, 9, 9, 14, 13, 13, 5, 13, 11, 13, 10, 10, 9, 13, 9, 10, 10, 6, 9, 12, 6, 13, 10, 13, 10, 7, 13, 5, 10, 12, 11, 13, 10, 13, 9, 9, 9, 9, 12, 10, 9, 12, 2, 10, 5, 10, 13, 14, 13, 14, 10, 4, 10, 4, 12, 13, 13, 9, 12, 9, 9, 13, 5, 6, 14, 4, 5, 8, 9, 9, 7, 5, 13, 9, 14, 11, 13, 4, 9, 7, 11, 10, 10, 16, 8, 14, 10, 13, 13, 9, 6, 10, 10, 10, 9, 9, 9, 7, 13, 10, 13, 14, 13, 14, 13, 9, 2, 15, 13, 10, 9, 9, 9, 11, 10, 10, 8, 11, 10, 5, 12, 9, 9, 11, 10, 9, 9, 13, 10, 7, 10, 10, 7, 13, 9, 9, 10, 14, 14, 9, 10, 7, 7, 12, 10, 5, 11, 13, 9, 10, 9, 9, 9, 9, 13, 11, 9, 9, 13, 5, 6, 9, 13, 13, 9, 9, 11, 9, 11, 4, 9, 9, 7, 13, 9, 9, 3, 9, 9, 13, 9, 10, 9, 9, 10, 6, 12, 13, 9, 10, 6, 9, 13, 10, 9, 13, 13, 14, 9, 9, 10, 13, 9, 10, 9, 10, 10, 15, 9, 14, 12, 14, 9, 9, 9, 13, 10, 11, 9, 16, 13, 10, 9, 5, 5, 10, 9, 14, 11, 9, 9, 9, 13, 10, 9, 13, 7, 9, 13, 14, 10, 9, 9, 10, 13, 9, 10, 13, 9, 11, 10, 9, 10, 9, 11, 13, 7, 15, 13, 13, 10, 10, 10, 13, 14, 10, 13, 9, 9, 13, 11, 10, 9, 6, 14, 10, 6, 9, 10, 9, 10, 13, 9, 9, 13, 13, 14, 13, 9, 10, 10, 7, 10, 9, 9, 13, 10, 4, 14, 10, 9, 12, 10, 9, 9, 10, 13, 9, 9, 13, 7, 13, 10, 14, 10, 9, 4, 10, 9, 9, 4, 10, 9, 10, 13, 14, 9, 9, 9, 10, 9, 9, 10, 9, 13, 4, 10, 11, 9, 16, 3, 9, 10, 9, 15, 13, 13, 4, 10, 7, 12, 11, 9, 8, 9, 9, 13, 10, 9, 11, 6, 6, 14, 10, 13, 10, 9, 10, 10, 9, 10, 14, 9, 9, 9, 9, 9, 9, 9, 9, 10, 9, 13, 9, 16, 12, 10, 9, 7, 7, 10, 13, 9, 16, 10, 4, 9, 10, 10, 10, 14, 14, 9, 15, 13, 11, 6, 9, 11, 10, 10, 10, 13, 7, 9, 14, 13, 9, 9, 6, 9, 11, 13, 9, 13, 9, 10, 9, 9, 3, 11, 10, 9, 13, 10, 9, 12, 9, 10, 10, 11, 9, 9, 14, 13, 13, 13, 7, 9, 13, 9, 10, 14, 13, 9, 14, 9, 10, 13, 9, 9, 10, 14, 13, 10, 11, 9, 10, 5, 2, 9, 10, 13, 14, 9, 3, 10, 9, 14, 10, 9, 9, 10, 9, 7, 9, 10, 13, 7, 6, 13, 5, 6, 13, 13, 9, 9, 13, 9, 10, 9, 9, 9, 13, 13, 12, 13, 14, 10, 9, 9, 13, 9, 9, 13, 9, 9, 10, 10, 9, 4, 9, 10, 13, 9, 7, 9, 13, 10, 13, 2, 13, 10, 9, 10, 9, 10, 4, 6, 9, 6, 9, 10, 10, 13, 9, 10, 9, 9, 9, 9, 10, 7, 10, 10, 10, 10, 4, 10, 4, 10, 9, 6, 9, 10, 16, 10, 15, 13, 10, 14, 6, 9, 9, 15, 3, 9, 10, 10, 5, 9, 9, 9, 9, 11, 14, 10, 14, 9, 9, 13, 9, 10, 13, 9, 13, 9, 13, 9, 13, 9, 9, 10, 9, 10, 16, 10, 9, 10, 13, 13, 9, 13, 13, 13, 9, 4, 6, 9, 9, 6, 10, 13, 15, 14, 16, 9, 13, 7, 9, 12, 9, 9, 12, 9, 10, 13, 9, 10, 14, 10, 13, 13, 10, 9, 9, 3, 13, 13, 6, 6, 8, 13, 9, 9, 13, 9, 11, 10, 13, 6, 10, 9, 9, 9, 7, 10, 10, 9, 10, 9, 10, 10, 9, 9, 9, 9, 10, 9, 9, 10, 9, 13, 12, 13, 9, 11, 12, 9, 9, 9, 10, 9, 9, 8, 4, 4, 13, 9, 9, 9, 6, 9, 9, 4, 10, 14, 16, 10, 13, 9, 13, 13, 7, 13, 9, 9, 9, 4, 12, 10, 3, 10, 13, 10, 10, 9, 9, 7, 10, 9, 13, 13, 13, 10, 13, 9, 13, 13, 14, 9, 10, 11, 10, 9, 10, 11, 13, 13, 13, 10, 9, 10, 8, 10, 9, 13, 15, 9, 9, 10, 7, 10, 14, 12, 9, 10, 9, 9, 10, 14, 12, 13, 4, 10, 14, 3, 11, 10, 9, 6, 9, 9, 9, 9, 9, 10, 13, 13, 14, 11, 9, 12, 3, 14, 9, 9, 9, 10, 11, 9, 9, 13, 9, 9, 14, 10, 7, 7, 9, 9, 8, 11, 12, 11, 9, 9, 10, 13, 10, 10, 13, 4, 5, 10, 12, 13, 10, 10, 16, 11, 7, 13, 10, 8, 9, 9, 10, 2, 13, 14, 15, 12, 9, 13, 15, 13, 9, 11, 9, 9, 9, 10, 13, 9, 10, 9, 9, 9, 9, 10, 10, 9, 9, 10, 11, 16, 9, 9, 5, 9, 11, 16, 11, 15, 13, 9, 13, 9, 10, 2, 10, 9, 10, 12, 9, 11, 9, 10, 13, 14, 14, 10, 10, 13, 3, 7, 9, 9, 9, 10, 9, 9, 12, 13, 9, 15, 12, 15, 9, 9, 10, 15, 13, 13, 10, 12, 4, 12, 9, 13, 13, 9, 10, 9, 10, 10, 9, 13, 10, 14, 13, 9, 11, 9, 12, 10, 13, 13, 13, 13, 9, 9, 10, 8, 13, 4, 9, 10, 8, 10, 9, 13, 9, 3, 13, 13, 9, 9, 10, 4, 5, 5, 9, 8, 7, 9, 10, 10, 15, 9, 9, 6, 9, 9, 9, 9, 9, 9, 10, 8, 9, 11, 10, 10, 9, 13, 4, 9, 10, 9, 10, 7, 9, 3, 10, 6, 10, 13, 11, 9, 13, 13, 9, 13, 6, 15, 7, 9, 14, 10, 9, 9, 9, 10, 10, 10, 9, 4, 14, 13, 13, 3, 10, 10, 10, 6, 9, 9, 13, 12, 13, 10, 10, 10, 9, 10, 9, 9, 13, 13, 9, 13, 9, 10, 9, 9, 9, 10, 7, 9, 13, 10, 13, 13, 9, 9, 9, 9, 14, 7, 13, 9, 8, 13, 10, 9, 9, 6, 13, 10, 9, 9, 10, 9, 10, 14, 9, 10, 9, 12, 13, 5, 9, 13, 9, 13, 9, 15, 9, 10, 13, 6, 9, 13, 9, 10, 16, 13, 12, 13, 10, 13, 13, 7, 14, 10, 10, 14, 9, 10, 5, 13, 12, 11, 9, 9, 6, 9, 8, 9, 11, 9, 12, 9, 13, 9, 10, 15, 13, 10, 9, 14, 9, 5, 9, 3, 9, 15, 13, 9, 10, 10, 4, 10, 14, 9, 16, 6, 13, 10, 10, 13, 10, 9, 9, 5, 14, 16, 10, 9, 13, 9, 10, 9, 15, 10, 13, 14, 12, 13, 10, 13, 9, 10, 9, 10, 11, 12, 10, 10, 10, 9, 10, 9, 9, 9, 10, 11, 9, 13, 12, 9, 13, 9, 13, 10, 10, 13, 7, 13, 6, 9, 13, 11, 10, 10, 9, 14, 5, 11, 9, 9, 9, 11, 9, 11, 15, 9, 9, 13, 12, 15, 12, 10, 9, 11, 13, 10, 9, 13, 13, 9, 12, 9, 10, 9, 9, 13, 9, 13, 13, 10, 13, 16, 10, 9, 9, 12, 10, 7, 6, 13, 7, 9, 10, 10, 10, 9, 6, 9, 9, 4, 9, 13, 9, 5, 9, 16, 9, 10, 9, 10, 9, 15, 10, 9, 10, 14, 5, 10, 10, 9, 10, 9, 10, 4, 9, 9, 9, 10, 11, 13, 9, 9, 10, 10, 10, 10, 13, 11, 9, 13, 10, 13, 9, 11, 13, 9, 9, 9, 13, 10, 9, 13, 9, 9, 4, 12, 15, 11, 6, 9, 13, 4, 14, 10, 9, 9, 10, 6, 12, 13, 10, 9, 13, 14, 12, 10, 5, 13, 10, 6, 9, 10, 12, 9, 13, 13, 9, 6, 9, 10, 13, 10, 10, 9, 16, 10, 10, 13, 9, 13, 10, 9, 10, 6, 9, 11, 10, 9, 12, 14, 4, 9, 10, 11, 9, 9, 10, 10, 10, 9, 9, 10, 16, 10, 3, 9, 9, 10, 10, 3, 9, 7, 6, 10, 4, 9, 8, 13, 9, 13, 9, 7, 13, 7, 9, 16, 13, 9, 10, 9, 5, 10, 9, 9, 13, 9, 11, 10, 13, 14, 13, 10, 3, 11, 9, 9, 9, 9, 9, 10, 6, 10, 14, 10, 13, 10, 9, 9, 5, 16, 10, 13, 13, 9, 10, 9, 9, 10, 15, 10, 11, 13, 7, 11, 7, 9, 10, 10, 9, 10, 15, 9, 10, 13, 9, 6, 5, 13, 2, 13, 9, 7, 9, 9, 10, 7, 13, 13, 9, 9, 9, 13, 9, 14, 13, 13, 9, 6, 9, 9, 15, 13, 9, 13, 15, 13, 15, 1, 10, 10, 9, 13, 13, 10, 5, 9, 9, 9, 9, 11, 10, 9, 14, 9, 9, 9, 9, 9, 10, 14, 10, 9, 9, 9, 10, 13, 10, 9, 12, 9, 6, 9, 13, 6, 13, 13, 14, 9, 9, 15, 9, 13, 10, 10, 6, 15, 10, 9, 9, 7, 11, 6, 6, 10, 12, 13, 10, 10, 9, 10, 4, 13, 10, 9, 11, 16, 10, 10, 13, 9, 9, 13, 9, 11, 10, 9, 10, 11, 9, 10, 5, 10, 9, 9, 6, 7, 10, 9, 10, 10, 9, 9, 10, 15, 10, 7, 4, 10, 7, 10, 10, 10, 12, 13, 14, 13, 9, 10, 13, 10, 9, 12, 10, 13, 7, 10, 9, 9, 3, 5, 13, 9, 5, 13, 9, 13, 9, 9, 10, 10, 9, 9, 9, 13, 10, 9, 14, 9, 13, 16, 10, 9, 9, 10, 5, 10, 10, 9, 12, 13, 9, 9, 13, 10, 13, 10, 10, 13, 10, 12, 9, 13, 9, 8, 9, 13, 5, 9, 14, 10, 9, 5, 4, 8, 7, 7, 9, 11, 13, 9, 15, 9, 9, 9, 9, 11, 9, 10, 10, 9, 6, 9, 8, 9, 13, 9, 10, 13, 11, 9, 9, 10, 9, 7, 11, 11, 10, 10, 9, 5, 9, 13, 8, 9, 13, 13, 12, 6, 12, 13, 9, 7, 9, 13, 13, 11, 10, 10, 3, 9, 10, 14, 13, 10, 7, 10, 16, 10, 10, 9, 9, 13, 13, 9, 9, 9, 9, 13, 9, 9, 9, 10, 9, 6, 9, 12, 14, 10, 10, 14, 14, 5, 13, 12, 8, 4, 10, 9, 13, 10, 3, 14, 9, 9, 9, 10, 10, 8, 6, 10, 9, 9, 9, 13, 15, 10, 14, 11, 9, 11, 9, 9, 7, 9, 13, 11, 13, 7, 10, 10, 10, 9, 7, 8, 13, 13, 9, 9, 10, 13, 13, 14, 9, 9, 13, 9, 9, 9, 9, 9, 13, 10, 9, 6, 13, 9, 10, 5, 10, 6, 10, 10, 10, 10, 13, 10, 9, 6, 10, 3, 9, 9, 5, 10, 9, 14, 14, 9, 9, 10, 10, 9, 7, 9, 9, 13, 13, 5, 10, 7, 9, 13, 13, 15, 9, 10, 6, 9, 9, 9, 6, 10, 9, 10, 10, 9, 14, 9, 9, 15, 9, 10, 6, 9, 9, 12, 12, 13, 9, 9, 9, 13, 6, 7, 14, 9, 15, 9, 9, 13, 12, 11, 10, 10, 13, 4, 10, 9, 10, 9, 13, 10, 11, 9, 14, 9, 13, 13, 10, 6, 9, 12, 10, 3, 4, 12, 13, 9, 10, 11, 10, 9, 10, 9, 13, 13, 2, 9, 7, 13, 10, 14, 13, 10, 9, 9, 13, 10, 9, 6, 9, 13, 9, 11, 10, 12, 9, 14, 9, 8, 9, 13, 9, 9, 15, 10, 13, 10, 9, 10, 10, 10, 15, 10, 9, 14, 9, 9, 9, 10, 8, 9, 9, 11, 9, 13, 13, 2, 13, 13, 9, 10, 7, 9, 9, 9, 11, 14, 13, 9, 11, 10, 9, 6, 10, 10, 10, 10, 9, 10, 10, 13, 13, 4, 4, 10, 13, 11, 9, 8, 10, 9, 9, 10, 9, 14, 10, 10, 11, 11, 9, 6, 13, 9, 10, 13, 11, 13, 9, 9, 10, 9, 13, 10, 10, 10, 10, 10, 11, 9, 7, 9, 9, 13, 2, 9, 10, 10, 13, 9, 13, 9, 9, 13, 15, 13, 9, 14, 10, 13, 9, 9, 13, 9, 13, 13, 10, 12, 13, 9, 13, 13, 9, 9, 15, 7, 6, 13, 9, 8, 9, 4, 10, 3, 10, 9, 13, 9, 6, 13, 9, 9, 9, 11, 13, 10, 9, 10, 9, 13, 10, 13, 10, 8, 10, 12, 9, 13, 14, 13, 9, 13, 9, 9, 13, 10, 10, 2, 13, 6, 10, 9, 13, 9, 9, 9, 13, 9, 10, 9, 9, 9, 9, 8, 9, 10, 4, 13, 9, 10, 16, 10, 3, 10, 10, 4, 9, 9, 13, 9, 13, 10, 10, 10, 9, 9, 13, 13, 10, 13, 13, 10, 10, 8, 10, 10, 10, 9, 10, 16, 9, 11, 9, 9, 9, 12, 9, 10, 10, 9, 13, 6, 10, 10, 2, 8, 9, 10, 10, 9, 9, 11, 9, 10, 9, 9, 9, 10, 10, 11, 11, 10, 12, 9, 10, 13, 6, 7, 10, 9, 9, 10, 7, 4, 10, 16, 9, 10, 5, 9, 13, 10, 9, 9, 9, 8, 13, 10, 14, 9, 10, 6, 10, 9, 5, 9, 13, 10, 9, 9, 3, 13, 14, 16, 9, 7, 11, 11, 10, 12, 7, 2, 6, 9, 9, 13, 9, 13, 4, 13, 7, 9, 8, 9, 15, 13, 9, 9, 10, 9, 16, 9, 13, 10, 10, 9, 10, 9, 6, 10, 13, 10, 9, 7, 10, 10, 13, 10, 9, 12, 10, 12, 9, 10, 9, 9, 3, 14, 9, 9, 9, 10, 5, 9, 10, 13, 9, 10, 6, 10, 7, 10, 9, 10, 9, 10, 10, 8, 10, 14, 7, 13, 10, 10, 9, 7, 13, 13, 13, 13, 14, 12, 9, 13, 13, 13, 14, 9, 4, 10, 9, 10, 9, 9, 13, 7, 14, 7, 9, 10, 9, 10, 9, 6, 14, 9, 9, 9, 9, 9, 14, 10, 9, 15, 14, 9, 9, 13, 9, 9, 9, 14, 9, 10, 10, 5, 13, 10, 9, 7, 9, 12, 14, 10, 12, 9, 11, 10, 10, 13, 13, 9, 6, 9, 13, 14, 12, 11, 16, 8, 9, 9, 11, 10, 5, 13, 14, 10, 9, 9, 9, 9, 10, 10, 10, 12, 9, 11, 13, 10, 13, 9, 10, 13, 12, 14, 13, 13, 9, 13, 9, 10, 7, 11, 10, 13, 10, 9, 9, 15, 9, 13, 9, 13, 9, 12, 13, 9, 9, 9, 14, 10, 6, 8, 12, 9, 11, 9, 14, 9, 5, 11, 7, 9, 9, 9, 8, 13, 11, 10, 13, 10, 10, 11, 9, 9, 9, 9, 9, 13, 2, 9, 13, 10, 10, 10, 13, 10, 9, 14, 10, 16, 9, 9, 14, 13, 11, 13, 13, 12, 5, 10, 9, 9, 9, 13, 14, 6, 11, 10, 10, 12, 10, 10, 12, 10, 10, 9, 9, 11, 9, 14, 14, 9, 13, 10, 13, 9, 9, 9, 11, 9, 6, 9, 9, 9, 14, 9, 9, 13, 6, 9, 9, 9, 9, 13, 13, 15, 13, 9, 13, 16, 12, 13, 10, 13, 9, 14, 9, 9, 11, 9, 13, 11, 13, 11, 10, 6, 6, 11, 6, 13, 9, 13, 11, 9, 10, 9, 14, 13, 9, 14, 10, 7, 10, 9, 9, 9, 9, 10, 9, 10, 13, 9, 9, 9, 10, 6, 13, 10, 9, 9, 13, 9, 13, 10, 9, 9, 10, 13, 10, 8, 11, 11, 9, 12, 5, 13, 7, 10, 9, 9, 10, 6, 10, 10, 13, 9, 6, 11, 7, 10, 10, 2, 6, 14, 9, 9, 12, 10, 13, 10, 10, 3, 9, 10, 14, 10, 2, 14, 13, 10, 9, 9, 13, 11, 3, 10, 9, 9, 7, 10, 10, 13, 9, 9, 13, 13, 10, 9, 10, 9, 11, 10, 9, 9, 11, 9, 4, 10, 13, 10, 9, 10, 9, 9, 9, 10, 10, 13, 14, 9, 9, 11, 9, 10, 6, 7, 15, 9, 10, 9, 13, 13, 7, 10, 6, 15, 9, 9, 9, 9, 9, 6, 10, 14, 9, 13, 9, 9, 6, 9, 10, 9, 9, 9, 10, 13, 14, 9, 7, 9, 9, 7, 10, 9, 10, 9, 9, 6, 9, 7, 9, 7, 9, 13, 9, 10, 10, 14, 10, 9, 4, 13, 13, 10, 6, 6, 10, 9, 9, 9, 10, 9, 13, 9, 13, 11, 12, 10, 10, 13, 9, 13, 9, 7, 10, 11, 8, 4, 9, 6, 14, 6, 7, 13, 10, 9, 13, 9, 13, 9, 9, 10, 13, 10, 14, 13, 9, 13, 9, 12, 9, 16, 14, 13, 10, 9, 9, 9, 10, 6, 9, 9, 4, 13, 9, 10, 9, 4, 4, 9, 9, 9, 9, 13, 11, 10, 10, 9, 9, 9, 11, 10, 9, 16, 9, 13, 9, 7, 13, 4, 9, 9, 9, 10, 13, 6, 7, 11, 10, 10, 12, 9, 10, 5, 9, 13, 13, 6, 7, 15, 13, 10, 13, 9, 10, 12, 9, 13, 13, 9, 9, 10, 10, 8, 9, 10, 9, 13, 9, 11, 16, 10, 9, 13, 9, 12, 13, 10, 10, 7, 10, 14, 11, 13, 9, 14, 10, 13, 4, 14, 9, 9, 12, 9, 10, 13, 9, 10, 10, 9, 11, 10, 12, 14, 10, 14, 13, 9, 10, 16, 9, 9, 9, 10, 13, 10, 12, 10, 7, 13, 9, 16, 14, 5, 9, 9, 14, 9, 9, 9, 5, 10, 9, 12, 9, 9, 10, 10, 13, 11, 10, 8, 9, 3, 13, 9, 13, 10, 6, 7, 11, 13, 9, 9, 10, 14, 14, 9, 10, 13, 13, 16, 6, 9, 14, 10, 13, 9, 10, 4, 9, 9, 13, 12, 13, 9, 10, 6, 12, 13, 14, 9, 9, 7, 7, 13, 15, 9, 9, 8, 9, 11, 6, 10, 10, 9, 9, 6, 13, 11, 9, 9, 13, 6, 12, 10, 9, 9, 6, 10, 10, 10, 6, 11, 9, 9, 13, 9, 9, 5, 9, 16, 13, 13, 13, 7, 9, 4, 14, 10, 9, 13, 14, 13, 10, 10, 13, 9, 10, 10, 3, 13, 9, 9, 7, 8, 10, 11, 13, 10, 11, 10, 13, 9, 13, 9, 8, 4, 10, 9, 9, 13, 13, 12, 9, 10, 9, 10, 12, 10, 13, 9, 9, 9, 13, 2, 10, 11, 13, 11, 13, 8, 10, 12, 9, 8, 9, 13, 9, 10, 9, 4, 10, 12, 13, 5, 14, 10, 9, 16, 10, 13, 9, 9, 9, 13, 14, 14, 11, 10, 10, 14, 14, 12, 9, 9, 9, 9, 10, 9, 13, 10, 7, 13, 10, 9, 10, 9, 5, 13, 10, 13, 9, 9, 10, 11, 9, 6, 9, 13, 11, 13, 14, 9, 9, 7, 9, 10, 4, 14, 9, 9, 10, 14, 6, 12, 12, 11, 14, 10, 4, 11, 4, 10, 13, 10, 9, 9, 10, 11, 9, 8, 10, 10, 9, 4, 9, 9, 10, 9, 14, 10, 4, 9, 13, 14, 13, 9, 9, 11, 13, 13, 9, 9, 15, 11, 11, 6, 5, 13, 10, 10, 13, 11, 10, 10, 10, 9, 13, 10, 14, 10, 13, 11, 10, 12, 9, 10, 4, 10, 14, 7, 15, 13, 10, 13, 13, 9, 14, 12, 15, 2, 4, 11, 9, 13, 14, 9, 9, 4, 6, 9, 7, 9, 9, 9, 6, 10, 9, 10, 10, 9, 10, 16, 9, 10, 12, 13, 9, 10, 14, 15, 12, 9, 9, 14, 9, 10, 13, 10, 13, 9, 13, 9, 13, 10, 10, 16, 13, 10, 9, 9, 14, 9, 15, 7, 9, 14, 10, 13, 10, 6, 13, 10, 9, 10, 9, 7, 10, 10, 13, 9, 9, 14, 8, 9, 5, 13, 10, 11, 6, 7, 13, 10, 9, 13, 9, 9, 10, 9, 10, 9, 11, 9, 10, 6, 13, 7, 13, 9, 10, 13, 10, 10, 9, 10, 14, 7, 11, 10, 9, 9, 13, 14, 13, 16, 9, 10, 13, 5, 9, 10, 13, 7, 9, 10, 10, 13, 13, 6, 9, 9, 10, 9, 10, 10, 9, 11, 9, 13, 9, 4, 10, 10, 10, 6, 9, 13, 13, 4, 9, 13, 9, 12, 14, 7, 13, 9, 13, 13, 13, 9, 7, 15, 14, 9, 13, 7, 9, 9, 10, 10, 3, 10, 9, 10, 10, 10, 13, 4, 10, 8, 9, 9, 10, 9, 9, 7, 5, 10, 9, 9, 10, 10, 10, 9, 15, 13, 13, 13, 13, 9, 5, 9, 7, 9, 15, 10, 10, 7, 10, 13, 9, 13, 11, 12, 14, 13, 10, 11, 10, 10, 7, 14, 9, 12, 8, 9, 8, 10, 9, 10, 9, 13, 3, 9, 11, 14, 9, 10, 9, 15, 5, 14, 13, 12, 4, 14, 6, 7, 9, 3, 10, 16, 14, 9, 9, 10, 10, 10, 5, 9, 10, 9, 9, 9, 9, 9, 9, 8, 1, 15, 15, 10, 14, 10, 10, 10, 7, 9, 9, 10, 10, 10, 9, 5, 9, 9, 9, 13, 6, 13, 10, 9, 4, 15, 10, 10, 9, 11, 7, 10, 9, 13, 9, 9, 9, 13, 9, 11, 4, 7, 9, 16, 13, 13, 9, 9, 9, 11, 14, 13, 10, 13, 13, 9, 9, 6, 10, 9, 13, 7, 9, 10, 3, 11, 9, 13, 9, 2, 11, 6, 13, 14, 10, 9, 9, 9, 6, 7, 9, 9, 12, 13, 9, 9, 10, 14, 9, 10, 9, 9, 7, 11, 11, 9, 13, 13, 9, 9, 6, 9, 9, 16, 10, 15, 13, 9, 11, 13, 10, 10, 13, 13, 10, 13, 10, 14, 11, 10, 10, 9, 13, 14, 13, 7, 9, 10, 13, 10, 9, 8, 13, 10, 10, 10, 16, 9, 10, 9, 9, 13, 13, 9, 13, 9, 13, 9, 9, 13, 10, 10, 9, 9, 7, 9, 7, 6, 13, 12, 10, 12, 13, 13, 10, 4, 9, 10, 9, 11, 14, 13, 9, 9, 9, 7, 9, 9, 14, 9, 11, 12, 9, 9, 7, 10, 13, 9, 13, 13, 10, 9, 9, 13, 11, 13, 9, 10, 14, 9, 10, 13, 12, 9, 8, 13, 10, 11, 9, 11, 7, 11, 9, 9, 10, 6, 7, 8, 5, 13, 9, 11, 13, 13, 11, 9, 13, 13, 10, 11, 9, 6, 10, 8, 9, 9, 10, 10, 10, 9, 16, 11, 9, 13, 14, 9, 10, 9, 10, 10, 13, 9, 9, 9, 13, 12, 10, 8, 13, 13, 9, 10, 14, 9, 12, 11, 13, 10, 13, 14, 8, 9, 13, 13, 9, 9, 3, 13, 13, 10, 3, 9, 10, 9, 9, 10, 10, 10, 10, 10, 10, 13, 10, 12, 9, 15, 14, 5, 10, 9, 9, 9, 13, 13, 10, 4, 5, 9, 9, 9, 9, 10, 9, 9, 9, 10, 14, 6, 9, 9, 9, 14, 10, 13, 13, 5, 4, 10, 13, 9, 9, 9, 7, 10, 10, 9, 10, 9, 12, 12, 14, 10, 10, 6, 10, 9, 9, 9, 12, 5, 13, 8, 10, 10, 9, 13, 9, 7, 10, 10, 9, 13, 3, 14, 13, 9, 13, 9, 10, 10, 9, 13, 9, 9, 10, 8, 13, 9, 9, 13, 9, 9, 10, 10, 11, 13, 13, 7, 3, 11, 14, 9, 13, 13, 9, 12, 10, 13, 9, 9, 9, 9, 9, 10, 10, 9, 13, 10, 9, 4, 13, 10, 9, 15, 13, 10, 13, 9, 6, 5, 10, 9, 10, 10, 13, 13, 10, 10, 13, 9, 11, 4, 14, 10, 10, 9, 14, 6, 12, 9, 4, 9, 9, 10, 9, 9, 6, 10, 13, 10, 9, 10, 10, 11, 9, 9, 13, 9, 9, 10, 9, 9, 12, 8, 9, 13, 9, 10, 10, 10, 14, 9, 10, 10, 9, 5, 10, 9, 9, 10, 9, 9, 10, 9, 15, 5, 9, 13, 10, 10, 6, 9, 9, 6, 10, 11, 13, 13, 10, 10, 10, 10, 10, 9, 9, 7, 9, 11, 9, 9, 10, 13, 10, 9, 10, 7, 13, 9, 9, 9, 9, 9, 10, 9, 14, 10, 9, 10, 9, 14, 9, 7, 13, 9, 9, 10, 13, 9, 14, 10, 13, 11, 12, 9, 13, 5, 10, 9, 9, 10, 9, 7, 9, 14, 10, 9, 10, 9, 9, 6, 10, 7, 13, 10, 9, 9, 10, 10, 13, 5, 7, 5, 9, 14, 13, 9, 9, 9, 9, 9, 9, 9, 13, 13, 13, 9, 7, 16, 6, 13, 3, 10, 13, 15, 10, 9, 13, 13, 13, 9, 13, 11, 13, 10, 10, 14, 14, 10, 2, 10, 13, 13, 9, 11, 13, 10, 9, 9, 9, 7, 10, 10, 10, 13, 4, 10, 12, 12, 10, 8, 10, 13, 13, 12, 7, 9, 12, 9, 9, 14, 12, 9, 10, 9, 13, 9, 10, 13, 9, 10, 9, 9, 12, 13, 8, 4, 10, 2, 10, 9, 13, 9, 10, 3, 16, 7, 9, 8, 15, 10, 9, 9, 10, 4, 11, 13, 9, 6, 9, 9, 10, 10, 10, 10, 10, 10, 13, 10, 14, 10, 13, 4, 11, 4, 13, 9, 9, 10, 10, 9, 10, 10, 15, 12, 9, 4, 9, 10, 4, 10, 10, 6, 10, 10, 6, 12, 10, 9, 13, 10, 9, 14, 13, 12, 10, 9, 10, 12, 8, 9, 9, 11, 10, 9, 9, 10, 10, 13, 3, 6, 10, 13, 10, 9, 10, 10, 9, 10, 15, 13, 14, 16, 13, 13, 9, 13, 6, 10, 10, 13, 6, 9, 10, 13, 10, 12, 14, 9, 9, 9, 9, 10, 13, 9, 13, 10, 13, 13, 9, 9, 10, 10, 10, 9, 13, 10, 14, 9, 9, 9, 9, 9, 9, 13, 13, 9, 5, 12, 9, 9, 15, 5, 9, 9, 13, 13, 5, 9, 7, 11, 13, 12, 13, 9, 7, 14, 11, 9, 9, 14, 13, 11, 10, 4, 9, 10, 7, 10, 10, 14, 10, 9, 10, 6, 13, 10, 4, 10, 10, 13, 14, 13, 9, 15, 7, 5, 10, 7, 10, 10, 13, 13, 14, 9, 13, 9, 14, 11, 13, 10, 9, 13, 13, 9, 6, 7, 11, 13, 8, 5, 11, 9, 10, 10, 10, 13, 9, 9, 10, 9, 9, 12, 9, 3, 10, 12, 10, 9, 10, 9, 10, 11, 9, 13, 9, 9, 10, 14, 10, 9, 7, 10, 14, 13, 14, 4, 9, 13, 13, 9, 7, 11, 5, 9, 10, 9, 10, 14, 10, 13, 7, 5, 12, 12, 9, 9, 13, 13, 10, 10, 1, 13, 15, 14, 9, 13, 11, 10, 13, 13, 4, 10, 14, 9, 10, 13, 13, 13, 1, 10, 13, 9, 13, 13, 10, 10, 9, 14, 13, 13, 9, 13, 9, 9, 9, 9, 10, 11, 7, 10, 6, 9, 7, 9, 15, 10, 10, 10, 11, 4, 7, 9, 10, 13, 10, 9, 10, 9, 9, 14, 16, 9, 9, 9, 13, 9, 9, 7, 9, 13, 12, 13, 11, 9, 9, 9, 9, 9, 9, 9, 13, 14, 9, 13, 9, 4, 14, 10, 9, 9, 7, 6, 13, 13, 11, 10, 10, 16, 14, 13, 4, 9, 9, 16, 9, 7, 13, 12, 8, 9, 9, 9, 13, 9, 10, 10, 10, 13, 10, 13, 10, 9, 13, 9, 9, 9, 10, 9, 8, 10, 13, 9, 9, 9, 13, 13, 9, 10, 12, 2, 10, 9, 13, 10, 13, 9, 4, 13, 10, 9, 11, 9, 7, 14, 9, 7, 6, 15, 7, 13, 13, 10, 10, 9, 8, 5, 13, 9, 9, 9, 10, 3, 7, 8, 11, 8, 9, 11, 9, 6, 10, 13, 10, 9, 13, 10, 7, 11, 10, 10, 9, 7, 13, 13, 10, 4, 11, 12, 9, 14, 9, 9, 13, 16, 12, 9, 9, 9, 15, 12, 7, 13, 9, 10, 9, 13, 9, 11, 9, 11, 13, 9, 9, 9, 10, 9, 12, 9, 9, 16, 10, 9, 14, 7, 10, 13, 11, 13, 10, 9, 9, 11, 9, 10, 13, 13, 9, 10, 5, 12, 9, 9, 10, 9, 9, 9, 4, 12, 9, 10, 15, 9, 9, 9, 9, 11, 2, 12, 13, 11, 10, 16, 10, 10, 13, 10, 13, 10, 4, 10, 9, 9, 9, 13, 10, 12, 9, 10, 14, 11, 9, 9, 10, 9, 9, 10, 6, 9, 8, 14, 8, 9, 9, 10, 10, 12, 13, 9, 10, 10, 14, 13, 9, 9, 9, 5, 9, 10, 11, 13, 9, 14, 10, 13, 10, 9, 9, 9, 8, 10, 9, 13, 10, 7, 9, 14, 9, 9, 10, 13, 9, 9, 13, 10, 5, 12, 9, 9, 9, 13, 13, 11, 1, 13, 9, 10, 9, 9, 13, 13, 6, 13, 11, 10, 10, 10, 9, 10, 10, 7, 6, 14, 9, 9, 9, 10, 3, 12, 9, 9, 10, 10, 9, 10, 10, 13, 7, 10, 9, 9, 6, 9, 9, 13, 10, 10, 10, 7, 14, 9, 13, 9, 10, 6, 10, 9, 3, 10, 3, 16, 10, 13, 10, 10, 9, 10, 4, 7, 9, 14, 12, 9, 9, 9, 1, 10, 13, 13, 3, 15, 9, 10, 13, 10, 10, 13, 9, 10, 10, 9, 11, 9, 11, 7, 7, 10, 10, 9, 14, 10, 9, 10, 9, 10, 13, 11, 10, 6, 9, 10, 4, 13, 13, 9, 9, 15, 10, 14, 10, 9, 16, 9, 10, 10, 10, 9, 9, 9, 10, 10, 9, 10, 9, 4, 10, 15, 10, 9, 7, 9, 9, 14, 15, 3, 11, 7, 9, 13, 11, 9, 9, 2, 13, 10, 13, 4, 13, 10, 10, 13, 13, 12, 9, 9, 14, 13, 10, 9, 8, 10, 14, 10, 15, 14, 13, 14, 13, 9, 9, 10, 9, 9, 13, 7, 9, 9, 2, 9, 10, 10, 8, 15, 10, 7, 10, 6, 9, 8, 10, 10, 4, 9, 7, 7, 9, 6, 9, 10, 9, 9, 9, 10, 7, 10, 10, 13, 6, 9, 10, 9, 9, 9, 9, 6, 9, 14, 9, 15, 5, 13, 13, 8, 7, 10, 10, 10, 9, 13, 14, 9, 10, 9, 9, 9, 13, 10, 11, 10, 9, 10, 13, 14, 10, 9, 9, 9, 15, 6, 13, 4, 12, 13, 10, 13, 9, 10, 13, 10, 9, 5, 11, 9, 13, 13, 9, 3, 9, 9, 13, 13, 15, 9, 11, 10, 9, 9, 5, 9, 13, 10, 10, 16, 10, 13, 10, 10, 9, 9, 10, 9, 13, 9, 9, 10, 13, 9, 14, 10, 16, 11, 14, 14, 14, 9, 14, 9, 10, 13, 13, 4, 13, 13, 13, 9, 10, 9, 9, 9, 11, 10, 14, 9, 10, 13, 9, 10, 9, 13, 13, 9, 13, 12, 10, 3, 10, 8, 11, 10, 6, 15, 9, 9, 7, 9, 9, 9, 13, 11, 9, 9, 12, 9, 10, 9, 9, 10, 10, 9, 10, 9, 9, 9, 9, 9, 10, 14, 7, 10, 13, 10, 9, 10, 9, 9, 10, 10, 10, 6, 10, 4, 10, 9, 13, 9, 9, 9, 10, 11, 6, 10, 10, 10, 11, 13, 7, 4, 9, 9, 14, 10, 14, 12, 9, 10, 10, 9, 10, 11, 9, 9, 13, 14, 13, 13, 10, 5, 13, 9, 10, 9, 9, 10, 9, 14, 11, 11, 9, 12, 9, 13, 10, 13, 9, 10, 9, 10, 9, 1, 10, 13, 9, 13, 11, 6, 13, 14, 10, 9, 10, 9, 13, 13, 9, 5, 10, 8, 9, 10, 10, 13, 9, 13, 7, 10, 13, 13, 9, 13, 10, 9, 4, 9, 9, 6, 9, 10, 13, 10, 9, 9, 13, 9, 9, 6, 14, 9, 10, 9, 9, 10, 13, 9, 10, 9, 14, 15, 9, 14, 13, 7, 9, 10, 13, 10, 7, 13, 12, 9, 9, 9, 9, 10, 12, 8, 13, 9, 10, 13, 10, 10, 9, 9, 14, 8, 9, 9, 10, 15, 6, 9, 14, 9, 9, 8, 10, 16, 10, 9, 9, 13, 9, 10, 13, 14, 9, 9, 10, 9, 10, 10, 16, 10, 9, 10, 16, 10, 13, 10, 10, 14, 9, 9, 9, 13, 14, 14, 13, 9, 7, 10, 2, 11, 9, 9, 12, 10, 10, 13, 10, 10, 9, 9, 7, 6, 9, 10, 9, 15, 9, 9, 14, 9, 10, 13, 13, 9, 3, 12, 9, 15, 10, 9, 10, 7, 13, 13, 7, 13, 14, 3, 9, 9, 14, 10, 10, 3, 9, 10, 2, 9, 6, 13, 9, 9, 9, 9, 10, 9, 5, 9, 10, 16, 10, 10, 9, 5, 13, 13, 13, 15, 4, 14, 9, 3, 10, 10, 9, 9, 4, 10, 13, 9, 7, 9, 13, 13, 9, 10, 12, 9, 14, 4, 9, 10, 9, 13, 8, 6, 14, 13, 9, 14, 16, 10, 9, 10, 9, 9, 9, 14, 10, 9, 9, 9, 12, 13, 9, 10, 9, 9, 10, 9, 4, 6, 9, 8, 7, 13, 7, 9, 14, 6, 9, 9, 14, 9, 13, 9, 9, 13, 12, 11, 9, 9, 10, 4, 7, 9, 15, 9, 9, 15, 10, 10, 13, 9, 9, 10, 10, 7, 9, 10, 11, 9, 7, 13, 9, 13, 9, 13, 9, 11, 13, 10, 7, 13, 9, 6, 10, 7, 5, 10, 12, 10, 10, 6, 6, 13, 9, 9, 9, 9, 13, 9, 10, 5, 7, 6, 13, 13, 9, 5, 6, 13, 9, 10, 11, 9, 10, 13, 9, 9, 2, 10, 14, 12, 9, 10, 12, 9, 9, 9, 10, 9, 10, 9, 12, 15, 9, 13, 9, 1, 9, 5, 11, 7, 14, 16, 9, 9, 9, 5, 15, 14, 15, 9, 11, 13, 12, 10, 9, 9, 10, 11, 10, 15, 13, 10, 9, 9, 10, 11, 9, 9, 14, 8, 9, 9, 7, 13, 14, 2, 10, 11, 12, 13, 4, 9, 14, 10, 9, 9, 7, 10, 9, 9, 9, 9, 9, 10, 13, 13, 16, 13, 12, 12, 9, 10, 10, 9, 13, 9, 13, 10, 9, 10, 9, 8, 11, 9, 10, 15, 13, 14, 14, 13, 10, 3, 13, 9, 13, 10, 10, 7, 9, 13, 11, 11, 13, 9, 15, 9, 10, 7, 13, 13, 9, 14, 11, 15, 14, 8, 10, 10, 11, 10, 11, 13, 9, 10, 13, 5, 10, 10, 9, 9, 13, 9, 9, 10, 9, 12, 9, 13, 9, 10, 10, 13, 14, 9, 5, 13, 9, 7, 9, 10, 9, 13, 10, 9, 10, 10, 14, 10, 14, 5, 4, 8, 11, 9, 10, 9, 10, 9, 10, 8, 13, 13, 13, 9, 12, 14, 13, 9, 10, 9, 13, 6, 14, 10, 13, 13, 10, 13, 9, 9, 13, 10, 10, 13, 15, 7, 16, 13, 13, 9, 9, 13, 13, 7, 9, 11, 13, 14, 13, 14, 14, 11, 13, 13, 9, 10, 9, 9, 9, 12, 13, 9, 14, 13, 13, 13, 10, 10, 9, 13, 4, 10, 15, 9, 10, 13, 15, 9, 10, 9, 7, 10, 9, 9, 13, 9, 7, 5, 9, 9, 10, 9, 10, 9, 9, 9, 9, 14, 9, 9, 9, 10, 10, 10, 8, 14, 13, 9, 10, 9, 9, 10, 4, 10, 11, 9, 10, 10, 13, 10, 11, 10, 10, 9, 9, 10, 13, 12, 9, 5, 10, 11, 10, 10, 9, 13, 10, 9, 9, 9, 9, 13, 9, 15, 14, 8, 10, 9, 10, 9, 11, 9, 14, 9, 10, 10, 9, 9, 13, 14, 12, 10, 8, 7, 9, 9, 10, 13, 11, 10, 10, 8, 13, 11, 9, 10, 10, 13, 9, 9, 3, 16, 9, 14, 8, 9, 7, 7, 10, 9, 13, 13, 4, 10, 8, 10, 13, 9, 3, 10, 9, 13, 9, 10, 13, 9, 13, 11, 9, 10, 7, 9, 3, 6, 10, 9, 9, 9, 9, 15, 14, 5, 10, 14, 8, 13, 10, 15, 10, 12, 9, 9, 9, 15, 9, 9, 12, 10, 11, 12, 6, 10, 6, 10, 9, 13, 9, 15, 7, 9, 13, 10, 9, 9, 1, 9, 9, 9, 11, 6, 9, 7, 13, 9, 10, 12, 10, 13, 13, 9, 14, 7, 7, 7, 9, 9, 10, 10, 10, 4, 9, 13, 13, 15, 9, 9, 9, 7, 10, 9, 3, 9, 14, 10, 10, 13, 10, 10, 13, 13, 10, 9, 13, 13, 9, 14, 10, 10, 5, 9, 14, 10, 10, 13, 9, 3, 13, 9, 13, 12, 11, 7, 14, 9, 9, 15, 2, 5, 9, 13, 13, 10, 10, 13, 9, 13, 13, 13, 10, 10, 10, 4, 12, 10, 9, 13, 13, 9, 9, 10, 12, 9, 9, 10, 14, 10, 6, 13, 10, 10, 9, 5, 7, 13, 13, 6, 13, 10, 9, 13, 9, 9, 13, 10, 9, 13, 10, 12, 10, 13, 13, 9, 10, 13, 3, 9, 3, 12, 10, 9, 9, 9, 16, 9, 11, 12, 9, 9, 9, 9, 11, 16, 11, 13, 12, 9, 9, 10, 11, 10, 10, 9, 6, 6, 5, 13, 10, 14, 13, 11, 9, 9, 10, 7, 10, 8, 9, 13, 10, 9, 10, 13, 10, 10, 7, 5, 10, 11, 13, 2, 10, 15, 10, 11, 14, 13, 10, 10, 4, 7, 10, 6, 9, 9, 13, 9, 11, 9, 6, 9, 10, 10, 10, 10, 9, 11, 9, 10, 9, 12, 10, 9, 9, 11, 9, 9, 12, 14, 10, 8, 12, 9, 9, 3, 7, 4, 10, 8, 3, 5, 9, 15, 4, 9, 4, 9, 9, 9, 10, 13, 9, 10, 10, 7, 14, 13, 10, 10, 16, 10, 15, 9, 10, 9, 11, 13, 9, 9, 9, 9, 9, 4, 9, 13, 9, 10, 9, 9, 9, 4, 8, 9, 12, 9, 13, 10, 9, 11, 13, 13, 12, 6, 5, 9, 10, 13, 10, 9, 13, 10, 13, 10, 9, 7, 10, 9, 9, 9, 9, 8, 9, 9, 13, 12, 9, 11, 9, 9, 9, 16, 13, 9, 9, 9, 13, 5, 10, 9, 10, 9, 16, 10, 9, 13, 9, 10, 9, 10, 10, 12, 9, 10, 15, 14, 13, 13, 15, 9, 10, 9, 3, 10, 9, 5, 13, 9, 8, 9, 9, 9, 9, 12, 8, 11, 10, 13, 9, 14, 9, 11, 10, 13, 13, 9, 14, 9, 13, 11, 14, 10, 11, 13, 11, 10, 16, 10, 10, 13, 13, 9, 10, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 10, 9, 14, 10, 9, 10, 10, 14, 9, 10, 12, 10, 13, 10, 13, 9, 10, 13, 8, 6, 10, 13, 13, 13, 9, 10, 7, 10, 12, 14, 10, 14, 15, 12, 13, 9, 7, 13, 7, 10, 9, 9, 9, 9, 14, 15, 10, 9, 10, 13, 10, 14, 9, 9, 14, 7, 10, 6, 6, 9, 10, 13, 9, 4, 9, 9, 15, 9, 9, 9, 9, 10, 9, 6, 16, 9, 9, 7, 9, 4, 7, 4, 16, 13, 6, 15, 10, 14, 9, 6, 9, 9, 11, 9, 10, 6, 11, 12, 13, 9, 7, 7, 14, 10, 15, 9, 4, 13, 9, 9, 10, 4, 13, 10, 9, 9, 10, 9, 13, 9, 13, 11, 6, 16, 9, 13, 10, 9, 9, 13, 14, 9, 11, 4, 13, 10, 10, 16, 9, 9, 10, 10, 10, 9, 9, 9, 9, 13, 9, 12, 10, 11, 6, 5, 10, 13, 9, 6, 3, 9, 10, 14, 9, 9, 11, 9, 9, 13, 13, 7, 9, 13, 9, 2, 14, 13, 11, 9, 9, 9, 4, 10, 9, 10, 11, 9, 13, 9, 10, 9, 11, 9, 10, 10, 13, 13, 5, 7, 13, 10, 10, 9, 9, 9, 10, 10, 10, 6, 9, 10, 9, 13, 14, 9, 9, 10, 14, 13, 10, 9, 9, 9, 15, 10, 9, 13, 14, 9, 15, 10, 13, 10, 13, 14, 14, 10, 9, 10, 10, 13, 10, 13, 6, 13, 13, 11, 9, 10, 9, 9, 10, 9, 13, 16, 7, 9, 11, 9, 9, 10, 10, 13, 14, 10, 12, 10, 11, 10, 10, 9, 9, 9, 10, 11, 9, 9, 13, 13, 2, 14, 10, 14, 9, 11, 10, 13, 10, 11, 9, 9, 13, 10, 8, 12, 10, 8, 6, 11, 9, 10, 13, 10, 10, 9, 15, 13, 7, 9, 10, 3, 10, 9, 10, 9, 13, 8, 9, 13, 13, 9, 10, 10, 9, 6, 11, 9, 9, 9, 9, 10, 9, 9, 10, 10, 9, 13, 3, 10, 9, 10, 5, 15, 10, 13, 10, 14, 13, 13, 11, 6, 9, 12, 13, 9, 9, 13, 9, 11, 6, 9, 9, 13, 9, 9, 13, 10, 13, 10, 11, 13, 10, 9, 10, 9, 9, 13, 13, 10, 12, 9, 13, 10, 10, 13, 13, 3, 10, 9, 9, 11, 11, 12, 11, 8, 4, 11, 13, 10, 13, 16, 4, 9, 9, 9, 14, 9, 9, 9, 6, 9, 10, 9, 16, 10, 9, 14, 9, 10, 6, 13, 10, 7, 6, 9, 9, 10, 14, 9, 13, 9, 9, 14, 10, 12, 9, 10, 11, 7, 9, 10, 10, 4, 10, 9, 4, 10, 10, 10, 13, 10, 13, 3, 9, 5, 9, 13, 7, 10, 9, 10, 9, 9, 11, 14, 13, 10, 3, 9, 9, 11, 9, 7, 9, 11, 10, 9, 13, 11, 13, 9, 13, 10, 9, 7, 9, 10, 9, 10, 4, 13, 8, 7, 9, 10, 13, 11, 5, 9, 6, 4, 9, 9, 12, 9, 13, 12, 8, 13, 10, 9, 9, 9, 13, 6, 10, 13, 9, 12, 14, 9, 10, 9, 13, 9, 10, 9, 10, 9, 6, 7, 10, 15, 9, 9, 10, 10, 16, 14, 4, 8, 10, 13, 10, 13, 8, 13, 10, 10, 9, 10, 13, 7, 13, 13, 14, 13, 9, 13, 9, 10, 10, 10, 9, 9, 9, 13, 13, 13, 6, 9, 9, 10, 7, 6, 9, 9, 12, 5, 9, 13, 10, 8, 10, 10, 3, 11, 10, 8, 13, 9, 3, 12, 2, 13, 10, 10, 11, 13, 9, 6, 10, 13, 9, 13, 7, 11, 8, 6, 10, 9, 10, 9, 11, 10, 9, 16, 10, 13, 9, 9, 10, 9, 10, 9, 14, 14, 9, 9, 14, 10, 9, 14, 10, 13, 12, 14, 7, 9, 13, 12, 10, 9, 10, 9, 9, 13, 13, 10, 12, 10, 13, 9, 14, 12, 11, 9, 13, 6, 14, 9, 13, 10, 13, 7, 9, 9, 9, 9, 9, 14, 10, 15, 7, 14, 7, 10, 10, 13, 13, 9, 4, 10, 6, 9, 15, 13, 10, 11, 12, 9, 14, 11, 7, 10, 9, 11, 9, 13, 10, 10, 10, 12, 6, 10, 10, 9, 9, 10, 9, 4, 9, 14, 14, 12, 9, 9, 10, 8, 13, 13, 10, 6, 10, 12, 9, 10, 6, 9, 13, 10, 15, 10, 9, 10, 9, 9, 9, 13, 13, 13, 10, 10, 9, 10, 9, 9, 9, 13, 13, 9, 10, 9, 12, 9, 13, 13, 13, 9, 9, 14, 9, 8, 4, 11, 15, 13, 9, 9, 13, 7, 9, 13, 5, 7, 10, 12, 9, 8, 7, 10, 9, 10, 13, 9, 9, 13, 9, 10, 14, 13, 10, 13, 10, 10, 16, 9, 10, 13, 7, 15, 4, 10, 14, 13, 12, 10, 9, 13, 10, 9, 16, 7, 10, 13, 9, 6, 9, 13, 9, 16, 10, 11, 9, 14, 9, 9, 13, 15, 9, 13, 10, 9, 13, 13, 13, 9, 11, 9, 1, 10, 10, 13, 13, 14, 9, 13, 13, 11, 13, 10, 9, 10, 7, 14, 10, 9, 13, 13, 10, 13, 10, 9, 14, 13, 9, 9, 15, 10, 13, 9, 11, 10, 11, 9, 10, 11, 10, 7, 9, 14, 9, 6, 9, 15, 10, 9, 11, 9, 12, 14, 12, 9, 16, 14, 9, 4, 9, 10, 9, 11, 10, 2, 7, 4, 10, 9, 9, 10, 13, 9, 6, 7, 10, 13, 9, 10, 10, 4, 9, 9, 11, 9, 5, 13, 10, 9, 9, 5, 9, 4, 10, 9, 6, 16, 5, 10, 12, 9, 9, 10, 9, 9, 13, 4, 16, 13, 10, 9, 10, 6, 10, 9, 10, 10, 9, 9, 13, 9, 13, 3, 12, 9, 13, 11, 9, 4, 13, 9, 12, 13, 9, 10, 15, 9, 10, 9, 13, 9, 13, 10, 16, 9, 14, 9, 9, 12, 10, 10, 9, 13, 11, 4, 15, 13, 5, 10, 10, 10, 9, 6, 9, 9, 10, 9, 9, 13, 9, 9, 9, 11, 9, 9, 12, 12, 9, 10, 14, 12, 9, 13, 7, 9, 6, 9, 12, 13, 13, 12, 10, 10, 13, 14, 4, 10, 6, 9, 9, 14, 9, 13, 9, 10, 14, 13, 13, 9, 7, 10, 10, 10, 9, 10, 10, 10, 6, 9, 13, 13, 13, 13, 13, 7, 13, 14, 15, 11, 10, 6, 7, 12, 10, 13, 13, 9, 9, 6, 9, 9, 16, 9, 10, 10, 11, 6, 9, 2, 12, 13, 10, 9, 9, 9, 7, 10, 9, 9, 10, 16, 9, 13, 10, 9, 9, 9, 13, 13, 14, 9, 4, 13, 10, 9, 13, 13, 13, 13, 13, 13, 12, 13, 13, 4, 14, 10, 9, 7, 9, 9, 14, 9, 11, 5, 10, 9, 9, 9, 4, 9, 9, 14, 9, 9, 10, 10, 15, 9, 14, 5, 5, 10, 10, 13, 9, 9, 7, 13, 11, 10, 9, 10, 10, 5, 10, 10, 10, 13, 5, 15, 14, 10, 9, 9, 9, 15, 10, 7, 10, 15, 7, 10, 9, 13, 11, 13, 4, 15, 10, 9, 13, 10, 7, 9, 5, 9, 9, 9, 13, 10, 7, 13, 9, 13, 9, 9, 13, 7, 13, 13, 12, 10, 9, 12, 10, 13, 9, 6, 9, 14, 9, 13, 6, 1, 9, 13, 11, 13, 10, 9, 14, 10, 13, 10, 9, 13, 14, 8, 13, 10, 10, 10, 9, 9, 9, 10, 6, 9, 9, 10, 6, 13, 6, 9, 10, 9, 13, 9, 13, 4, 9, 12, 13, 11, 9, 13, 10, 10, 13, 9, 13, 11, 7, 9, 7, 9, 11, 9, 10, 10, 10, 13, 9, 13, 7, 9, 9, 6, 10, 10, 13, 9, 10, 9, 13, 13, 16, 9, 11, 11, 9, 13, 10, 9, 9, 8, 14, 10, 9, 8, 9, 9, 10, 10, 16, 8, 10, 10, 10, 10, 9, 9, 7, 9, 13, 9, 9, 14, 9, 9, 14, 9, 8, 10, 10, 9, 16, 9, 15, 9, 12, 14, 9, 10, 15, 12, 7, 9, 10, 13, 10, 6, 9, 10, 9, 10, 11, 10, 5, 7, 9, 5, 9, 1, 9, 9, 9, 10, 9, 9, 11, 9, 10, 13, 9, 10, 9, 9, 13, 9, 8, 9, 9, 9, 10, 9, 13, 13, 10, 10, 10, 9, 10, 9, 9, 10, 10, 9, 10, 14, 13, 9, 9, 14, 7, 9, 10, 9, 10, 14, 9, 9, 9, 10, 10, 9, 9, 9, 10, 10, 4, 10, 10, 9, 4, 7, 13, 13, 9, 9, 9, 13, 9, 9, 9, 16, 9, 10, 12, 9, 14, 9, 9, 13, 9, 6, 10, 13, 7, 7, 13, 9, 9, 13, 12, 15, 10, 9, 9, 9, 9, 10, 4, 9, 12, 10, 9, 6, 9, 13, 13, 10, 9, 5, 10, 13, 9, 10, 13, 9, 10, 9, 10, 11, 10, 10, 10, 11, 13, 7, 10, 16, 10, 13, 10, 13, 12, 13, 10, 12, 13, 9, 9, 9, 13, 10, 9, 4, 9, 9, 9, 9, 15, 13, 10, 15, 10, 9, 7, 13, 9, 9, 10, 10, 9, 5, 6, 9, 8, 10, 10, 10, 12, 15, 9, 9, 13, 10, 9, 13, 9, 12, 11, 8, 13, 9, 9, 10, 9, 12, 13, 13, 10, 12, 13, 6, 16, 10, 5, 14, 13, 10, 9, 5, 9, 13, 8, 9, 5, 9, 9, 13, 13, 13, 9, 10, 9, 9, 10, 10, 12, 11, 10, 9, 10, 8, 9, 5, 14, 9, 10, 9, 9, 12, 10, 10, 10, 9, 7, 10, 9, 13, 13, 10, 9, 13, 9, 13, 13, 9, 6, 9, 10, 9, 9, 13, 5, 4, 10, 7, 13, 11, 10, 10, 9, 9, 14, 10, 10, 9, 12, 5, 10, 10, 9, 2, 9, 9, 9, 9, 5, 7, 9, 13, 10, 10, 13, 10, 13, 10, 6, 13, 10, 12, 12, 10, 10, 9, 9, 9, 6, 9, 9, 10, 13, 7, 10, 6, 9, 9, 9, 9, 5, 11, 13, 10, 9, 14, 5, 10, 10, 10, 10, 13, 9, 9, 7, 13, 13, 7, 4, 14, 12, 5, 9, 10, 5, 10, 9, 10, 13, 9, 15, 13, 9, 8, 14, 13, 9, 10, 10, 9, 9, 12, 12, 9, 13, 9, 9, 10, 10, 9, 11, 13, 10, 10, 10, 10, 9, 10, 11, 9, 9, 10, 12, 9, 9, 9, 9, 5, 10, 10, 9, 6, 13, 11, 9, 10, 10, 6, 9, 9, 13, 13, 12, 9, 14, 13, 10, 10, 13, 9, 13, 14, 9, 1, 10, 8, 12, 9, 14, 9, 9, 14, 9, 9, 6, 4, 1, 7, 9, 10, 9, 13, 11, 13, 14, 9, 10, 10, 9, 9, 9, 10, 9, 9, 5, 13, 10, 10, 16, 10, 13, 10, 11, 9, 11, 9, 9, 5, 4, 9, 13, 9, 4, 14, 10, 15, 8, 11, 13, 13, 10, 9, 4, 9, 9, 7, 14, 10, 12, 13, 10, 10, 10, 10, 6, 10, 10, 9, 9, 9, 12, 16, 4, 9, 3, 9, 9, 13, 10, 9, 10, 4, 10, 10, 10, 13, 10, 13, 13, 13, 11, 13, 10, 7, 10, 11, 8, 13, 9, 9, 10, 9, 9, 14, 15, 10, 13, 12, 14, 10, 9, 9, 6, 9, 9, 10, 13, 12, 6, 9, 1, 10, 9, 13, 10, 10, 9, 9, 9, 8, 13, 7, 7, 9, 14, 10, 16, 13, 9, 4, 10, 11, 10, 7, 14, 9, 10, 9, 9, 13, 9, 13, 9, 13, 16, 12, 12, 13, 9, 13, 10, 12, 9, 12, 13, 13, 4, 14, 4, 4, 10, 13, 9, 6, 13, 3, 10, 9, 9, 9, 15, 14, 9, 15, 11, 11, 10, 14, 13, 9, 7, 11, 9, 10, 13, 9, 4, 13, 13, 13, 6, 8, 10, 10, 15, 13, 15, 10, 15, 9, 3, 10, 11, 8, 13, 9, 9, 9, 10, 9, 9, 9, 9, 14, 10, 13, 5, 6, 14, 9, 9, 10, 10, 13, 10, 9, 10, 9, 9, 6, 3, 13, 9, 13, 3, 9, 13, 14, 13, 14, 10, 13, 13, 16, 9, 9, 10, 9, 6, 4, 10, 9, 10, 9, 10, 9, 13, 9, 9, 10, 9, 10, 14, 9, 13, 14, 11, 7, 11, 9, 10, 9, 13, 11, 14, 14, 7, 12, 10, 10, 9, 9, 13, 12, 10, 10, 7, 9, 10, 9, 5, 10, 9, 7, 9, 9, 11, 13, 9, 9, 10, 9, 11, 10, 11, 9, 13, 10, 10, 12, 14, 9, 9, 12, 9, 13, 13, 10, 9, 13, 16, 14, 9, 9, 13, 9, 9, 10, 9, 9, 12, 10, 13, 6, 10, 9, 6, 6, 10, 16, 13, 12, 5, 9, 13, 10, 9, 10, 13, 9, 6, 10, 13, 12, 10, 9, 9, 12, 9, 9, 9, 6, 9, 9, 10, 13, 9, 13, 9, 10, 6, 10, 9, 9, 14, 10, 9, 12, 5, 10, 12, 9, 9, 10, 16, 5, 14, 10, 12, 10, 10, 10, 9, 3, 10, 10, 9, 10, 10, 6, 6, 10, 6, 14, 13, 5, 13, 10, 11, 10, 6, 13, 10, 9, 12, 9, 9, 10, 10, 9, 15, 12, 9, 14, 6, 9, 9, 3, 7, 9, 2, 11, 16, 10, 12, 9, 14, 13, 10, 13, 13, 9, 9, 11, 9, 9, 9, 8, 10, 13, 9, 9, 3, 9, 7, 13, 13, 9, 9, 13, 4, 6, 10, 9, 13, 10, 13, 9, 9, 10, 13, 9, 6, 10, 5, 13, 13, 7, 13, 9, 7, 4, 10, 13, 10, 9, 10, 12, 4, 13, 9, 13, 10, 10, 15, 7, 10, 10, 9, 10, 14, 13, 9, 12, 13, 9, 9, 9, 9, 10, 9, 10, 12, 5, 9, 13, 15, 13, 9, 10, 13, 6, 9, 10, 12, 10, 9, 13, 13, 14, 8, 9, 13, 9, 9, 1, 11, 13, 10, 8, 13, 13, 10, 13, 14, 9, 14, 10, 9, 10, 9, 11, 10, 9, 10, 9, 13, 14, 11, 9, 9, 8, 9, 13, 13, 5, 13, 13, 9, 9, 6, 9, 10, 7, 9, 14, 10, 10, 9, 10, 10, 14, 10, 15, 10, 9, 10, 10, 10, 12, 6, 9, 10, 6, 9, 9, 11, 6, 12, 5, 9, 15, 9, 10, 9, 9, 6, 13, 13, 9, 7, 6, 14, 16, 4, 16, 13, 9, 9, 10, 13, 10, 14, 9, 10, 13, 15, 9, 10, 13, 9, 10, 10, 11, 13, 12, 10, 7, 7, 10, 11, 9, 10, 9, 2, 10, 9, 10, 9, 13, 7, 10, 9, 11, 12, 9, 9, 13, 7, 9, 14, 4, 9, 10, 10, 10, 10, 8, 13, 13, 15, 7, 15, 13, 10, 14, 7, 13, 9, 16, 6, 10, 14, 10, 9, 16, 13, 10, 9, 4, 11, 10, 13, 8, 9, 10, 9, 7, 13, 10, 13, 9, 13, 9, 10, 14, 11, 13, 14, 10, 13, 13, 9, 7, 16, 9, 11, 10, 6, 9, 13, 13, 10, 10, 13, 13, 9, 9, 10, 13, 14, 4, 5, 13, 13, 13, 9, 4, 14, 10, 10, 11, 10, 13, 3, 9, 9, 9, 10, 10, 9, 9, 14, 4, 9, 3, 13, 13, 9, 4, 9, 14, 12, 9, 13, 10, 9, 10, 13, 9, 9, 9, 9, 10, 8, 10, 9, 14, 9, 10, 7, 9, 13, 9, 10, 9, 10, 15, 9, 9, 10, 14, 13, 13, 10, 4, 10, 14, 13, 3, 13, 13, 9, 11, 9, 10, 10, 9, 10, 14, 13, 9, 8, 10, 10, 9, 13, 8, 10, 9, 15, 10, 9, 7, 13, 9, 1, 10, 14, 10, 9, 10, 5, 10, 12, 9, 9, 9, 9, 6, 9, 16, 4, 9, 13, 9, 10, 11, 9, 14, 13, 9, 7, 10, 9, 13, 16, 9, 12, 16, 14, 9, 9, 5, 9, 9, 7, 9, 4, 10, 13, 11, 10, 12, 13, 9, 12, 10, 13, 9, 9, 10, 9, 9, 9, 10, 13, 10, 11, 9, 14, 9, 7, 9, 14, 10, 9, 12, 10, 7, 10, 10, 8, 9, 13, 9, 11, 10, 9, 13, 9, 9, 11, 9, 13, 13, 16, 9, 9, 9, 9, 14, 12, 9, 7, 15, 11, 9, 9, 12, 13, 10, 9, 10, 10, 9, 9, 9, 9, 13, 13, 9, 14, 16, 8, 10, 9, 13, 13, 9, 14, 9, 10, 10, 13, 13, 9, 13, 9, 15, 7, 10, 10, 9, 4, 9, 14, 9, 13, 13, 9, 7, 15, 8, 13, 9, 16, 7, 3, 13, 9, 13, 7, 13, 10, 9, 9, 13, 13, 9, 4, 15, 11, 15, 9, 9, 9, 10, 14, 9, 9, 13, 6, 10, 10, 9, 10, 9, 9, 6, 10, 12, 13, 14, 13, 13, 9, 13, 14, 13, 10, 15, 9, 13, 9, 10, 13, 10, 12, 9, 9, 7, 9, 16, 14, 10, 10, 5, 16, 9, 9, 10, 13, 10, 13, 10, 9, 10, 9, 10, 13, 9, 10, 15, 9, 11, 14, 10, 9, 10, 10, 14, 9, 10, 6, 13, 9, 7, 13, 14, 2, 9, 5, 15, 9, 10, 9, 10, 9, 9, 9, 10, 13, 9, 8, 13, 6, 9, 9, 13, 13, 16, 13, 9, 9, 13, 10, 9, 14, 14, 10, 9, 14, 9, 13, 13, 9, 10, 9, 13, 9, 10, 4, 9, 14, 13, 10, 9, 10, 13, 9, 10, 9, 4, 11, 13, 13, 9, 10, 14, 10, 9, 13, 11, 12, 10, 15, 10, 9, 10, 13, 13, 10, 10, 9, 9, 7, 11, 10, 13, 3, 16, 10, 14, 9, 5, 10, 9, 4, 13, 9, 12, 3, 9, 13, 10, 9, 9, 13, 10, 9, 10, 10, 10, 9, 10, 9, 9, 5, 4, 10, 13, 8, 16, 7, 5, 11, 10, 13, 6, 14, 9, 7, 13, 10, 14, 14, 13, 9, 10, 11, 9, 15, 10, 6, 9, 9, 10, 9, 9, 13, 9, 9, 10, 10, 16, 9, 9, 14, 10, 10, 9, 13, 9, 9, 9, 13, 13, 8, 15, 13, 9, 9, 10, 10, 13, 10, 13, 10, 14, 10, 6, 9, 13, 9, 10, 10, 10, 11, 10, 10, 9, 7, 12, 9, 9, 9, 11, 7, 10, 12, 12, 9, 9, 9, 11, 9, 13, 10, 9, 9, 10, 10, 10, 14, 9, 9, 10, 10, 13, 9, 13, 4, 10, 10, 9, 9, 9, 15, 11, 3, 9, 13, 5, 13, 9, 10, 14, 14, 6, 13, 9, 13, 11, 10, 11, 9, 9, 8, 13, 16, 10, 10, 2, 16, 9, 5, 10, 10, 13, 10, 9, 13, 15, 9, 15, 10, 9, 8, 10, 13, 5, 9, 9, 10, 13, 4, 9, 4, 9, 13, 10, 14, 9, 10, 6, 11, 8, 16, 11, 10, 11, 4, 15, 9, 12, 10, 9, 9, 13, 16, 9, 9, 10, 9, 9, 10, 13, 9, 10, 10, 13, 6, 10, 9, 13, 10, 14, 9, 7, 13, 10, 9, 13, 1, 9, 13, 9, 13, 4, 9, 9, 9, 12, 10, 14, 13, 10, 10, 9, 9, 10, 13, 10, 12, 9, 10, 12, 7, 13, 11, 4, 9, 11, 10, 9, 9, 10, 14, 9, 10, 9, 5, 10, 14, 9, 13, 13, 13, 10, 9, 9, 10, 9, 14, 13, 10, 10, 9, 13, 9, 14, 9, 10, 9, 9, 11, 9, 9, 9, 9, 10, 13, 16, 12, 9, 9, 10, 10, 9, 10, 9, 11, 13, 16, 9, 9, 13, 10, 9, 7, 10, 9, 7, 14, 13, 13, 10, 9, 12, 10, 10, 10, 9, 9, 10, 9, 12, 9, 1, 13, 9, 9, 10, 14, 11, 13, 10, 9, 9, 9, 15, 4, 11, 5, 9, 10, 10, 9, 11, 9, 13, 10, 13, 9, 15, 9, 9, 10, 8, 9, 13, 13, 7, 15, 9, 14, 6, 13, 10, 15, 13, 13, 10, 9, 12, 9, 14, 10, 9, 4, 9, 10, 11, 14, 7, 10, 9, 10, 10, 6, 9, 4, 13, 10, 7, 9, 13, 9, 6, 10, 9, 4, 10, 9, 9, 14, 13, 10, 10, 9, 10, 12, 9, 10, 12, 10, 6, 9, 9, 9, 13, 10, 9, 9, 9, 9, 9, 11, 10, 10, 9, 10, 13, 9, 9, 10, 13, 10, 13, 7, 4, 9, 6, 9, 14, 9, 6, 10, 9, 13, 13, 12, 10, 9, 10, 13, 9, 6, 14, 10, 9, 10, 13, 14, 13, 3, 14, 10, 12, 10, 13, 14, 9, 9, 13, 10, 11, 13, 14, 7, 10, 9, 9, 7, 6, 9, 9, 14, 14, 6, 9, 9, 10, 13, 6, 9, 2, 9, 10, 11, 10, 9, 10, 9, 12, 13, 9, 7, 9, 9, 9, 9, 7, 7, 3, 6, 10, 14, 13, 13, 9, 10, 10, 9, 4, 14, 16, 3, 13, 5, 9, 12, 9, 15, 13, 12, 9, 5, 10, 10, 13, 6, 10, 9, 9, 10, 13, 16, 10, 11, 12, 13, 9, 7, 10, 11, 3, 13, 9, 10, 12, 10, 13, 10, 12, 9, 9, 11, 11, 13, 9, 9, 9, 13, 9, 9, 9, 13, 7, 14, 13, 9, 9, 7, 13, 12, 7, 11, 9, 7, 10, 13, 12, 9, 13, 10, 5, 10, 13, 13, 13, 9, 13, 8, 13, 9, 6, 9, 15, 15, 9, 9, 13, 16, 6, 13, 10, 10, 9, 2, 5, 10, 9, 10, 3, 11, 13, 9, 13, 13, 9, 16, 9, 10, 13, 9, 7, 13, 10, 9, 7, 13, 13, 10, 9, 15, 9, 3, 13, 9, 9, 13, 14, 9, 10, 9, 10, 7, 15, 10, 13, 6, 9, 10, 13, 14, 9, 10, 9, 7, 10, 9, 14, 9, 4, 8, 9, 10, 13, 12, 4, 14, 13, 9, 6, 11, 10, 9, 7, 15, 9, 10, 8, 9, 11, 9, 9, 14, 9, 10, 9, 14, 10, 9, 13, 13, 4, 10, 10, 13, 9, 10, 11, 13, 10, 11, 9, 2, 9, 13, 10, 11, 7, 11, 9, 10, 14, 9, 11, 11, 14, 6, 13, 10, 10, 13, 9, 13, 13, 12, 9, 13, 10, 9, 9, 11, 4, 9, 10, 10, 13, 9, 10, 9, 13, 13, 13, 12, 6, 9, 9, 10, 10, 9, 10, 7, 13, 10, 10, 10, 9, 15, 13, 10, 9, 4, 10, 9, 13, 6, 8, 6, 16, 10, 10, 9, 14, 9, 12, 11, 13, 13, 12, 14, 2, 10, 9, 13, 9, 5, 4, 9, 9, 14, 13, 13, 9, 10, 8, 14, 13, 12, 14, 10, 9, 9, 11, 8, 13, 11, 12, 9, 13, 9, 9, 3, 10, 13, 9, 7, 14, 9, 14, 11, 9, 13, 6, 10, 13, 9, 13, 14, 15, 9, 10, 10, 10, 14, 10, 11, 13, 10, 9, 9, 13, 13, 14, 9, 13, 9, 10, 9, 16, 13, 11, 13, 8, 10, 10, 9, 9, 13, 11, 9, 1, 9, 9, 9, 9, 14, 10, 14, 13, 9, 10, 11, 4, 14, 14, 6, 4, 10, 14, 9, 9, 10, 9, 9, 9, 10, 9, 9, 8, 10, 13, 7, 10, 10, 6, 6, 9, 9, 13, 10, 10, 2, 9, 10, 12, 11, 10, 13, 13, 9, 10, 9, 9, 9, 13, 10, 13, 10, 9, 9, 13, 12, 13, 13, 9, 9, 10, 14, 15, 13, 10, 14, 9, 10, 14, 6, 9, 9, 4, 9, 11, 13, 12, 13, 9, 9, 10, 1, 13, 10, 9, 10, 14, 14, 13, 14, 9, 10, 10, 13, 9, 10, 9, 13, 8, 9, 9, 13, 14, 9, 10, 6, 13, 9, 10, 13, 9, 9, 9, 10, 9, 9, 7, 9, 13, 13, 7, 9, 9, 9, 14, 10, 10, 9, 7, 13, 13, 13, 9, 12, 10, 9, 9, 9, 7, 13, 9, 10, 12, 11, 7, 10, 10, 13, 9, 10, 16, 8, 13, 4, 13, 11, 13, 13, 9, 16, 6, 13, 9, 4, 13, 9, 11, 11, 9, 10, 9, 14, 3, 13, 13, 7, 13, 10, 9, 10, 9, 6, 10, 9, 6, 9, 13, 13, 14, 13, 9, 9, 7, 11, 10, 10, 10, 10, 9, 11, 14, 10, 13, 10, 9, 13, 7, 13, 6, 10, 13, 3, 9, 15, 10, 10, 13, 14, 9, 9, 9, 10, 13, 9, 10, 13, 7, 7, 9, 10, 10, 9, 16, 12, 13, 9, 9, 9, 9, 13, 9, 9, 9, 14, 3, 13, 9, 3, 9, 13, 9, 8, 9, 10, 7, 15, 9, 9, 6, 10, 9, 9, 9, 3, 9, 13, 13, 9, 8, 9, 13, 6, 10, 13, 15, 14, 13, 10, 13, 14, 10, 13, 9, 13, 10, 10, 4, 10, 13, 9, 12, 10, 11, 10, 10, 9, 9, 7, 13, 11, 12, 10, 9, 10, 10, 7, 13, 13, 9, 10, 15, 10, 13, 13, 13, 11, 12, 10, 9, 10, 9, 9, 8, 5, 9, 9, 10, 7, 9, 7, 3, 13, 13, 10, 14, 10, 13, 13, 10, 4, 13, 13, 13, 7, 7, 10, 10, 3, 13, 6, 9, 12, 9, 9, 3, 7, 13, 9, 9, 10, 9, 9, 10, 13, 10, 11, 9, 13, 10, 10, 8, 9, 9, 2, 13, 9, 9, 9, 10, 13, 9, 6, 10, 9, 10, 4, 10, 3, 13, 10, 8, 10, 9, 9, 11, 10, 10, 7, 9, 9, 9, 4, 9, 7, 12, 9, 10, 13, 10, 10, 13, 9, 10, 4, 11, 9, 4, 7, 9, 9, 9, 13, 13, 11, 10, 9, 7, 14, 9, 9, 10, 9, 10, 9, 13, 12, 9, 12, 9, 10, 10, 9, 14, 9, 9, 9, 10, 9, 9, 5, 9, 9, 10, 10, 10, 10, 15, 7, 13, 9, 7, 11, 9, 10, 14, 11, 11, 9, 11, 10, 9, 13, 9, 8, 13, 12, 13, 13, 13, 12, 6, 14, 14, 14, 13, 9, 14, 13, 10, 13, 9, 10, 13, 9, 9, 9, 6, 14, 10, 5, 9, 7, 12, 13, 9, 13, 13, 7, 13, 9, 9, 7, 14, 13, 13, 11, 10, 12, 9, 10, 10, 11, 13, 7, 9, 12, 4, 9, 11, 11, 7, 9, 9, 9, 14, 6, 9, 6, 9, 6, 10, 9, 7, 14, 4, 13, 9, 10, 9, 6, 4, 14, 9, 11, 9, 3, 4, 10, 13, 10, 10, 7, 12, 14, 14, 9, 9, 3, 9, 8, 9, 13, 5, 9, 9, 9, 13, 9, 10, 10, 9, 9, 10, 10, 9, 13, 9, 9, 10, 9, 3, 10, 11, 12, 13, 13, 10, 13, 13, 15, 9, 6, 11, 9, 5, 7, 13, 7, 9, 11, 10, 12, 4, 10, 14, 7, 10, 3, 14, 10, 9, 9, 7, 9, 15, 10, 13, 5, 8, 9, 10, 13, 14, 9, 9, 6, 10, 4, 15, 9, 10, 9, 10, 10, 10, 10, 9, 12, 10, 10, 10, 10, 14, 13, 10, 10, 10, 8, 9, 10, 10, 9, 3, 10, 10, 9, 9, 13, 14, 10, 9, 13, 9, 16, 13, 9, 10, 9, 12, 9, 13, 9, 13, 13, 10, 10, 9, 2, 10, 9, 13, 13, 9, 9, 7, 5, 13, 15, 15, 10, 9, 14, 9, 15, 14, 14, 7, 12, 10, 14, 10, 9, 10, 6, 9, 9, 14, 9, 13, 9, 10, 9, 10, 10, 10, 4, 13, 13, 9, 9, 9, 7, 10, 9, 8, 9, 9, 10, 9, 9, 10, 9, 10, 15, 9, 13, 10, 10, 12, 9, 4, 9, 9, 5, 9, 10, 12, 10, 13, 9, 6, 9, 13, 10, 7, 13, 11, 14, 9, 9, 9, 9, 9, 4, 10, 7, 10, 5, 13, 10, 3, 10, 13, 10, 13, 10, 9, 9, 13, 13, 10, 9, 9, 10, 9, 14, 14, 9, 13, 9, 13, 10, 9, 10, 10, 10, 9, 11, 10, 14, 9, 9, 11, 9, 10, 13, 13, 10, 11, 10, 10, 13, 13, 11, 4, 9, 13, 14, 9, 3, 13, 11, 9, 9, 9, 6, 14, 9, 9, 4, 12, 9, 9, 10, 3, 10, 9, 9, 10, 9, 2, 6, 10, 9, 12, 10, 9, 2, 9, 10, 9, 12, 5, 10, 9, 13, 10, 11, 10, 13, 10, 9, 9, 9, 10, 4, 10, 10, 13, 9, 11, 12, 6, 9, 12, 8, 10, 12, 9, 9, 10, 9, 9, 9, 9, 9, 13, 9, 2, 5, 13, 9, 9, 10, 13, 13, 10, 13, 11, 8, 4, 9, 13, 8, 14, 9, 9, 9, 14, 13, 4, 9, 10, 9, 16, 13, 9, 13, 9, 15, 13, 9, 9, 9, 6, 9, 13, 9, 10, 9, 10, 9, 10, 10, 9, 8, 9, 10, 9, 13, 10, 10, 9, 6, 9, 10, 4, 10, 9, 9, 9, 15, 9, 9, 13, 5, 13, 4, 13, 12, 10, 13, 13, 9, 2, 10, 13, 9, 14, 9, 9, 14, 9, 9, 13, 13, 10, 9, 11, 9, 5, 5, 9, 10, 10, 16, 9, 13, 10, 9, 9, 14, 13, 13, 9, 13, 9, 9, 9, 7, 9, 10, 9, 7, 13, 16, 9, 9, 16, 10, 10, 10, 13, 9, 9, 10, 7, 14, 9, 10, 9, 10, 9, 9, 9, 10, 13, 10, 15, 15, 13, 10, 10, 9, 13, 10, 9, 14, 12, 4, 9, 13, 8, 9, 10, 9, 9, 11, 8, 9, 2, 10, 3, 9, 13, 13, 13, 13, 9, 5, 10, 12, 9, 16, 9, 9, 13, 3, 9, 7, 10, 10, 9, 10, 9, 13, 13, 13, 10, 9, 2, 9, 9, 9, 7, 9, 9, 10, 9, 16, 9, 10, 9, 9, 6, 7, 15, 9, 16, 16, 9, 9, 13, 9, 14, 9, 13, 10, 10, 9, 9, 10, 7, 7, 9, 9, 9, 10, 11, 13, 14, 9, 9, 12, 14, 10, 13, 9, 3, 9, 10, 10, 12, 15, 9, 9, 9, 9, 9, 10, 10, 13, 9, 10, 10, 10, 9, 9, 10, 9, 14, 9, 9, 9, 14, 5, 13, 15, 14, 10, 7, 13, 9, 9, 9, 9, 10, 13, 4, 10, 14, 13, 10, 9, 13, 13, 9, 9, 9, 9, 10, 16, 9, 10, 9, 11, 13, 10, 9, 9, 10, 9, 10, 10, 13, 10, 9, 9, 9, 9, 13, 13, 9, 9, 13, 9, 9, 10, 9, 13, 13, 14, 10, 9, 9, 6, 13, 9, 13, 13, 9, 9, 10, 9, 11, 13, 10, 1, 13, 10, 10, 9, 9, 9, 10, 7, 9, 13, 4, 10, 9, 9, 9, 6, 10, 9, 5, 9, 3, 10, 9, 9, 10, 13, 9, 9, 7, 10, 9, 14, 5, 10, 9, 10, 9, 14, 13, 9, 10, 9, 11, 9, 10, 3, 3, 7, 9, 4, 10, 13, 10, 9, 12, 10, 14, 11, 13, 9, 7, 13, 13, 13, 9, 9, 13, 9, 9, 9, 9, 10, 5, 9, 13, 9, 5, 10, 9, 13, 10, 6, 6, 9, 12, 5, 13, 3, 10, 9, 9, 12, 9, 13, 9, 9, 9, 13, 5, 9, 11, 13, 9, 10, 9, 4, 10, 10, 12, 10, 10, 2, 13, 11, 10, 16, 13, 13, 13, 12, 9, 7, 9, 15, 9, 11, 10, 14, 4, 10, 9, 6, 15, 9, 11, 10, 7, 13, 14, 13, 13, 14, 13, 9, 10, 6, 9, 10, 9, 9, 10, 14, 9, 9, 13, 10, 9, 13, 12, 10, 10, 14, 9, 11, 4, 14, 13, 13, 3, 10, 10, 6, 7, 14, 9, 12, 6, 9, 10, 9, 10, 14, 13, 9, 9, 9, 5, 10, 9, 13, 3, 10, 4, 9, 10, 10, 9, 9, 13, 9, 13, 2, 9, 13, 9, 9, 13, 3, 10, 3, 13, 12, 10, 9, 10, 11, 10, 9, 9, 9, 12, 10, 10, 10, 9, 13, 13, 7, 10, 9, 13, 10, 16, 9, 10, 10, 7, 12, 9, 7, 9, 12, 9, 9, 10, 13, 13, 9, 13, 11, 10, 9, 11, 9, 10, 15, 15, 10, 9, 9, 9, 7, 13, 11, 9, 9, 10, 10, 5, 15, 10, 7, 13, 16, 14, 14, 10, 12, 11, 13, 9, 10, 10, 10, 13, 14, 9, 10, 9, 5, 9, 4, 10, 13, 14, 13, 11, 7, 9, 12, 10, 14, 14, 9, 11, 9, 9, 9, 10, 14, 10, 9, 9, 10, 9, 10, 7, 10, 3, 4, 9, 11, 9, 13, 10, 9, 6, 14, 9, 9, 9, 9, 10, 15, 10, 13, 9, 10, 12, 9, 9, 9, 9, 7, 9, 3, 8, 11, 9, 10, 13, 14, 14, 3, 9, 10, 13, 7, 7, 10, 10, 9, 16, 13, 13, 10, 9, 10, 9, 6, 9, 9, 9, 7, 4, 13, 7, 10, 14, 2, 10, 10, 13, 6, 10, 13, 11, 6, 6, 9, 12, 10, 11, 9, 13, 9, 16, 9, 10, 12, 13, 9, 14, 10, 9, 13, 9, 9, 9, 10, 10, 13, 10, 9, 13, 7, 3, 9, 13, 8, 10, 10, 9, 10, 9, 10, 10, 9, 10, 13, 10, 14, 4, 10, 15, 6, 16, 10, 4, 10, 9, 6, 9, 9, 9, 9, 3, 9, 9, 9, 9, 10, 12, 9, 13, 14, 9, 13, 9, 10, 10, 9, 4, 10, 9, 6, 10, 13, 13, 13, 13, 10, 13, 13, 11, 3, 13, 9, 9, 10, 9, 13, 10, 13, 9, 10, 9, 14, 10, 9, 14, 9, 9, 9, 13, 12, 13, 9, 10, 10, 13, 13, 10, 14, 10, 9, 13, 9, 12, 10, 9, 13, 7, 10, 14, 10, 13, 9, 4, 9, 7, 9, 9, 10, 11, 10, 9, 13, 13, 9, 10, 9, 13, 9, 9, 9, 10, 10, 11, 9, 9, 14, 4, 9, 13, 9, 10, 10, 14, 6, 14, 12, 4, 10, 10, 12, 13, 10, 9, 10, 10, 10, 13, 4, 13, 9, 9, 10, 9, 10, 10, 13, 4, 10, 14, 5, 10, 10, 9, 10, 10, 13, 15, 9, 16, 5, 9, 9, 10, 10, 10, 11, 9, 13, 10, 10, 6, 9, 9, 9, 9, 14, 9, 10, 10, 7, 13, 9, 13, 13, 12, 14, 14, 13, 9, 13, 9, 9, 9, 9, 8, 9, 10, 5, 9, 9, 9, 10, 9, 5, 13, 10, 10, 10, 14, 9, 16, 9, 10, 12, 9, 9, 5, 13, 10, 10, 9, 10, 9, 9, 14, 10, 10, 4, 10, 10, 13, 10, 9, 13, 13, 14, 6, 10, 9, 11, 8, 9, 13, 11, 9, 9, 9, 10, 11, 10, 4, 10, 9, 3, 9, 13, 9, 10, 13, 10, 12, 10, 10, 10, 14, 10, 9, 9, 4, 9, 4, 10, 9, 13, 9, 5, 13, 10, 8, 10, 10, 9, 11, 10, 10, 10, 9, 10, 6, 9, 13, 13, 11, 10, 11, 10, 10, 9, 11, 10, 10, 10, 9, 10, 10, 10, 7, 9, 5, 16, 13, 7, 9, 10, 10, 14, 16, 9, 10, 5, 9, 13, 10, 13, 7, 9, 9, 7, 10, 13, 10, 14, 13, 11, 14, 9, 9, 9, 9, 10, 6, 9, 9, 10, 9, 10, 9, 9, 9, 13, 13, 4, 9, 10, 10, 13, 10, 10, 10, 13, 10, 9, 10, 10, 9, 9, 13, 14, 10, 9, 10, 9, 13, 9, 9, 10, 11, 10, 9, 9, 9, 9, 10, 9, 11, 13, 12, 9, 12, 13, 10, 10, 7, 9, 10, 9, 9, 10, 10, 9, 10, 6, 10, 13, 6, 15, 10, 13, 9, 9, 9, 13, 13, 13, 10, 10, 10, 4, 9, 15, 13, 13, 9, 9, 13, 9, 9, 11, 9, 13, 10, 9, 9, 15, 9, 8, 9, 6, 10, 11, 9, 10, 13, 9, 14, 9, 10, 13, 3, 10, 5, 10, 13, 13, 14, 9, 10, 6, 10, 10, 9, 9, 3, 13, 4, 13, 9, 12, 11, 10, 10, 9, 10, 14, 15, 9, 9, 9, 13, 12, 10, 10, 9, 9, 10, 9, 14, 14, 9, 13, 9, 13, 10, 13, 9, 8, 10, 13, 10, 9, 2, 13, 14, 10, 13, 9, 15, 10, 10, 9, 9, 13, 10, 10, 10, 10, 9, 3, 4, 9, 9, 7, 9, 9, 6, 9, 9, 9, 10, 9, 14, 9, 13, 9, 9, 13, 10, 9, 9, 9, 9, 15, 9, 13, 12, 13, 11, 6, 10, 9, 9, 14, 13, 10, 13, 9, 9, 9, 9, 3, 9, 10, 13, 9, 9, 10, 9, 9, 9, 10, 5, 9, 9, 9, 9, 13, 9, 10, 9, 3, 11, 9, 10, 9, 12, 9, 9, 12, 9, 13, 9, 2, 9, 9, 10, 13, 2, 11, 11, 11, 9, 9, 9, 11, 13, 9, 13, 15, 9, 13, 12, 10, 13, 15, 7, 6, 9, 9, 10, 10, 2, 7, 9, 10, 10, 10, 9, 10, 11, 9, 10, 5, 10, 9, 9, 2, 7, 10, 15, 10, 4, 13, 13, 7, 9, 13, 13, 10, 10, 15, 4, 7, 13, 9, 10, 10, 10, 9, 9, 8, 11, 9, 14, 11, 9, 14, 9, 12, 9, 13, 13, 9, 11, 10, 9, 9, 9, 10, 9, 9, 13, 10, 13, 13, 14, 10, 6, 6, 10, 10, 9, 9, 9, 9, 10, 10, 7, 10, 10, 10, 10, 16, 12, 10, 9, 6, 9, 10, 9, 12, 5, 7, 16, 11, 10, 14, 10, 9, 9, 14, 10, 9, 13, 11, 10, 13, 10, 10, 9, 9, 10, 13, 10, 13, 9, 6, 13, 11, 13, 9, 9, 13, 10, 9, 9, 13, 9, 9, 9, 7, 9, 3, 9, 13, 13, 9, 13, 6, 11, 10, 10, 10, 10, 13, 9, 13, 10, 10, 9, 16, 13, 13, 2, 10, 13, 12, 9, 9, 9, 9, 9, 7, 10, 12, 9, 9, 15, 10, 4, 6, 10, 13, 14, 13, 14, 9, 9, 10, 10, 9, 10, 14, 13, 13, 13, 9, 9, 13, 6, 15, 11, 11, 9, 13, 13, 13, 13, 10, 5, 12, 13, 9, 9, 7, 10, 10, 9, 11, 11, 9, 9, 10, 9, 13, 7, 11, 10, 10, 10, 10, 15, 12, 13, 9, 10, 4, 10, 9, 10, 14, 9, 7, 13, 10, 13, 4, 13, 12, 3, 14, 13, 9, 6, 2, 13, 11, 9, 12, 13, 13, 5, 9, 13, 14, 13, 13, 12, 13, 9, 11, 9, 9, 10, 9, 9, 11, 13, 9, 13, 13, 9, 10, 12, 10, 10, 14, 15, 13, 9, 10, 9, 14, 15, 10, 8, 9, 13, 9, 9, 10, 9, 13, 10, 9, 9, 6, 9, 13, 10, 14, 11, 12, 15, 9, 2, 11, 9, 8, 13, 8, 12, 13, 10, 10, 9, 10, 9, 9, 9, 10, 9, 15, 7, 9, 10, 12, 14, 13, 12, 10, 13, 10, 13, 9, 13, 12, 9, 11, 13, 13, 9, 12, 9, 9, 13, 10, 12, 14, 13, 10, 11, 10, 10, 7, 3, 10, 13, 10, 13, 9, 13, 13, 6, 13, 15, 10, 13, 13, 7, 10, 13, 9, 4, 10, 9, 9, 9, 9, 13, 10, 9, 15, 9, 9, 13, 11, 7, 11, 9, 9, 11, 10, 10, 11, 13, 10, 9, 10, 13, 10, 5, 9, 9, 10, 10, 10, 9, 10, 10, 9, 13, 3, 10, 2, 4, 12, 9, 14, 10, 13, 13, 14, 9, 13, 10, 14, 9, 10, 9, 10, 9, 15, 13, 7, 2, 9, 14, 14, 9, 9, 9, 13, 10, 9, 11, 9, 14, 10, 10, 16, 14, 9, 9, 10, 11, 6, 6, 10, 14, 9, 15, 10, 7, 9, 9, 13, 9, 10, 10, 10, 9, 10, 13, 9, 13, 10, 14, 10, 10, 9, 9, 14, 9, 9, 10, 4, 16, 13, 9, 10, 9, 13, 16, 10, 7, 10, 10, 4, 7, 10, 15, 13, 9, 9, 10, 5, 13, 9, 5, 6, 13, 7, 14, 10, 10, 13, 13, 9, 14, 9, 10, 10, 9, 9, 10, 5, 9, 9, 10, 12, 7, 10, 13, 13, 9, 14, 13, 9, 9, 13, 6, 9, 14, 9, 14, 9, 13, 10, 12, 10, 7, 9, 9, 14, 11, 10, 9, 9, 10, 9, 7, 10, 10, 9, 14, 10, 10, 13, 10, 16, 13, 13, 10, 7, 10, 10, 10, 9, 10, 13, 10, 10, 11, 10, 7, 10, 10, 9, 9, 13, 10, 12, 11, 9, 13, 9, 12, 9, 13, 10, 10, 13, 9, 9, 13, 9, 9, 14, 10, 9, 9, 9, 6, 9, 9, 9, 9, 9, 8, 7, 11, 11, 10, 10, 15, 13, 9, 14, 9, 11, 14, 7, 10, 5, 14, 13, 10, 13, 6, 11, 9, 9, 10, 9, 13, 6, 13, 9, 14, 9, 6, 9, 9, 14, 9, 9, 11, 14, 7, 6, 7, 13, 13, 9, 13, 10, 10, 11, 13, 15, 9, 9, 3, 9, 9, 9, 13, 7, 10, 9, 7, 4, 9, 10, 9, 14, 13, 12, 9, 10, 10, 9, 9, 10, 7, 9, 9, 9, 10, 10, 9, 13, 7, 9, 9, 5, 5, 13, 13, 10, 16, 9, 10, 9, 9, 14, 9, 12, 9, 5, 8, 11, 11, 9, 9, 6, 12, 9, 8, 9, 9, 13, 13, 12, 11, 9, 9, 10, 10, 10, 13, 10, 10, 7, 7, 9, 13, 13, 10, 6, 13, 10, 9, 10, 10, 15, 12, 10, 9, 4, 9, 10, 7, 15, 10, 10, 6, 7, 11, 9, 14, 10, 10, 10, 13, 13, 9, 10, 10, 8, 13, 10, 9, 10, 10, 10, 9, 7, 9, 14, 9, 10, 7, 14, 12, 9, 11, 13, 9, 7, 10, 9, 15, 7, 15, 10, 14, 10, 9, 10, 10, 14, 9, 8, 13, 13, 10, 9, 14, 10, 10, 9, 9, 9, 14, 10, 5, 9, 10, 14, 4, 9, 9, 9, 10, 10, 9, 13, 9, 15, 13, 9, 9, 3, 4, 9, 9, 13, 9, 9, 10, 10, 5, 14, 10, 13, 9, 4, 9, 6, 13, 13, 9, 10, 9, 10, 11, 10, 9, 9, 4, 10, 12, 9, 10, 13, 10, 7, 10, 13, 11, 9, 7, 3, 10, 10, 6, 10, 9, 9, 9, 9, 7, 7, 9, 10, 16, 13, 9, 9, 13, 10, 9, 13, 9, 13, 11, 9, 3, 4, 11, 12, 9, 9, 10, 11, 9, 9, 9, 3, 11, 8, 10, 9, 9, 6, 9, 10, 9, 6, 9, 9, 14, 14, 12, 9, 13, 10, 9, 9, 9, 14, 10, 9, 9, 9, 12, 8, 6, 14, 10, 9, 9, 10, 6, 9, 9, 10, 10, 14, 7, 9, 13, 13, 7, 11, 6, 11, 11, 10, 3, 10, 14, 10, 10, 9, 9, 10, 13, 9, 13, 10, 10, 9, 15, 9, 13, 13, 11, 13, 9, 9, 13, 9, 11, 9, 9, 9, 9, 6, 9, 13, 6, 9, 9, 9, 10, 10, 8, 9, 9, 10, 9, 10, 9, 13, 9, 3, 8, 10, 11, 13, 9, 13, 9, 9, 11, 9, 10, 13, 9, 4, 9, 9, 10, 13, 14, 9, 12, 9, 10, 3, 11, 16, 12, 9, 10, 9, 10, 9, 10, 10, 9, 14, 6, 6, 10, 9, 10, 7, 9, 10, 9, 10, 10, 9, 9, 9, 3, 9, 10, 7, 10, 14, 8, 10, 10, 13, 9, 10, 9, 9, 10, 10, 9, 13, 9, 10, 14, 4, 12, 9, 5, 14, 13, 6, 9, 16, 12, 9, 11, 9, 10, 13, 10, 7, 10, 9, 14, 9, 9, 13, 10, 9, 16, 12, 2, 9, 10, 9, 13, 9, 13, 7, 14, 12, 9, 10, 10, 11, 12, 9, 13, 10, 12, 10, 13, 7, 5, 9, 9, 9, 9, 11, 12, 3, 10, 9, 12, 9, 9, 6, 9, 13, 13, 14, 11, 9, 4, 9, 10, 9, 9, 13, 9, 10, 6, 13, 10, 7, 9, 7, 11, 10, 10, 13, 9, 13, 9, 13, 9, 10, 10, 13, 13, 6, 13, 9, 9, 9, 15, 7, 10, 6, 14, 9, 9, 13, 11, 12, 13, 9, 12, 9, 11, 13, 9, 9, 13, 14, 13, 13, 9, 10, 10, 14, 9, 10, 10, 13, 13, 9, 9, 9, 5, 13, 13, 13, 4, 9, 16, 9, 9, 12, 11, 10, 8, 9, 13, 12, 9, 15, 10, 7, 9, 6, 10, 13, 14, 10, 9, 9, 9, 9, 5, 9, 10, 13, 14, 7, 10, 10, 11, 9, 10, 9, 9, 10, 9, 9, 13, 10, 9, 13, 10, 12, 10, 6, 9, 9, 10, 9, 9, 9, 9, 10, 13, 9, 9, 9, 13, 9, 9, 13, 14, 5, 13, 13, 10, 9, 9, 10, 9, 10, 6, 9, 13, 12, 9, 9, 12, 10, 15, 9, 9, 9, 13, 9, 8, 10, 9, 10, 7, 10, 9, 10, 14, 10, 10, 11, 10, 13, 11, 6, 7, 10, 10, 9, 9, 10, 6, 9, 9, 12, 12, 15, 15, 9, 10, 6, 12, 10, 9, 9, 13, 10, 10, 12, 9, 9, 12, 9, 2, 9, 13, 9, 9, 10, 9, 9, 10, 12, 13, 13, 7, 13, 10, 7, 12, 4, 9, 13, 13, 11, 9, 10, 14, 10, 7, 15, 14, 12, 6, 11, 15, 14, 14, 13, 9, 2, 9, 10, 9, 10, 13, 5, 7, 13, 9, 9, 13, 8, 9, 9, 2, 2, 13, 9, 9, 9, 14, 9, 13, 11, 10, 6, 13, 14, 13, 14, 12, 9, 10, 9, 9, 9, 13, 9, 10, 9, 9, 11, 15, 10, 9, 9, 10, 8, 13, 9, 8, 10, 10, 9, 9, 8, 5, 13, 13, 10, 5, 13, 9, 9, 5, 10, 12, 10, 12, 10, 13, 14, 7, 9, 10, 9, 16, 9, 11, 9, 10, 9, 4, 10, 9, 7, 9, 11, 9, 9, 13, 13, 13, 9, 7, 13, 9, 9, 10, 9, 9, 10, 10, 14, 9, 9, 11, 9, 9, 13, 10, 9, 13, 14, 9, 13, 7, 9, 9, 9, 9, 10, 9, 9, 9, 9, 10, 11, 9, 10, 10, 13, 9, 16, 12, 13, 9, 13, 12, 13, 11, 7, 13, 9, 9, 10, 9, 9, 9, 3, 14, 10, 13, 12, 13, 10, 13, 14, 10, 7, 10, 9, 9, 9, 11, 9, 13, 10, 10, 13, 9, 14, 4, 13, 14, 9, 10, 9, 10, 8, 9, 9, 4, 9, 10, 9, 13, 10, 9, 11, 10, 9, 16, 10, 10, 9, 13, 6, 7, 11, 13, 9, 3, 9, 13, 13, 13, 10, 13, 10, 14, 14, 11, 9, 13, 10, 5, 10, 13, 10, 9, 10, 9, 12, 14, 13, 9, 9, 12, 14, 10, 7, 9, 14, 14, 14, 10, 13, 13, 3, 10, 9, 10, 7, 9, 9, 2, 13, 10, 9, 12, 14, 6, 13, 14, 11, 8, 9, 9, 10, 5, 9, 9, 10, 14, 10, 11, 9, 10, 10, 9, 11, 9, 9, 5, 9, 9, 10, 13, 10, 10, 6, 5, 11, 10, 10, 14, 9, 14, 7, 10, 10, 10, 10, 14, 13, 9, 13, 6, 2, 13, 9, 13, 9, 14, 13, 15, 13, 14, 10, 12, 11, 10, 10, 5, 15, 10, 9, 9, 13, 13, 11, 10, 10, 9, 9, 9, 13, 9, 10, 13, 9, 13, 9, 10, 9, 9, 9, 10, 9, 10, 9, 9, 16, 11, 9, 13, 10, 10, 10, 9, 10, 10, 9, 10, 6, 14, 13, 10, 10, 9, 8, 10, 4, 13, 13, 11, 11, 9, 10, 12, 10, 10, 10, 15, 11, 9, 9, 12, 10, 10, 13, 14, 9, 9, 9, 9, 11, 6, 10, 13, 6, 7, 10, 5, 9, 10, 6, 10, 7, 2, 13, 9, 10, 9, 8, 9, 10, 13, 12, 10, 9, 15, 13, 9, 9, 6, 9, 13, 10, 9, 13, 10, 11, 15, 9, 15, 9, 9, 10, 13, 6, 9, 6, 10, 9, 11, 10, 13, 9, 11, 9, 10, 5, 13, 10, 13, 13, 13, 10, 9, 9, 9, 14, 10, 9, 9, 9, 12, 9, 14, 9, 6, 9, 14, 11, 10, 13, 7, 14, 10, 4, 5, 10, 10, 10, 10, 10, 13, 10, 11, 13, 13, 12, 9, 13, 9, 16, 9, 12, 9, 15, 10, 9, 13, 9, 13, 13, 10, 9, 10, 9, 10, 10, 10, 13, 9, 9, 13, 9, 14, 9, 13, 10, 14, 14, 9, 13, 10, 13, 9, 13, 5, 13, 13, 10, 10, 3, 14, 9, 3, 9, 9, 13, 2, 15, 6, 6, 13, 9, 7, 9, 10, 9, 9, 13, 13, 9, 14, 10, 9, 9, 9, 10, 10, 9, 13, 9, 14, 13, 12, 9, 9, 13, 15, 13, 11, 10, 9, 9, 10, 9, 10, 10, 12, 13, 15, 9, 14, 9, 10, 6, 9, 14, 10, 13, 9, 10, 9, 10, 9, 10, 14, 14, 9, 7, 11, 7, 7, 10, 10, 7, 9, 9, 10, 10, 13, 9, 14, 14, 14, 9, 9, 13, 14, 10, 9, 7, 9, 9, 9, 9, 12, 10, 4, 9, 14, 10, 10, 9, 8, 15, 9, 9, 8, 9, 9, 13, 9, 11, 9, 12, 9, 12, 13, 13, 10, 11, 12, 13, 10, 6, 10, 13, 10, 9, 9, 10, 13, 9, 11, 10, 11, 9, 13, 13, 13, 3, 13, 16, 9, 14, 11, 11, 10, 13, 2, 13, 9, 10, 9, 11, 12, 9, 9, 9, 10, 10, 6, 14, 10, 9, 10, 4, 9, 10, 13, 7, 9, 11, 9, 13, 6, 13, 9, 9, 10, 9, 3, 13, 9, 10, 13, 3, 8, 10, 13, 9, 9, 16, 9, 9, 13, 13, 9, 9, 9, 16, 9, 9, 11, 9, 10, 9, 13, 7, 13, 9, 15, 9, 12, 10, 10, 10, 9, 13, 13, 9, 9, 15, 14, 11, 6, 10, 14, 14, 9, 5, 13, 6, 12, 10, 9, 8, 9, 13, 13, 12, 9, 13, 9, 10, 16, 13, 9, 9, 10, 9, 9, 5, 9, 15, 13, 9, 9, 10, 13, 9, 10, 13, 9, 13, 9, 4, 13, 13, 9, 9, 9, 4, 9, 10, 9, 13, 9, 9, 10, 9, 13, 15, 10, 13, 14, 13, 10, 4, 6, 10, 9, 9, 10, 9, 10, 9, 9, 11, 4, 9, 14, 11, 9, 13, 13, 10, 9, 9, 9, 6, 9, 9, 8, 7, 10, 9, 10, 10, 13, 9, 11, 12, 10, 10, 10, 9, 9, 10, 12, 10, 7, 8, 9, 9, 9, 9, 10, 11, 10, 5, 10, 9, 9, 12, 13, 10, 10, 9, 10, 11, 9, 10, 4, 10, 9, 10, 10, 14, 11, 13, 10, 7, 16, 12, 4, 15, 13, 13, 10, 9, 10, 12, 13, 15, 15, 15, 7, 13, 11, 10, 13, 10, 10, 10, 13, 9, 9, 10, 6, 9, 9, 13, 2, 10, 13, 10, 9, 10, 13, 10, 9, 10, 6, 13, 9, 16, 9, 9, 13, 12, 10, 15, 10, 9, 13, 10, 9, 10, 13, 9, 9, 10, 9, 9, 10, 14, 9, 10, 1, 15, 10, 14, 14, 7, 10, 10, 9, 13, 12, 9, 10, 9, 10, 13, 4, 16, 14, 13, 15, 11, 5, 9, 9, 10, 5, 10, 10, 10, 14, 9, 15, 9, 10, 11, 10, 13, 12, 12, 9, 12, 9, 9, 10, 13, 10, 10, 13, 10, 11, 11, 10, 15, 10, 9, 11, 9, 13, 9, 9, 9, 10, 10, 11, 13, 8, 2, 9, 6, 13, 10, 9, 7, 13, 2, 9, 9, 9, 13, 9, 9, 12, 10, 13, 10, 14, 10, 13, 6, 9, 12, 10, 10, 9, 14, 10, 10, 10, 8, 13, 10, 9, 13, 6, 14, 6, 8, 12, 14, 10, 9, 14, 10, 9, 9, 13, 9, 5, 5, 10, 4, 10, 13, 9, 10, 10, 13, 11, 7, 9, 9, 9, 13, 10, 9, 10, 7, 10, 10, 13, 9, 9, 4, 13, 10, 13, 5, 14, 10, 12, 13, 13, 7, 9, 13, 6, 11, 7, 9, 9, 9, 12, 8, 9, 9, 9, 9, 16, 10, 9, 9, 13, 14, 15, 10, 13, 10, 10, 11, 9, 9, 10, 9, 13, 10, 10, 13, 11, 10, 10, 10, 15, 10, 9, 10, 7, 10, 12, 10, 8, 10, 10, 14, 12, 13, 10, 9, 9, 9, 2, 14, 9, 9, 14, 9, 13, 13, 9, 14, 7, 13, 10, 12, 9, 8, 9, 10, 9, 14, 13, 14, 10, 4, 13, 9, 4, 1, 7, 13, 13, 9, 13, 9, 11, 13, 2, 10, 13, 10, 9, 14, 9, 13, 10, 9, 13, 9, 8, 13, 10, 10, 9, 16, 10, 11, 13, 8, 13, 9, 8, 9, 9, 13, 9, 4, 11, 14, 10, 11, 13, 13, 5, 13, 10, 9, 5, 10, 7, 9, 9, 9, 13, 13, 13, 11, 9, 13, 11, 9, 9, 10, 9, 13, 14, 4, 9, 14, 10, 10, 10, 12, 9, 5, 13, 4, 14, 9, 6, 9, 10, 10, 10, 9, 9, 10, 9, 9, 13, 7, 11, 10, 14, 15, 10, 9, 9, 9, 14, 14, 7, 15, 9, 10, 10, 9, 10, 9, 10, 12, 13, 11, 13, 8, 7, 3, 9, 10, 13, 10, 9, 13, 10, 9, 13, 13, 13, 10, 13, 14, 9, 9, 6, 9, 12, 14, 13, 13, 9, 10, 14, 6, 10, 13, 9, 10, 9, 13, 9, 8, 13, 9, 13, 10, 10, 6, 9, 9, 9, 10, 10, 10, 14, 9, 12, 16, 13, 8, 3, 10, 10, 11, 13, 8, 11, 13, 3, 10, 9, 9, 9, 9, 7, 9, 16, 9, 1, 5, 9, 10, 11, 10, 15, 9, 10, 10, 9, 12, 10, 11, 15, 9, 11, 13, 9, 9, 9, 12, 10, 13, 14, 13, 13, 9, 13, 13, 10, 9, 16, 9, 15, 6, 10, 9, 10, 9, 7, 13, 14, 14, 9, 11, 9, 10, 10, 12, 9, 10, 10, 9, 8, 9, 10, 13, 7, 9, 13, 12, 13, 16, 5, 11, 9, 10, 10, 9, 13, 10, 10, 9, 10, 13, 14, 4, 9, 9, 9, 10, 2, 13, 10, 6, 10, 14, 8, 10, 10, 9, 9, 10, 13, 9, 9, 9, 10, 9, 10, 10, 16, 13, 6, 13, 8, 9, 9, 10, 3, 7, 9, 13, 9, 9, 14, 13, 10, 9, 13, 8, 9, 13, 13, 7, 11, 4, 9, 6, 11, 11, 10, 10, 10, 1, 9, 14, 9, 10, 11, 14, 9, 4, 9, 9, 15, 13, 10, 13, 11, 9, 9, 9, 9, 9, 10, 5, 9, 13, 10, 9, 9, 10, 12, 10, 10, 9, 8, 13, 9, 9, 9, 9, 9, 13, 10, 4, 10, 2, 9, 7, 10, 10, 13, 10, 9, 9, 10, 3, 9, 9, 12, 10, 13, 13, 13, 9, 9, 7, 13, 2, 9, 9, 13, 16, 4, 4, 1, 13, 9, 9, 15, 9, 14, 13, 10, 10, 10, 9, 9, 9, 9, 6, 4, 14, 5, 10, 10, 10, 10, 13, 7, 14, 13, 13, 9, 13, 9, 9, 13, 9, 15, 5, 9, 16, 16, 10, 14, 14, 12, 9, 8, 8, 12, 4, 14, 11, 10, 10, 10, 9, 10, 10, 10, 9, 5, 9, 9, 7, 9, 10, 7, 9, 9, 9, 7, 5, 13, 9, 14, 6, 11, 13, 6, 9, 9, 13, 9, 14, 11, 6, 11, 4, 9, 10, 9, 9, 7, 9, 12, 11, 6, 10, 9, 9, 13, 13, 16, 13, 10, 8, 13, 9, 13, 16, 9, 9, 9, 12, 14, 12, 12, 9, 15, 10, 10, 6, 11, 14, 14, 10, 12, 9, 9, 9, 9], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Education-Num\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8e132a7b-3dfd-4fcc-8188-f21173a90f6d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5F7eUgrRwA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Capital Gain\n",
        "fig = px.box(df_3_adult, y=\"Capital-Gain\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F21uipR6Sy95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Capital Loss\n",
        "fig = px.box(df_3_adult, y=\"Capital-Loss\")\n",
        "fig.show()\n",
        "# Hours per Week\n",
        "fig = px.box(df_3_adult, y=\"Hours-per-week\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvxB8Eg_SFP",
        "colab_type": "text"
      },
      "source": [
        "### Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJoFm6JR1UUP",
        "colab_type": "text"
      },
      "source": [
        "**Steps**\n",
        "1. Specify hyperparameters for the chosen classification algorithm by means of grid search using standard ranges. Specify by means of on the dataset as a whole (full min. and maj.).\n",
        "2. Specify what is the majority and what is the minority class based on a value of a feature that identifies groups that are at risk of being discriminated. \n",
        "3. First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. For each step, train a random forest and test by mans of 10-fold cross validation.\n",
        "4. Attach the final outcome predictions  to the individual dataframes of the different datsets with diff. min. group sizes as seperate feature. -> *Open questions: is that manageable because we have different models (e.g. by means of majority vote of the 10 models with regards to classification)? If not possible then necessary to store the metric information directly without data frame as intermediate step (for that check calculation of fairness metric) (-> then also easier to store validation score to have an underfit/overfit check) OR use train/test split instead of k-fold cv (least prefered)*\n",
        "- a) Then filter that dataset so that only the minority group is displayed. Then, calculate  the confusion matrix and the F1-score. \n",
        "- b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric).\n",
        "5. Store the F1-score and the fairness metric for each training set size in a data frame. The data frame should have the following features: \n",
        "- a) Absolute number of training examples for the minority group, \n",
        "- b) percentage share of the minority group in the majority, \n",
        "- c) F-1 score for the min. group (maybe also seperately including std. dev. upper and lower bound), \n",
        "- d) fairness metric based on the min. & maj. group prediction comparisons. **Extended**: in case the fairness metric is not in the range of [0, 1] like the F1-score, normalize/standardize it (make sure that this is not biasing/altering the metrics results).\n",
        "6. Based on the data frame from step 5, plot a learning curve that shows the relationship between the different absolute sizes of training examples and the performance and fairness metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h328nriHDn-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "# Setup classifiers \n",
        "# a) Linear\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# b) Nonlinear\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# c) Others\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzjd5IvaEJk-",
        "colab_type": "text"
      },
      "source": [
        "###### 0) Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6eTkXMo9h-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA IMPORT\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "               \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_3_adult = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7qYs8QGGBvm",
        "colab_type": "code",
        "outputId": "cd3afa80-ded5-4bdc-bef7-e28615400170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Over-50K</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Over-50K</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;=50K</th>\n",
              "      <td>24720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;50K</th>\n",
              "      <td>7841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Over-50K\n",
              "Over-50K          \n",
              "<=50K        24720\n",
              ">50K          7841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valYK2e7ub-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Data Preprocessing Function\n",
        "\n",
        "def fair_preprocess(data, label, neg_class, pos_class):\n",
        "\n",
        "  ''' Applies binary encoding on the values of the label feature.\n",
        "\n",
        "  Returns two datasets in the following order: data_input_features and data_label. '''\n",
        "\n",
        "  # Binary encoding\n",
        "  data[label] = data[label].replace({neg_class: 0, pos_class: 1})\n",
        "\n",
        "  # Create separate dataset version for input features and label\n",
        "  data_input_features = data.drop(columns=[label])\n",
        "  data_label = data[label]\n",
        "\n",
        "  return data_input_features, data_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HHPEI2vf-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute preprocessing function\n",
        "df_3_adult_train_input, df_3_adult_train_label = fair_preprocess(data = df_3_adult, \n",
        "                                                                 label = \"Over-50K\", \n",
        "                                                                 neg_class = \"<=50K\", \n",
        "                                                                 pos_class = \">50K\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_u077HEWs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check whether binary encoding was successful and seperate datasets were created\n",
        "print(df_3_adult.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))\n",
        "print(df_3_adult_train_input.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY190l6ZkNN3",
        "colab_type": "text"
      },
      "source": [
        "###### 1) Define hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KIh3rYxkIn_",
        "colab_type": "text"
      },
      "source": [
        "Specify hyperparameters for the chosen classification algorithm by means of grid search using standard ranges. Specify by means of on the dataset as a whole (full min. and maj.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD2vKHoA-_kk",
        "colab_type": "text"
      },
      "source": [
        "In order to set training_size, check the number of training examples in the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kjFVSOtljy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "dd13edab-259b-42e2-e80c-4738ca6b434b"
      },
      "source": [
        "# HYPERPARAMETER TUNING\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "# Important: Keys in the dictionary must be valid hyperparameters \n",
        "param_grid = {\"n_estimators\": [10, 25, 50, 75, 100, 200, 300],\n",
        "              \"max_depth\": [2, 5, 8, 15, 25, 30],        \n",
        "              \"min_samples_split\": [2, 5, 10, 15, 100],\n",
        "              \"min_samples_leaf\": [1, 2, 5, 10]}\n",
        "\n",
        "# n_estimators = number of trees in the foreset\n",
        "# max_features = max number of features considered for splitting a node\n",
        "# max_depth = max number of levels in each decision tree\n",
        "# min_samples_split = min number of data points placed in a node before the node is split\n",
        "# min_samples_leaf = min number of data points allowed in a leaf node\n",
        "# bootstrap = method for sampling data points (with or without replacement)\n",
        "\n",
        "# Define classifier\n",
        "rf_grid_search = RandomForestClassifier(criterion= \"gini\",\n",
        "                                        max_features = \"auto\")\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define model\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "grid_rf_class = sklearn.model_selection.GridSearchCV(estimator = rf_grid_search,\n",
        "                                                     param_grid = param_grid, \n",
        "                                                     scoring= f1,\n",
        "                                                     n_jobs = 2, \n",
        "                                                     cv = 5,\n",
        "                                                     refit = True,\n",
        "                                                     return_train_score = True)\n",
        "\n",
        "# Dummy Coding\n",
        "df_3_adult_train_input = pd.get_dummies(df_3_adult_train_input)\n",
        "\n",
        "# Fit model\n",
        "grid_rf_class.fit(df_3_adult_train_input, df_3_adult_train_label) # To Do: specify dataset variable name\n",
        "\n",
        "print(grid_rf_class.best_params_)\n",
        "print(grid_rf_class.best_estimator_) # print in order to check actual final hyperparameters"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-1b14310a5fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                      return_train_score = True)\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_3_adult_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_3_adult_train_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# To Do: specify dataset variable name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_rf_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'State-gov'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIowsnp2zZrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(grid_rf_class.best_estimator_) # grid_rf_class.best_estimator_ is an estimator that we can directly use for fitting and prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeCn37qQpS-",
        "colab_type": "text"
      },
      "source": [
        "###### 2) Create datasets with diff. minority group sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTkqDkW4QrWV",
        "colab_type": "text"
      },
      "source": [
        "First, take the whole dataset with the majority and the minority class. Then, filter the dataset such that the majority class is fixed in size and only a certain number of training examples that are considered for the minority class is considered. Change this number step-by-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGE2KGthLrrm",
        "colab_type": "code",
        "outputId": "ac0f97e0-0e8f-419b-93f2-404d4081c0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Determine number of training examples for the examples that are at risk of being discriminated\n",
        "print(df_3_adult.groupby(['Race', \"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             Over-50K\n",
            "Race               Over-50K          \n",
            "Amer-Indian-Eskimo 0              275\n",
            "                   1               36\n",
            "Asian-Pac-Islander 0              763\n",
            "                   1              276\n",
            "Black              0             2737\n",
            "                   1              387\n",
            "Other              0              246\n",
            "                   1               25\n",
            "White              0            20699\n",
            "                   1             7117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDhiWlO__qTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create datasets with different minority group sizes\n",
        "\n",
        "def create_datasets_new(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "    datasets = []\n",
        "    for training_size in training_sizes:\n",
        "        dataset_min_sample = min_data.sample(n=training_size, random_state=1)\n",
        "        dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "        datasets.append(dataset)\n",
        "    return datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-GcZxMQEaxl",
        "colab_type": "code",
        "outputId": "5221298e-a659-4fd5-c3ba-81592ff564d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Define arguments\n",
        "\n",
        "# training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500, 600, \n",
        "                    # 700, 800, 900, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, \n",
        "                    # 4000, 4250, 4500, 4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, \n",
        "                  300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, \n",
        "                  1500, 1750, 2000, 2250, 2500, 2750, 3000]\n",
        "\n",
        "# Creating dfs for minority and majority group\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "df_3_adult_black = df_3_adult[is_black]  # Minority\n",
        "df_3_adult_white = df_3_adult[is_white]  # Majority\n",
        "\n",
        "# Execute function\n",
        "list_dfs = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)\n",
        "print(len(list_dfs))\n",
        "print([df.shape[0] for df in list_dfs])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "[27821, 27826, 27836, 27846, 27856, 27866, 27891, 27916, 27941, 27966, 27991, 28016, 28066, 28116, 28166, 28216, 28266, 28316, 28416, 28516, 28616, 28716, 28816, 29066, 29316, 29566, 29816, 30066, 30316, 30566, 30816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyn0FoHBJmAm",
        "colab_type": "code",
        "outputId": "b96e2cf7-1801-4d52-fd34-5632fc55fad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "df_test = list_dfs[15]\n",
        "print(df_test.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Over-50K\n",
            "Over-50K          \n",
            "0            21046\n",
            "1             7170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkzaawcuCKZ",
        "colab_type": "text"
      },
      "source": [
        "###### 3) Get metrics based on classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFhl8q7uOiO",
        "colab_type": "text"
      },
      "source": [
        "Train a Random Forest classifier with prespecified hyperparameters on the different dataset sizes and test by means of 10-fold cross validation.\n",
        "\n",
        "a) Get F1-score per iteration for the minority group.\n",
        "\n",
        "b) Get fairness metrics per iteration for the comparison of the minority and majority group.\n",
        "\n",
        "**Or**: get final prediction outcomes per training example per iteration. Then continue with 5) to calculate the metrics on my own. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbAUP_kTFoSu",
        "colab_type": "text"
      },
      "source": [
        "Save the following metrics per iteration:\n",
        "\n",
        "- Index -> Counting upwards per row\n",
        "- Number of total training examples\n",
        "\n",
        "Minority Group\n",
        "- Number of training examples for minority group\n",
        "- F1 score for minority group\n",
        "- TPR, FPR, TNR, FNR for minority group\n",
        "\n",
        "Majority Group\n",
        "- Number of training examples for majority group\n",
        "- F1 score for majority group\n",
        "- TPR, FPR, TNR, FNR for majority group\n",
        "\n",
        "Calculate afterwards:\n",
        "- Relative share of minority group of the total training examples\n",
        "- Fairness metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh9WTLfDNMC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  # Initialize lists for metrics \n",
        "  # COMPLETE\n",
        "  rows_compl_list = [] \n",
        "  f1_compl_list = []\n",
        "  # MINORITY\n",
        "  rows_min_list = [] \n",
        "  f1_min_list = []\n",
        "  tpr_min_list = []\n",
        "  fpr_min_list = []\n",
        "  prob_y_1_min_list = []\n",
        "  # MAJORITY\n",
        "  rows_maj_list = []\n",
        "  f1_maj_list = []\n",
        "  tpr_maj_list = []\n",
        "  fpr_maj_list = []\n",
        "  prob_y_1_maj_list = []\n",
        "\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      # NUMBER OF ROWS\n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)   \n",
        "      # F1\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = tn_min / (tn_min+fp_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "      # Cond. Prob. P(y=1|minority)\n",
        "      filter_race_black_y_1 = dataset_var[discr_feature].isin([min_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_black = dataset_var[discr_feature].isin([min_value])\n",
        "      prob_race_black_y_1 = len(dataset_var[filter_race_black_y_1].index) / len(dataset_var[filter_race_black].index)\n",
        "      prob_y_1_min_list.append(prob_y_1_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "      # Cond. Prob. P(y=1|minority)\n",
        "      filter_race_white_y_1 = dataset_var[discr_feature].isin([maj_value]) & dataset_var[\"y_pred\"].isin([1])\n",
        "      filter_race_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      prob_race_white_y_1 = len(dataset_var[filter_race_white_y_1].index) / len(dataset_var[filter_race_white].index)\n",
        "      prob_y_1_maj_list.append(prob_race_white_y_1)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                             \"rows_minority\": rows_min_list,\n",
        "                             \"rows_majority\": rows_maj_list, \n",
        "                             'f1_complete': f1_compl_list,\n",
        "                             'f1_minority': f1_min_list,\n",
        "                             'f1_majority': f1_maj_list,\n",
        "                             'tpr_minority': tpr_min_list,\n",
        "                             \"prob_yhat_1_minority\": prob_y_1_min_list,\n",
        "                             \"tpr_majority\": tpr_maj_list,\n",
        "                             \"fpr_minority\": fpr_min_list,\n",
        "                             \"fpr_majority\": fpr_maj_list,\n",
        "                             \"prob_yhat_1_majority\": prob_y_1_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"])\n",
        "  # FAIRNESS METRICS\n",
        "  # Average Absolute Odds Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"aver_abs_odds_diff\"] = 0.5*(abs(results_df[\"fpr_minority\"] - results_df[\"fpr_majority\"])+abs(results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"])) \n",
        "  # Statistical Parity Difference -> The closer to 0, the fairer.\n",
        "  results_df[\"stat_parity_diff\"] =  results_df[\"prob_yhat_1_minority\"] - results_df[\"prob_yhat_1_majority\"]\n",
        "  # Equal Opportunity Distance -> The closer to 0, the fairer.\n",
        "  results_df[\"equal_opport_dist\"] = results_df[\"tpr_minority\"] - results_df[\"tpr_majority\"]  \n",
        "  # Disparate Impact -> The closer to 1, the fairer.\n",
        "  results_df[\"disparate_impact\"] =  results_df[\"prob_yhat_1_minority\"] / results_df[\"prob_yhat_1_majority\"]\n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jni_30XIRcHU",
        "colab_type": "code",
        "outputId": "840894bf-5eb8-4633-ac7b-2c1aa7017792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Prepare groupby data\n",
        "filter_race_black_y_1 = df_check[\"Race\"].isin([\"Black\"]) & df_check[\"y_pred\"].isin([1])\n",
        "filter_race_white_y_1 = df_check[\"Race\"].isin([\"White\"]) & df_check[\"y_pred\"].isin([1])\n",
        "\n",
        "filter_race_black = df_check[\"Race\"].isin([\"Black\"])\n",
        "filter_race_white = df_check[\"Race\"].isin([\"White\"])\n",
        "\n",
        "prob_race_black_y_1 = len(df_check[filter_race_black_y_1].index) / len(df_check[filter_race_black].index)\n",
        "prob_race_white_y_1 = len(df_check[filter_race_white_y_1].index) / len(df_check[filter_race_white].index)\n",
        "\n",
        "print(prob_race_black_y_1)\n",
        "print(prob_race_white_y_1)\n",
        "\n",
        "\n",
        "# rating_probs = df_check.groupby(\"Race\").size().div(len(df_check))\n",
        "# groupby_probs = df_check.groupby(['y_pred', \"Race\"]).size().div(len(df_check)).div(rating_probs, axis=0, level=\"Race\")\n",
        "# print(groupby_probs)\n",
        "\n",
        "# Experiments identify how to subset groupby in order to get right conditional probability \n",
        "rating_probs = df_check.groupby('Race').size().div(len(df_check))\n",
        "print(df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race'))\n",
        "group_df_1 = df_check.groupby(['y_pred', 'Race']).size().div(len(df_check)).div(rating_probs, axis=0, level='Race')\n",
        "# list_groupby = list(group_df_1)\n",
        "# print(list_groupby)\n",
        "# print(list_groupby[2])\n",
        "# print(list_groupby[3])\n",
        "\n",
        "# print(group_df_1[[2]])\n",
        "# print(group_df_1[[3]])\n",
        "# test = group_df_1[[3]]\n",
        "# print(test)\n",
        "\n",
        "# # Create list\n",
        "# list_groupby_probs = list(groupby_probs)\n",
        "# print(list_groupby_probs)\n",
        "# prob_y_1_min = list_groupby_probs[2]\n",
        "# print(prob_y_1_min)\n",
        "\n",
        "# prob_y_1_maj = list_groupby_probs[3]\n",
        "# print(prob_y_1_maj)\n",
        "\n",
        "# # Cond. Prob. P(y=1|minority)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.11737848720161058\n",
            "y_pred  Race \n",
            "0       Black    1.000000\n",
            "        White    0.882622\n",
            "1       White    0.117378\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBSkMj5qSUX_",
        "colab_type": "code",
        "outputId": "eeb5652e-8122-4a92-8690-e637af4c9276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define arguments\n",
        "list_dfs = list_dfs\n",
        "label = \"Over-50K\"\n",
        "model = RandomForestClassifier(criterion= \"gini\",\n",
        "                               max_features = \"auto\",\n",
        "                               max_depth = 4,\n",
        "                               min_samples_leaf = 4,\n",
        "                               n_estimators = 100)\n",
        "# model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "cv = 10\n",
        "discr_feature = \"Race\"\n",
        "min_value = \"Black\"\n",
        "maj_value = \"White\"\n",
        "\n",
        "# Execute function\n",
        "results_df = metrics_to_df(list_dfs = list_dfs, label = label, model = model, cv = cv, \n",
        "                           discr_feature = discr_feature, min_value = min_value, maj_value = maj_value)\n",
        "results_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rows_complete</th>\n",
              "      <th>rows_minority</th>\n",
              "      <th>rows_majority</th>\n",
              "      <th>f1_complete</th>\n",
              "      <th>f1_minority</th>\n",
              "      <th>f1_majority</th>\n",
              "      <th>tpr_minority</th>\n",
              "      <th>prob_yhat_1_minority</th>\n",
              "      <th>tpr_majority</th>\n",
              "      <th>fpr_minority</th>\n",
              "      <th>fpr_majority</th>\n",
              "      <th>prob_yhat_1_majority</th>\n",
              "      <th>rel_share_min_of_maj</th>\n",
              "      <th>aver_abs_odds_diff</th>\n",
              "      <th>stat_parity_diff</th>\n",
              "      <th>equal_opport_dist</th>\n",
              "      <th>disparate_impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27821</td>\n",
              "      <td>5</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.525145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.525145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.381481</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975458</td>\n",
              "      <td>0.115869</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.203012</td>\n",
              "      <td>0.001150</td>\n",
              "      <td>-0.381481</td>\n",
              "      <td>1.009929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27826</td>\n",
              "      <td>10</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.509869</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.509869</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.364760</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977294</td>\n",
              "      <td>0.110224</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.193733</td>\n",
              "      <td>0.006795</td>\n",
              "      <td>-0.364760</td>\n",
              "      <td>1.061644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27836</td>\n",
              "      <td>20</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.522170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.522323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.377266</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976859</td>\n",
              "      <td>0.113747</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.200203</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>-0.377266</td>\n",
              "      <td>1.028761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27846</td>\n",
              "      <td>30</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.528189</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.528393</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.385696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.974492</td>\n",
              "      <td>0.117666</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.205602</td>\n",
              "      <td>-0.000647</td>\n",
              "      <td>-0.385696</td>\n",
              "      <td>0.994500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27856</td>\n",
              "      <td>40</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.518439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.518691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.375299</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975313</td>\n",
              "      <td>0.114395</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.199993</td>\n",
              "      <td>0.002624</td>\n",
              "      <td>-0.375299</td>\n",
              "      <td>1.022942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27866</td>\n",
              "      <td>50</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.521327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.521731</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.378671</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.974926</td>\n",
              "      <td>0.115545</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.190244</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>-0.378671</td>\n",
              "      <td>1.012757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27891</td>\n",
              "      <td>75</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.516756</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.517215</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.373612</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.975554</td>\n",
              "      <td>0.113783</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.145762</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>-0.282703</td>\n",
              "      <td>1.028436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>27916</td>\n",
              "      <td>100</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.519644</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.520120</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.376844</td>\n",
              "      <td>0.976190</td>\n",
              "      <td>0.975168</td>\n",
              "      <td>0.114898</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>0.095183</td>\n",
              "      <td>0.002121</td>\n",
              "      <td>-0.189344</td>\n",
              "      <td>1.018461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>27941</td>\n",
              "      <td>125</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.510518</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.510889</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.365885</td>\n",
              "      <td>0.990385</td>\n",
              "      <td>0.977149</td>\n",
              "      <td>0.110620</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.070513</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>-0.127789</td>\n",
              "      <td>1.057849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>27966</td>\n",
              "      <td>150</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.528178</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.528691</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.385134</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.975313</td>\n",
              "      <td>0.116911</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>0.083462</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>1.000923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>27991</td>\n",
              "      <td>175</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.530300</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.531148</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.389349</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.973622</td>\n",
              "      <td>0.119248</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.136067</td>\n",
              "      <td>-0.002229</td>\n",
              "      <td>-0.258915</td>\n",
              "      <td>0.981308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>28016</td>\n",
              "      <td>200</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.532418</td>\n",
              "      <td>0.342857</td>\n",
              "      <td>0.533053</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.391457</td>\n",
              "      <td>0.988439</td>\n",
              "      <td>0.973429</td>\n",
              "      <td>0.119931</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.092123</td>\n",
              "      <td>-0.002912</td>\n",
              "      <td>-0.169235</td>\n",
              "      <td>0.975719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>28066</td>\n",
              "      <td>250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.523718</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.524450</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.380497</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.975748</td>\n",
              "      <td>0.115401</td>\n",
              "      <td>0.008988</td>\n",
              "      <td>0.077783</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>-0.145203</td>\n",
              "      <td>1.014019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>28116</td>\n",
              "      <td>300</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.527252</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.528066</td>\n",
              "      <td>0.243902</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.385977</td>\n",
              "      <td>0.988417</td>\n",
              "      <td>0.973912</td>\n",
              "      <td>0.118169</td>\n",
              "      <td>0.010785</td>\n",
              "      <td>0.078290</td>\n",
              "      <td>-0.001150</td>\n",
              "      <td>-0.142075</td>\n",
              "      <td>0.990265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>28166</td>\n",
              "      <td>350</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.514755</td>\n",
              "      <td>0.349206</td>\n",
              "      <td>0.515780</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.368554</td>\n",
              "      <td>0.986755</td>\n",
              "      <td>0.979178</td>\n",
              "      <td>0.109793</td>\n",
              "      <td>0.012583</td>\n",
              "      <td>0.073482</td>\n",
              "      <td>0.007226</td>\n",
              "      <td>-0.139387</td>\n",
              "      <td>1.065815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>28216</td>\n",
              "      <td>400</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.517907</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.519003</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.373191</td>\n",
              "      <td>0.991354</td>\n",
              "      <td>0.977680</td>\n",
              "      <td>0.112094</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.080225</td>\n",
              "      <td>0.004925</td>\n",
              "      <td>-0.146776</td>\n",
              "      <td>1.043938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28266</td>\n",
              "      <td>450</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.525992</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.527345</td>\n",
              "      <td>0.224138</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.382746</td>\n",
              "      <td>0.987245</td>\n",
              "      <td>0.976327</td>\n",
              "      <td>0.115545</td>\n",
              "      <td>0.016178</td>\n",
              "      <td>0.084763</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>-0.158608</td>\n",
              "      <td>1.012757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>28316</td>\n",
              "      <td>500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.502248</td>\n",
              "      <td>0.303797</td>\n",
              "      <td>0.503793</td>\n",
              "      <td>0.184615</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.359281</td>\n",
              "      <td>0.995402</td>\n",
              "      <td>0.976955</td>\n",
              "      <td>0.109074</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>0.096556</td>\n",
              "      <td>0.007945</td>\n",
              "      <td>-0.174665</td>\n",
              "      <td>1.072841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28416</td>\n",
              "      <td>600</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.522839</td>\n",
              "      <td>0.367347</td>\n",
              "      <td>0.524325</td>\n",
              "      <td>0.236842</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.377828</td>\n",
              "      <td>0.992366</td>\n",
              "      <td>0.978212</td>\n",
              "      <td>0.112885</td>\n",
              "      <td>0.021570</td>\n",
              "      <td>0.077570</td>\n",
              "      <td>0.004134</td>\n",
              "      <td>-0.140986</td>\n",
              "      <td>1.036624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28516</td>\n",
              "      <td>700</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.518597</td>\n",
              "      <td>0.403361</td>\n",
              "      <td>0.519930</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.375720</td>\n",
              "      <td>0.991803</td>\n",
              "      <td>0.976086</td>\n",
              "      <td>0.113927</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>0.062385</td>\n",
              "      <td>0.003092</td>\n",
              "      <td>-0.109053</td>\n",
              "      <td>1.027138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>28616</td>\n",
              "      <td>800</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.500689</td>\n",
              "      <td>0.358209</td>\n",
              "      <td>0.502595</td>\n",
              "      <td>0.228571</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.353801</td>\n",
              "      <td>0.992806</td>\n",
              "      <td>0.981400</td>\n",
              "      <td>0.104364</td>\n",
              "      <td>0.028760</td>\n",
              "      <td>0.068318</td>\n",
              "      <td>0.012655</td>\n",
              "      <td>-0.125229</td>\n",
              "      <td>1.121254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>28716</td>\n",
              "      <td>900</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.511131</td>\n",
              "      <td>0.352113</td>\n",
              "      <td>0.513356</td>\n",
              "      <td>0.215517</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.365885</td>\n",
              "      <td>0.998724</td>\n",
              "      <td>0.979516</td>\n",
              "      <td>0.108858</td>\n",
              "      <td>0.032355</td>\n",
              "      <td>0.084788</td>\n",
              "      <td>0.008161</td>\n",
              "      <td>-0.150367</td>\n",
              "      <td>1.074967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>28816</td>\n",
              "      <td>1000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.512009</td>\n",
              "      <td>0.361446</td>\n",
              "      <td>0.514459</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.368695</td>\n",
              "      <td>0.995392</td>\n",
              "      <td>0.977777</td>\n",
              "      <td>0.110871</td>\n",
              "      <td>0.035951</td>\n",
              "      <td>0.079518</td>\n",
              "      <td>0.006148</td>\n",
              "      <td>-0.141422</td>\n",
              "      <td>1.055447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>29066</td>\n",
              "      <td>1250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.512761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.515146</td>\n",
              "      <td>0.263804</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.367992</td>\n",
              "      <td>0.991720</td>\n",
              "      <td>0.979129</td>\n",
              "      <td>0.109685</td>\n",
              "      <td>0.044938</td>\n",
              "      <td>0.058390</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>-0.104188</td>\n",
              "      <td>1.066863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>29316</td>\n",
              "      <td>1500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.505194</td>\n",
              "      <td>0.344538</td>\n",
              "      <td>0.508958</td>\n",
              "      <td>0.215789</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.363215</td>\n",
              "      <td>0.994656</td>\n",
              "      <td>0.977970</td>\n",
              "      <td>0.109326</td>\n",
              "      <td>0.053926</td>\n",
              "      <td>0.082056</td>\n",
              "      <td>0.007693</td>\n",
              "      <td>-0.147425</td>\n",
              "      <td>1.070372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>29566</td>\n",
              "      <td>1750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.498699</td>\n",
              "      <td>0.354244</td>\n",
              "      <td>0.502573</td>\n",
              "      <td>0.223256</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.356751</td>\n",
              "      <td>0.994788</td>\n",
              "      <td>0.978356</td>\n",
              "      <td>0.107384</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.074964</td>\n",
              "      <td>0.009635</td>\n",
              "      <td>-0.133496</td>\n",
              "      <td>1.089722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>29816</td>\n",
              "      <td>2000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.508799</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>0.513007</td>\n",
              "      <td>0.235537</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.365744</td>\n",
              "      <td>0.994881</td>\n",
              "      <td>0.979323</td>\n",
              "      <td>0.108966</td>\n",
              "      <td>0.071901</td>\n",
              "      <td>0.072882</td>\n",
              "      <td>0.008053</td>\n",
              "      <td>-0.130207</td>\n",
              "      <td>1.073903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>30066</td>\n",
              "      <td>2250</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.513378</td>\n",
              "      <td>0.303030</td>\n",
              "      <td>0.520152</td>\n",
              "      <td>0.182482</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.374456</td>\n",
              "      <td>0.996964</td>\n",
              "      <td>0.977535</td>\n",
              "      <td>0.112525</td>\n",
              "      <td>0.080889</td>\n",
              "      <td>0.105701</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>-0.191974</td>\n",
              "      <td>1.039936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30316</td>\n",
              "      <td>2500</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.505372</td>\n",
              "      <td>0.281167</td>\n",
              "      <td>0.513632</td>\n",
              "      <td>0.174342</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.369257</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>0.976424</td>\n",
              "      <td>0.112022</td>\n",
              "      <td>0.089876</td>\n",
              "      <td>0.104692</td>\n",
              "      <td>0.004997</td>\n",
              "      <td>-0.194915</td>\n",
              "      <td>1.044608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30566</td>\n",
              "      <td>2750</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.496142</td>\n",
              "      <td>0.354610</td>\n",
              "      <td>0.502085</td>\n",
              "      <td>0.221893</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.355346</td>\n",
              "      <td>0.995854</td>\n",
              "      <td>0.979323</td>\n",
              "      <td>0.106306</td>\n",
              "      <td>0.098864</td>\n",
              "      <td>0.074992</td>\n",
              "      <td>0.010713</td>\n",
              "      <td>-0.133453</td>\n",
              "      <td>1.100778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30816</td>\n",
              "      <td>3000</td>\n",
              "      <td>27816</td>\n",
              "      <td>0.492158</td>\n",
              "      <td>0.371308</td>\n",
              "      <td>0.497896</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.117019</td>\n",
              "      <td>0.349164</td>\n",
              "      <td>0.993161</td>\n",
              "      <td>0.981642</td>\n",
              "      <td>0.102998</td>\n",
              "      <td>0.107852</td>\n",
              "      <td>0.060777</td>\n",
              "      <td>0.014021</td>\n",
              "      <td>-0.110034</td>\n",
              "      <td>1.136126</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rows_complete  rows_minority  ...  equal_opport_dist  disparate_impact\n",
              "0           27821              5  ...          -0.381481          1.009929\n",
              "1           27826             10  ...          -0.364760          1.061644\n",
              "2           27836             20  ...          -0.377266          1.028761\n",
              "3           27846             30  ...          -0.385696          0.994500\n",
              "4           27856             40  ...          -0.375299          1.022942\n",
              "5           27866             50  ...          -0.378671          1.012757\n",
              "6           27891             75  ...          -0.282703          1.028436\n",
              "7           27916            100  ...          -0.189344          1.018461\n",
              "8           27941            125  ...          -0.127789          1.057849\n",
              "9           27966            150  ...          -0.157861          1.000923\n",
              "10          27991            175  ...          -0.258915          0.981308\n",
              "11          28016            200  ...          -0.169235          0.975719\n",
              "12          28066            250  ...          -0.145203          1.014019\n",
              "13          28116            300  ...          -0.142075          0.990265\n",
              "14          28166            350  ...          -0.139387          1.065815\n",
              "15          28216            400  ...          -0.146776          1.043938\n",
              "16          28266            450  ...          -0.158608          1.012757\n",
              "17          28316            500  ...          -0.174665          1.072841\n",
              "18          28416            600  ...          -0.140986          1.036624\n",
              "19          28516            700  ...          -0.109053          1.027138\n",
              "20          28616            800  ...          -0.125229          1.121254\n",
              "21          28716            900  ...          -0.150367          1.074967\n",
              "22          28816           1000  ...          -0.141422          1.055447\n",
              "23          29066           1250  ...          -0.104188          1.066863\n",
              "24          29316           1500  ...          -0.147425          1.070372\n",
              "25          29566           1750  ...          -0.133496          1.089722\n",
              "26          29816           2000  ...          -0.130207          1.073903\n",
              "27          30066           2250  ...          -0.191974          1.039936\n",
              "28          30316           2500  ...          -0.194915          1.044608\n",
              "29          30566           2750  ...          -0.133453          1.100778\n",
              "30          30816           3000  ...          -0.110034          1.136126\n",
              "\n",
              "[31 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcV57a9kuF4K",
        "colab_type": "text"
      },
      "source": [
        "###### a. Test functions step-by-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPLEvwTtD4UV",
        "colab_type": "code",
        "outputId": "bcd32d03-bd28-48a6-d83d-576919d94c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "df_check = list_dfs[1]\n",
        "df_check.groupby([\"Over-50K\"]).agg({\"Over-50K\": 'count'})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Over-50K</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Over-50K</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Over-50K\n",
              "Over-50K          \n",
              "0            21046\n",
              "1             7170"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTyXOp-WEJnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST FUNCTION \n",
        "\n",
        "# Import relevant modules\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Get specific Dataframe from list of dataframes\n",
        "df_check = list_dfs[5]\n",
        "\n",
        "# Define Input and target columns\n",
        "label = \"Over-50K\"\n",
        "df_train_input = df_check.drop(columns=[label])  # Input\n",
        "df_train_label = df_check[label]                 # Target\n",
        "\n",
        "# Apply dummy coding\n",
        "df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "## TRAIN & TEST\n",
        "# Define model\n",
        "rf_test = RandomForestClassifier(criterion= \"gini\", \n",
        "                                 max_features = \"auto\",\n",
        "                                 max_depth = 4,\n",
        "                                 min_samples_leaf = 4,\n",
        "                                 n_estimators = 100)\n",
        "# Predict \n",
        "y_train_pred = cross_val_predict(rf_test,\n",
        "                                 df_train_input, \n",
        "                                 df_train_label, \n",
        "                                 cv = 10)\n",
        "\n",
        "# Append prediction labels to original dataset\n",
        "df_check['y_pred'] = y_train_pred\n",
        "\n",
        "# Create dataset version for minority class \n",
        "is_black = df_check[\"Race\"].isin([\"Black\"]) \n",
        "df_check_black = df_check[is_black]  # Minority group\n",
        "# Create dataset version for majority class\n",
        "is_white = df_check[\"Race\"].isin([\"White\"])\n",
        "df_check_white = df_check[is_white] # Majority group\n",
        "\n",
        "\n",
        "## METRICS\n",
        "# Get metrics for the COMPLETE dataset\n",
        "rows_compl_list = [] \n",
        "rows_compl = len(df_check.index)\n",
        "rows_compl_list.append(rows_compl)\n",
        "\n",
        "f1_compl_list = []\n",
        "f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "f1_compl_list.append(f1_compl) \n",
        "\n",
        "# metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)) \n",
        "# -> https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi\n",
        "\n",
        "# Get metrics for the MINORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_min_list = [] \n",
        "rows_min = len(df_check_black.index)\n",
        "rows_min_list.append(rows_min)\n",
        "# F1\n",
        "f1_min_list = []\n",
        "f1_min = f1_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "f1_min_list.append(f1_min)\n",
        "# TPR, RECALL\n",
        "tpr_min_list = []\n",
        "tpr_min = recall_score(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "tpr_min_list.append(tpr_min)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_min_list = []\n",
        "tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_min = tn_min / (tn_min+fp_min)\n",
        "fpr_min_list.append(fpr_min)\n",
        "\n",
        "# Get metrics for the MAJORITY group\n",
        "# NUMBER OF ROWS\n",
        "rows_maj_list = [] \n",
        "rows_maj = len(df_check_white.index)\n",
        "rows_maj_list.append(rows_maj)\n",
        "# F1\n",
        "f1_maj_list = []\n",
        "f1_maj = f1_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "f1_maj_list.append(f1_maj)\n",
        "# TPR, RECALL\n",
        "tpr_maj_list = []\n",
        "tpr_maj = recall_score(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "tpr_maj_list.append(tpr_maj)\n",
        "# FPR, SPECIFICITY\n",
        "fpr_maj_list = []\n",
        "tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "\n",
        "# Store metrics for different iterations in Data Frame\n",
        "results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                           \"rows_minority\": rows_min_list,\n",
        "                           \"rows_majority\": rows_maj_list, \n",
        "                           'f1_complete': f1_compl_list,\n",
        "                           'f1_minority': f1_min_list,\n",
        "                           'f1_majority': f1_maj_list,\n",
        "                           'tpr_minority': tpr_min_list,\n",
        "                           \"tpr_majority\": tpr_maj_list,\n",
        "                           \"fpr_min\": fpr_min_list,\n",
        "                           \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "# Calculate new columns and append to df \n",
        "results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "# TBD if this calculation is correct that way or if (min/(maj+min))*100\n",
        "\n",
        "# results_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwK1rX29xRA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train scores\n",
        "\n",
        "f1 = make_scorer(f1_score)\n",
        "\n",
        "cross_val_results = cross_validate(estimator = rf_test, \n",
        "                                  X = df_train_input, y = df_train_label, cv = 10, \n",
        "                                  scoring = f1, return_train_score=True)\n",
        "\n",
        "f1_avg_train_score = np.mean(cross_val_results[\"train_score\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCYCAXf8kj5c",
        "colab_type": "text"
      },
      "source": [
        "###### 4) Create dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra2MPCcmQsit",
        "colab_type": "text"
      },
      "source": [
        "Attach the final outcome predictions  to the individual dataframes of the different datsets with diff. min. group sizes as seperate feature. -> *Open questions: is that manageable because we have different models (e.g. by means of majority vote of the 10 models with regards to classification)? If not possible then necessary to store the metric information directly without data frame as intermediate step (for that check calculation of fairness metric) (-> then also easier to store validation score to have an underfit/overfit check) OR use train/test split instead of k-fold cv (least prefered)*\n",
        "- a) Then filter that dataset so that only the minority group is displayed. Then, calculate  the confusion matrix and the F1-score. \n",
        "- b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qitqu79dgsZG",
        "colab_type": "text"
      },
      "source": [
        "b) Calculate the fairness metric by means of comparing the predictions for the minority class with the prediction for the majority class (check: https://aif360.readthedocs.io/en/latest/modules/metrics.html#binary-label-dataset-metric)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbXQvAkgi8vN",
        "colab_type": "text"
      },
      "source": [
        "###### 5) Create visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Q99hS0jOXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "(ggplot(data = mpg)         # defining what data to use\n",
        " + aes(x='class')    # defining what variable to use\n",
        " + geom_bar(size=20) # defining the type of plot to use\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssH5Y-F0Lsqt"
      },
      "source": [
        "## 2) COMPAS Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NisZhXDXXYm-",
        "colab_type": "text"
      },
      "source": [
        "The COMPAS dataset contains records for defendants from Broward County indicating their jail and prison times, demographics, criminal histories, and COMPAS risk scores from 2013 to 2014 [75]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuD8297pES1T",
        "colab_type": "text"
      },
      "source": [
        "See: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_COMPAS.ipynb#scrollTo=67DYIFxoevt2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-gROLSu6ag",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6N-hG5iEaTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training dataset from CSV\n",
        "\n",
        "# TO DO: Check if dataset from link below matches dataset from Github repository: https://github.com/propublica/compas-analysis/\n",
        "\n",
        "df_1_compas = pd.read_csv('https://storage.googleapis.com/what-if-tool-resources/computefest2019/cox-violent-parsed_filt.csv')\n",
        "df_1_compas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_xvYT2xu8L8",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnFYh3lAzWki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization with ggplot2\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from plotnine import *\n",
        "from plotnine.data import mpg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "(ggplot(data = mpg)         # defining what data to use\n",
        " + aes(x='class')    # defining what variable to use\n",
        " + geom_bar(size=20) # defining the type of plot to use\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e15N3XXu_D9",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_TuRWrE1opa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQUh58KAvBv-",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2JLfELvDd9",
        "colab_type": "text"
      },
      "source": [
        "#### Testing & Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FJWgcyYfQl77"
      },
      "source": [
        "Inspiration: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_Age_Regression.ipynb#scrollTo=jlwjF-Nnmoww"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTeamji4P8zv",
        "colab": {}
      },
      "source": [
        "# Installation of the What-If Tool\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNPNLbbQQuLv",
        "colab": {}
      },
      "source": [
        "# Define helper functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64) or df[f].dtype is np.dtype(np.int32):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64) or df[col].dtype is np.dtype(np.int32):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z3LwUneCQoBB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IYZKMnSKML95"
      },
      "source": [
        "Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G24wEMCJMLhK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYLBIMnHMHQy"
      },
      "source": [
        "## 3) Homicide Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbdFqBmv7QI",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOxm-Ju9FYTp",
        "colab_type": "code",
        "outputId": "141ca171-d9aa-4794-8838-ee621644de00",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a828888-7343-4f93-bf18-78708a02560c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1a828888-7343-4f93-bf18-78708a02560c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 2_Homicides_database.csv to 2_Homicides_database (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADu8zzjBFUQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _io\n",
        "df_2_homicide = pd.read_csv(io.StringIO(uploaded['2_Homicides_database (2).csv'].decode('utf-8')))\n",
        "df_2_homicide"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVK_ZUyw8Dep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize Data Import option \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tdRkreU8pcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"https://drive.google.com/open?id=1x27KDf6_fgqYHwccAb6U4h3HAh60eK2z\"\n",
        "df_2_homicide = pd.read_csv(path)\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQebsy5Hv9vk",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c73mjMaswAFK",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV3K9exiXuvf",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aXmWZngwDGL",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5PQKjKwpdT",
        "colab_type": "text"
      },
      "source": [
        "## 4) German Credit Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwsnRs9XPxz",
        "colab_type": "text"
      },
      "source": [
        "The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [42]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9tpy66--lw",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGCylXv_vSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"]  # TO DO\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_4_german = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
        "\n",
        "df_4_german"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SAZPLhwuUD",
        "colab_type": "text"
      },
      "source": [
        "## 5) Communities and Crime Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRiwaHW2XE43",
        "colab_type": "text"
      },
      "source": [
        "The Communities and Crime dataset gathers information from different communities in the United States related to several factors that can highly influence some common crimes such as robberies, murders or rapes. The data includes crime data obtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains socio-economic data from the 1990 US Census."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v2ws3jO_65w",
        "colab_type": "text"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Tr9JoE_99n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = [\n",
        "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
        "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
        "  \"Hours-per-week\", \"Country\", \"Over-50K\"] # TO DO\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df_5_communities = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
        "\n",
        "df_5_communities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgjHGsyBv5q",
        "colab_type": "text"
      },
      "source": [
        "## 6) FICO score credit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aovkFDB0Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "responsibly.dataset\n",
        "# https://docs.responsibly.ai/dataset.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i07ZtFVpOwcv"
      },
      "source": [
        "# Generalizable Rule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "83bzqiZWPKMw"
      },
      "source": [
        "## Learning Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CKxRQB2TdJSq"
      },
      "source": [
        "Helpful Links: \n",
        "\n",
        "-   https://github.com/laxmimerit/Learning-Curve-Machine-Learning-in-Python-KGP-Talkie/blob/master/Learning%20Curve%20Machine%20Learning%20in%20Python%20KGP%20Talkie.ipynb -> Learning Curve Tutorial\n",
        "-   https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "-   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8jwRfLidRaPv",
        "colab": {}
      },
      "source": [
        "# Import all relevant packages and modules\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KjDPQyhx-uC",
        "colab_type": "text"
      },
      "source": [
        "### Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5EAwBCrxhpn",
        "colab_type": "text"
      },
      "source": [
        "**Learning Curve Variant 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg_ZYx3mRu5h",
        "colab": {}
      },
      "source": [
        "# Define plot_curve function\n",
        "\n",
        "def plot_curve():\n",
        "    # instantiate\n",
        "    lg = LinearRegression()\n",
        "\n",
        "    # fit\n",
        "    lg.fit(X, y)\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and traning learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : integer, cross-validation generator, optional\n",
        "        If an integer is passed, it is the number of folds (defaults to 3).\n",
        "        Specific cross-validation objects can be passed, see\n",
        "        sklearn.cross_validation module for the list of possible objects\n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "        \n",
        "    x1 = np.linspace(0, 10, 8, endpoint=True) produces\n",
        "        8 evenly spaced points in the range 0 to 10\n",
        "    \"\"\"\n",
        "    \n",
        "    train_sizes, train_scores, test_scores = learning_curve(lg, X, y, n_jobs=-1, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
        "\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.title(\"RandomForestClassifier\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # box-like grid\n",
        "    plt.grid()\n",
        "    \n",
        "    # plot the std deviation as a transparent range at each training set size\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    \n",
        "    # plot the average training and test score lines at each training set size\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "    # sizes the window for readability and displays the plot\n",
        "    # shows error from 0 to 1.1\n",
        "    plt.ylim(-.1,1.1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MYvlz3RpR8xg"
      },
      "source": [
        "Compared to the theory we covered, here our y-axis is 'score', not 'error', so the higher the score, the better the performance of the model.\n",
        "\n",
        "Training score (red line) decreases and plateau\n",
        "Indicates underfitting\n",
        "High bias\n",
        "Cross-validation score (green line) stagnating throughout\n",
        "Unable to learn from data\n",
        "Low scores (high errors)\n",
        "Should tweak model (perhaps increase model complexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hcWC4NFxbdB",
        "colab_type": "text"
      },
      "source": [
        "**Learning Curve Variant 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "FSU6uQbqbAgZ",
        "colab": {}
      },
      "source": [
        "# Create CV training and test scores for various training set sizes\n",
        "train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), \n",
        "                                                        X, \n",
        "                                                        y,\n",
        "                                                        # Number of folds in cross-validation\n",
        "                                                        cv=10,\n",
        "                                                        # Evaluation metric\n",
        "                                                        scoring='accuracy',\n",
        "                                                        # Use all computer cores\n",
        "                                                        n_jobs=-1, \n",
        "                                                        # 50 different sizes of the training set\n",
        "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
        "\n",
        "# Create means and standard deviations of training set scores\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "\n",
        "# Create means and standard deviations of test set scores\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Draw lines\n",
        "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
        "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
        "\n",
        "# Draw bands\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
        "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D1D6eHmyGkG",
        "colab_type": "text"
      },
      "source": [
        "## Implementation on Case Study Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUBpyP0PcQZX",
        "colab_type": "text"
      },
      "source": [
        "# Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zs8SxSqZGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "  \n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    See :ref:`examples/model_selection/plot_learning_curve.py\n",
        "    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
        "    \"\"\"\n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtMtLvu3b7Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_to_df(list_dfs, label, model, cv, discr_feature, min_value, maj_value):\n",
        "\n",
        "  # Import relevant modules\n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  import sklearn.metrics\n",
        "  from sklearn.metrics import make_scorer\n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import recall_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "\n",
        "      # Define Input and target columns\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]                 # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "      ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input,\n",
        "                                       df_train_label,\n",
        "                                       cv = cv)\n",
        "\n",
        "      # Append prediction labels to original dataset\n",
        "      dataset_var['y_pred'] = y_train_pred\n",
        "\n",
        "      # Create dataset for MINORITY group \n",
        "      is_black = dataset_var[discr_feature].isin([min_value])\n",
        "      df_check_black = dataset_var[is_black] \n",
        "\n",
        "      # Create dataset for MAJORITY group\n",
        "      is_white = dataset_var[discr_feature].isin([maj_value])\n",
        "      df_check_white = dataset_var[is_white] \n",
        "\n",
        "      ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred) \n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_min_list = [] \n",
        "      rows_min = len(df_check_black.index)\n",
        "      rows_min_list.append(rows_min)\n",
        "      # F1\n",
        "      f1_min_list = []\n",
        "      f1_min = f1_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))  # or labels = [0,1]\n",
        "      f1_min_list.append(f1_min)\n",
        "      # TPR/RECALL\n",
        "      tpr_min_list = []\n",
        "      tpr_min = recall_score(df_check_black[label], df_check_black[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      tpr_min_list.append(tpr_min)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_min_list = []\n",
        "      tn_min, fp_min, fn_min, tp_min = confusion_matrix(df_check_black[label], df_check_black[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_min = tn_min / (tn_min+fp_min)\n",
        "      fpr_min_list.append(fpr_min)\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "      # NUMBER OF ROWS\n",
        "      rows_maj_list = [] \n",
        "      rows_maj = len(df_check_white.index)\n",
        "      rows_maj_list.append(rows_maj)\n",
        "      # F1\n",
        "      f1_maj_list = []\n",
        "      f1_maj = f1_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"]))\n",
        "      f1_maj_list.append(f1_maj)\n",
        "      # TPR/RECALL\n",
        "      tpr_maj_list = []\n",
        "      tpr_maj = recall_score(df_check_white[label], df_check_white[\"y_pred\"], labels=np.unique(df_check_black[\"y_pred\"])) #average='weighted'\n",
        "      tpr_maj_list.append(tpr_maj)\n",
        "      # FPR/SPECIFICITY\n",
        "      fpr_maj_list = []\n",
        "      tn_maj, fp_maj, fn_maj, tp_maj = confusion_matrix(df_check_white[label], df_check_white[\"y_pred\"], labels=[0,1]).ravel()\n",
        "      fpr_maj = tn_maj / (tn_maj+fp_maj)\n",
        "      fpr_maj_list.append(fpr_maj)\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'rows_complete': rows_compl_list,\n",
        "                              \"rows_minority\": rows_min_list,\n",
        "                              \"rows_majority\": rows_maj_list, \n",
        "                              'f1_complete': f1_compl_list,\n",
        "                              'f1_minority': f1_min_list,\n",
        "                              'f1_majority': f1_maj_list,\n",
        "                              'tpr_minority': tpr_min_list,\n",
        "                              \"tpr_majority\": tpr_maj_list,\n",
        "                              \"fpr_min\": fpr_min_list,\n",
        "                              \"fpr_maj\": fpr_maj_list})\n",
        "\n",
        "  # Calculate new metric columns and append to df \n",
        "  results_df[\"rel_share_min_of_maj\"] = (results_df[\"rows_minority\"] / results_df[\"rows_majority\"]) \n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcvZqMvgw-XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPROCESSING manually\n",
        "# Encoding Binary \n",
        "\n",
        "df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Check if encoding was successful \n",
        "df_3_adult[\"Over-50K\"].dtypes\n",
        "\n",
        "# Input features\n",
        "df_3_adult_train_input = df_3_adult.drop(columns=[\"Over-50K\"])\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_train_label = df_3_adult[\"Over-50K\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc12Lw3VC-_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding Binary \n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].apply(lambda val: 1 if val == \">50K\" else val == 0)\n",
        "# df_3_adult[\"Over-50K\"] = df_3_adult[\"Over-50K\"].replace(to_replace=['<=50K', '>50K'], value=[0, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJsTc_t6-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OLD FUNCTION\n",
        "\n",
        "label = \"Over-50K\"\n",
        "model = grid_rf_class # <- Best model from hyperparameter tuning\n",
        "list_dfs = list_dfs\n",
        "\n",
        "def metrics_to_df(list_dfs, label, model, cv = 10):\n",
        "\n",
        "  for dataset_var in list_dfs:\n",
        "  \n",
        "    ## DATA PREPROCESSING\n",
        "      # Seperate dataset by input features and labels\n",
        "      df_train_input = dataset_var.drop(columns=[label])  # Input\n",
        "      df_train_label = dataset_var[label]]                # Target\n",
        "\n",
        "      # Apply dummy coding\n",
        "      df_train_input = pd.get_dummies(df_train_input)\n",
        "\n",
        "    ## TRAIN & TEST\n",
        "      # Predict \n",
        "      y_train_pred = cross_val_predict(model,\n",
        "                                       df_train_input, \n",
        "                                       df_train_label, \n",
        "                                       cv = cv)\n",
        "    \n",
        "    ## METRICS\n",
        "      # Get metrics for the COMPLETE dataset\n",
        "\n",
        "      # confusion_matrix(df_train_label, y_train_pred) \n",
        "\n",
        "      rows_compl_list = [] \n",
        "      rows_compl = len(dataset_var.index)\n",
        "      rows_compl_list.append(rows_compl)\n",
        "\n",
        "      f1_compl_list = []\n",
        "      f1_compl = f1_score(df_train_label, y_train_pred)\n",
        "      f1_compl_list.append(f1_compl) \n",
        "\n",
        "      # Band for difference in Training and Validation Set\n",
        "        # train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring, \n",
        "                                                                # n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "        train_scores_mean = np.mean(train_scores, axis=1)\n",
        "        train_scores_std  = np.std(train_scores, axis=1)\n",
        "        test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "        test_scores_std   = np.std(test_scores, axis=1)\n",
        "        plt.grid()\n",
        "\n",
        "      # Get metrics for the MINORITY group\n",
        "        \n",
        "        row_min_list = []\n",
        "        rows_min = len(dataset_min.index)\n",
        "        row_min_list.append(rows_min)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "\n",
        "        f1_min_list = []\n",
        "        f1_min = f1_score(df_train_label, y_train_pred)\n",
        "        f1_min_list.append()\n",
        "\n",
        "      # Get metrics for the MAJORITY group\n",
        "\n",
        "        # Number of rows\n",
        "        row_maj_list = []\n",
        "        rows_maj = len(dataset_maj.index)\n",
        "        row_maj_list.append(rows_maj)\n",
        "\n",
        "        # TPR\n",
        "        # FPR\n",
        "        # TNR\n",
        "        # FNR\n",
        "      \n",
        "        f1_maj_list = []\n",
        "        f1_maj = f1_score(df_train_label, y_train_pred)\n",
        "        f1_maj_list.append()\n",
        "\n",
        "\n",
        "  # Store metrics for different iterations in Data Frame\n",
        "  results_df = pd.DataFrame({'Total_rows':rows_compl_list, \n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list,\n",
        "                             'f1_complete':f1_compl_list})\n",
        "  \n",
        "  # Setting a column as an index\n",
        "  # dogs_ind = dogs.set_index(\"name\")\n",
        "\n",
        "  return(results_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFpvWx8bRWi",
        "colab_type": "text"
      },
      "source": [
        "Calculate Confusion Matrix Cell Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-VTE1ryDbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DEPRECATED\n",
        "\n",
        "  # neighbors_list = list(range(5,500, 5))\n",
        "  # accuracy_list = []\n",
        "  # for test_number in neighbors_list:\n",
        "  # model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "  # predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "  # accuracy = accuracy_score(y_test, predictions)\n",
        "  # accuracy_list.append(accuracy)\n",
        "  # results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "\n",
        "  # # Initialize df \n",
        "  # algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs']) # To Do: Change metrics here\n",
        "\n",
        "  # def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "  #     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "  #                                             columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "  #                                             index=[name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdV1izcpevA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "algo_metrics = pd.DataFrame(columns=['number_training_examples', 'rel_share_min', 'min_f1', \"total_f1\", 'fairness metric'])\n",
        "\n",
        "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], \n",
        "                                            columns=['model', 'fair_metrics', 'prediction', 'probs'], \n",
        "                                            index=[name]))\n",
        "    \n",
        "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
        "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
        "    # fair_metrics function available in the metrics.py file\n",
        "    fair = fair_metrics(data, pred)\n",
        "    \n",
        "    if plot:\n",
        "        # plot_fair_metrics function available in the visualisations.py file\n",
        "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
        "        plot_fair_metrics(fair)\n",
        "        display(fair)\n",
        "    \n",
        "    return fair\n",
        "\n",
        "# Fairness Metric\n",
        "import aif360\n",
        "import aequitas\n",
        "import auditai\n",
        "\n",
        "# 1: Average Absolute Odd Difference\n",
        "average_abs_odds_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference\n",
        "    def average_odds_difference(self):\n",
        "        r\"\"\"Average of difference in FPR and TPR for unprivileged and privileged\n",
        "        groups:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "           \\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n",
        "           + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\n",
        "\n",
        "        A value of 0 indicates equality of odds.\n",
        "        \"\"\"\n",
        "        return 0.5 * (self.difference(self.false_positive_rate)\n",
        "                    + self.difference(self.true_positive_rate))\n",
        "\n",
        "# This metric's scale would need to be \"reversed\", presumably.\n",
        "\n",
        "# 2: Equal Opportunity Distance\n",
        "equal_opportunity_difference()\n",
        "# https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference\n",
        "\n",
        "    def equal_opportunity_difference(self):\n",
        "        \"\"\"Alias of :meth:`true_positive_rate_difference`.\"\"\"\n",
        "        return self.true_positive_rate_difference()\n",
        "\n",
        "# ClassificationMetric and BinaryLabelDatasetMetric.\n",
        "\n",
        "# Template:\n",
        "\n",
        "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
        "# algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
        "# def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
        "#     return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkTibSibQO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_tpr(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FN = 0\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "#     TPR = TP/(TP+FN)\n",
        "\n",
        "# Calculation of confusion matrix rates\n",
        "\n",
        "# # Sensitivity, hit rate, recall, or true positive rate\n",
        "# TPR = TP/(TP+FN)\n",
        "# # Specificity or true negative rate\n",
        "# TNR = TN/(TN+FP) \n",
        "# # Precision or positive predictive value\n",
        "# PPV = TP/(TP+FP)\n",
        "# # Negative predictive value\n",
        "# NPV = TN/(TN+FN)\n",
        "# # Fall out or false positive rate\n",
        "# FPR = FP/(FP+TN)\n",
        "# # False negative rate\n",
        "# FNR = FN/(TP+FN)\n",
        "# # False discovery rate\n",
        "# FDR = FP/(TP+FP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkh5YSJaZej",
        "colab_type": "text"
      },
      "source": [
        "Approach 2: Take slices of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKws3DBaYHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slice code for minority example\n",
        "\n",
        "# Setup slices of the dataset\n",
        "is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "is_female = df_3_adult[\"Sex\"].isin([\"Female\"])\n",
        "is_male = df_3_adult[\"Sex\"].isin([\"Male\"])\n",
        "\n",
        "# Create filtered version of the dataset\n",
        "# Minority group\n",
        "df_3_adult_black = df_3_adult[is_black]\n",
        "df_3_adult_female = df_3_adult[is_female]\n",
        "# Majority group\n",
        "df_3_adult_white = df_3_adult[is_white]\n",
        "df_3_adult_male = df_3_adult[is_male]\n",
        "\n",
        "\n",
        "# Dummy coding for slice\n",
        "\n",
        "df_3_adult_black_dummies = pd.get_dummies(df_3_adult_black)\n",
        "\n",
        "# Features of complete dataset\n",
        "print(df_3_adult.columns)\n",
        "\n",
        "# Input features\n",
        "df_3_adult_black_input = df_3_adult_black_dummies.drop(columns=[\"Over-50K\"])\n",
        "print(df_3_adult_black_input.columns)\n",
        "\n",
        "# Target feature\n",
        "df_3_adult_black_label = df_3_adult_black_dummies[\"Over-50K\"]\n",
        "print(df_3_adult_black_label)\n",
        "\n",
        "# Learning Curve for Slice \n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define arguments\n",
        "f1 = make_scorer(f1_score) # theoretically, set (zero_division=1)\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 12)\n",
        "sizes_minority = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "                  350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "                  2000, 2250, 2500, 2750] \n",
        "\n",
        "# Plot actual learning curve\n",
        "plot_learning_curve(estimator = random_forest, \n",
        "                    title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_black_input, y = df_3_adult_black_label, \n",
        "                    cv = 10, \n",
        "                    scoring = f1, \n",
        "                    ylim = (0, 1), \n",
        "                    train_sizes = sizes_minority)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "# plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41F18rCYhL1",
        "colab_type": "text"
      },
      "source": [
        "Fairness Metric per Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dZ5qd1YrYS",
        "colab_type": "text"
      },
      "source": [
        "https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0E7etjXYgGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Define protected class\n",
        "dataset_orig = df_3_adult(\n",
        "    protected_attribute_names=['Race'],           \n",
        "    privileged_classes=\"White\",     \n",
        "    features_to_drop=['personal_status', 'sex'] \n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'Race': \"White\"}]\n",
        "unprivileged_groups = [{'Race': \"Black\"}]\n",
        "\n",
        "\n",
        "# Convert dataset into aif360 adequate form\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XdytLDYXUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(regr.predict(diabetes_X_test))\n",
        "\n",
        "rfc_model_3 = RandomForestClassifier(n_estimators=200)\n",
        "rfc_model_3.predict(X_test)\n",
        "\n",
        "X_test['survived'] = rfc_model_3.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKx-nFqaTvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect original Learning Curve function\n",
        "import inspect\n",
        "from sklearn.model_selection import learning_curve\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOolonVAnCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Function to get TPR, FPR, usw.\n",
        "\n",
        "# # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "\n",
        "# def perf_measures(y_actual, y_hat): # Two lists: original outcomes and prediction outcomes\n",
        "#     TP = 0\n",
        "#     FP = 0\n",
        "#     TN = 0\n",
        "#     FN = 0\n",
        "\n",
        "#     for i in range(len(y_hat)): \n",
        "#         if y_actual[i]==y_hat[i]==1:\n",
        "#            TP += 1\n",
        "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "#            FP += 1\n",
        "#         if y_actual[i]==y_hat[i]==0:\n",
        "#            TN += 1\n",
        "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "#            FN += 1\n",
        "\n",
        "#     # Calculation of confusion matrix rates\n",
        "#     # Sensitivity, hit rate, recall, or true positive rate\n",
        "#     TPR = TP/(TP+FN)\n",
        "#     # Specificity or true negative rate\n",
        "#     TNR = TN/(TN+FP) \n",
        "#     # Precision or positive predictive value\n",
        "#     PPV = TP/(TP+FP)\n",
        "#     # Negative predictive value\n",
        "#     NPV = TN/(TN+FN)\n",
        "#     # Fall out or false positive rate\n",
        "#     FPR = FP/(FP+TN)\n",
        "#     # False negative rate\n",
        "#     FNR = FN/(TP+FN)\n",
        "#     # False discovery rate\n",
        "#     FDR = FP/(TP+FP)\n",
        "\n",
        "#     return TPR, FPR, TNR, FNR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYvv_KIktiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated CONFUSION MATRIX RATES\n",
        "\n",
        "# tpr_min_list = []\n",
        "# tpr_min = get_tpr(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "\n",
        "# tpr_min_list = [] \n",
        "# fpr_min_list = []\n",
        "# tnr_min_list = []\n",
        "# fnr_min_list = []\n",
        "# tpr_min, fpr_min, tnr_min, fnr_min = perf_measures(df_check_black[\"Over-50K\"], df_check_black[\"y_pred\"])\n",
        "# tpr_min_list.append(tpr_min)\n",
        "# fpr_min_list.append(fpr_min)\n",
        "# tnr_min_list.append(tnr_min)\n",
        "# fnr_min_list.append(fnr_min)\n",
        "\n",
        "# tpr_maj_list = []\n",
        "# fpr_maj_list = []\n",
        "# tnr_maj_list = []\n",
        "# fnr_maj_list = []\n",
        "# tpr_maj, fpr_maj, tnr_maj, fnr_maj = perf_measures(df_check_white[\"Over-50K\"], df_check_white[\"y_pred\"])\n",
        "# tpr_maj_list.append(tpr_maj)\n",
        "# fpr_maj_list.append(fpr_maj)\n",
        "# tnr_maj_list.append(tnr_maj)\n",
        "# fnr_maj_list.append(fnr_maj)\n",
        "\n",
        "#  \"tpr_majority\": tpr_maj_list\n",
        "#  \"tpr_minority\": tpr_min_list\n",
        "#  \"fpr_majority\": fpr_maj_list\n",
        "#  \"fpr_minority\": fpr_min_list\n",
        "#  \"tnr_majority\": tnr_maj_list\n",
        "#  \"tnr_minority\": tnr_min_list\n",
        "#  \"fnr_majority\": fnr_maj_list\n",
        "#  \"fnr_minority\": fnr_min_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuG9uvV5U19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_datasets_new(min_data: pd.DataFrame, maj_data: pd.DataFrame, training_sizes: list):\n",
        "#     datasets = []\n",
        "#     for training_size in training_sizes:\n",
        "#         \n",
        "#             dataset_min_sample = min_data.sample(n=training_size, random_state=1)\n",
        "#             dataset = pd.concat((dataset_min_sample, maj_data))\n",
        "#             datasets.append(dataset)\n",
        "#     return datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FwySPCFNK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "lines = inspect.getsource(learning_curve)\n",
        "print(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUaC5dzFQcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_curve(estimator, X, y, groups=None,\n",
        "                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None,\n",
        "                   scoring=None, exploit_incremental_learning=False,\n",
        "                   n_jobs=None, pre_dispatch=\"all\", verbose=0, shuffle=False,\n",
        "                   random_state=None, error_score=np.nan, return_times=False):\n",
        "    \"\"\"Learning curve.\n",
        "\n",
        "    Determines cross-validated training and test scores for different training\n",
        "    set sizes.\n",
        "\n",
        "    A cross-validation generator splits the whole dataset k times in training\n",
        "    and test data. Subsets of the training set with varying sizes will be used\n",
        "    to train the estimator and a score for each training subset size and the\n",
        "    test set will be computed. Afterwards, the scores will be averaged over\n",
        "    all k runs for each training subset size.\n",
        "\n",
        "    Read more in the :ref:`User Guide <learning_curve>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    groups : array-like, with shape (n_samples,), optional\n",
        "        Group labels for the samples used while splitting the dataset into\n",
        "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
        "        instance (e.g., :class:`GroupKFold`).\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "\n",
        "        - None, to use the default 5-fold cross validation,\n",
        "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
        "        - :term:`CV splitter`,\n",
        "        - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
        "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
        "        other cases, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validation strategies that can be used here.\n",
        "\n",
        "        .. versionchanged:: 0.22\n",
        "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
        "\n",
        "    scoring : string, callable or None, optional, default: None\n",
        "        A string (see model evaluation documentation) or\n",
        "        a scorer callable object / function with signature\n",
        "        ``scorer(estimator, X, y)``.\n",
        "\n",
        "    exploit_incremental_learning : boolean, optional, default: False\n",
        "        If the estimator supports incremental learning, this will be\n",
        "        used to speed up fitting for different training set sizes.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    pre_dispatch : integer or string, optional\n",
        "        Number of predispatched jobs for parallel execution (default is\n",
        "        all). The option can reduce the allocated memory. The string can\n",
        "        be an expression like '2*n_jobs'.\n",
        "\n",
        "    verbose : integer, optional\n",
        "        Controls the verbosity: the higher, the more messages.\n",
        "\n",
        "    shuffle : boolean, optional\n",
        "        Whether to shuffle training data before taking prefixes of it\n",
        "        based on``train_sizes``.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`. Used when ``shuffle`` is True.\n",
        "\n",
        "    error_score : 'raise' or numeric\n",
        "        Value to assign to the score if an error occurs in estimator fitting.\n",
        "        If set to 'raise', the error is raised.\n",
        "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
        "        does not affect the refit step, which will always raise the error.\n",
        "\n",
        "    return_times : boolean, optional (default: False)\n",
        "        Whether to return the fit and score times.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
        "        Numbers of training examples that has been used to generate the\n",
        "        learning curve. Note that the number of ticks might be less\n",
        "        than n_ticks because duplicate entries will be removed.\n",
        "\n",
        "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on training sets.\n",
        "\n",
        "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
        "        Scores on test set.\n",
        "\n",
        "    fit_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for fitting in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "\n",
        "    score_times : array, shape (n_ticks, n_cv_folds)\n",
        "        Times spent for scoring in seconds. Only present if ``return_times``\n",
        "        is True.\n",
        "    \"\"\"\n",
        "    \n",
        "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
        "        raise ValueError(\"An estimator must support the partial_fit interface \"\n",
        "                         \"to exploit incremental learning\")\n",
        "    X, y, groups = indexable(X, y, groups)\n",
        "\n",
        "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
        "    # Store it as list as we will be iterating over the list multiple times\n",
        "    cv_iter = list(cv.split(X, y, groups))\n",
        "\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "\n",
        "    n_max_training_samples = len(cv_iter[0][0])\n",
        "    # Because the lengths of folds can be significantly different, it is\n",
        "    # not guaranteed that we use all of the available training data when we\n",
        "    # use the first 'n_max_training_samples' samples.\n",
        "    train_sizes_abs = _translate_train_sizes(train_sizes,\n",
        "                                             n_max_training_samples)\n",
        "    n_unique_ticks = train_sizes_abs.shape[0]\n",
        "    if verbose > 0:\n",
        "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
        "\n",
        "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
        "                        verbose=verbose)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = check_random_state(random_state)\n",
        "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
        "\n",
        "    if exploit_incremental_learning:\n",
        "        classes = np.unique(y) if is_classifier(estimator) else None\n",
        "        out = parallel(delayed(_incremental_fit_estimator)(\n",
        "            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n",
        "            scorer, verbose, return_times) for train, test in cv_iter)\n",
        "    else:\n",
        "        train_test_proportions = []\n",
        "        for train, test in cv_iter:\n",
        "            for n_train_samples in train_sizes_abs:\n",
        "                train_test_proportions.append((train[:n_train_samples], test))\n",
        "\n",
        "        out = parallel(delayed(_fit_and_score)(\n",
        "            clone(estimator), X, y, scorer, train, test, verbose,\n",
        "            parameters=None, fit_params=None, return_train_score=True,\n",
        "            error_score=error_score, return_times=return_times)\n",
        "            for train, test in train_test_proportions)\n",
        "        out = np.array(out)\n",
        "        n_cv_folds = out.shape[0] // n_unique_ticks\n",
        "        dim = 4 if return_times else 2\n",
        "        out = out.reshape(n_cv_folds, n_unique_ticks, dim)\n",
        "\n",
        "    out = np.asarray(out).transpose((2, 1, 0))\n",
        "\n",
        "    ret = train_sizes_abs, out[0], out[1]\n",
        "\n",
        "    if return_times:\n",
        "        ret = ret + (out[2], out[3])\n",
        "\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqnx5tPkCpWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# min_dataset_list = list()\n",
        "# compl_dataset_list = list()\n",
        "\n",
        "# is_black = df_3_adult[\"Race\"].isin([\"Black\"])\n",
        "# is_white = df_3_adult[\"Race\"].isin([\"White\"])\n",
        "# def create_datasets(min_data, maj_data, training_sizes):\n",
        "#     for training_size in training_sizes:\n",
        "#         while len(min_data.index) >= training_size: # Code should stop when number of rows of df with min. group is smaller than training size iteration\n",
        "#             dataset_min_slice = min_data.sample(n = training_size, random_state = 1) # Get a random subset of the minority group\n",
        "#             min_dataset_list.append(dataset_min_slice) # Create list of data frames that include observations of minority group of different sizes\n",
        "#             for min_dataset_component in min_dataset_list:\n",
        "#                 dataset_list = maj_data.append(min_dataset_component) # Merge observations of min. group of different sizes with observations from maj. group\n",
        "#                 compl_dataset_list.append(dataset_list) # Create list of data frames that include majority group (fixed size) and minority group (different sizes)\n",
        "#                 return compl_dataset_list\n",
        "\n",
        "# training_sizes_2 = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, \n",
        "                      300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750]\n",
        "\n",
        "# list_dfs_2 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_2)\n",
        "# print(len(list_dfs_2))\n",
        "# print([df.shape[0] for df in list_dfs_2])\n",
        "\n",
        "# training_sizes_3 =  [2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500]\n",
        "\n",
        "# list_dfs_3 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_3)\n",
        "# print(len(list_dfs_3))\n",
        "# print([df.shape[0] for df in list_dfs_3])\n",
        "\n",
        "# training_sizes_4 = [4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# list_dfs_4 = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes_4)\n",
        "# print(len(list_dfs_4))\n",
        "# print([df.shape[0] for df in list_dfs_4])\n",
        "\n",
        "# # Execute function\n",
        "# training_sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "#                   350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "#                   2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "#                   4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000]\n",
        "\n",
        "# df_3_adult_black = df_3_adult[is_black]  # Define minority group based on original data frame\n",
        "# df_3_adult_white = df_3_adult[is_white]  # Define majority group based on original data frame\n",
        "\n",
        "# list_dfs = create_datasets_new(min_data = df_3_adult_black, maj_data = df_3_adult_white, training_sizes = training_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhAaGcytjPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_3_adult_train_input\n",
        "# df_3_adult_train_label\n",
        "\n",
        "# # Row bind\n",
        "# df1.append(df2)\n",
        "\n",
        "# def get_min_datasets(data, ):\n",
        "#   for specific_size in min_sizes:\n",
        "#     if n_row(dataset) >= specific_size:\n",
        "#     elif n_row(dataset) < specific_size:\n",
        "#       stop\n",
        "#     dataset_min_slice = training_examples.sample(frac=1) # shuffle dataset with minority group randomely before slicing\n",
        "#     dataset_min_slice = dataset_min_slice.slice[min_sizes]\n",
        "#     diff_min_datasets.append(dataset_min) #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_a7-0Dg1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ggf. feature importance für jede Iteration auch noch berechnen und dann ans DF anhängen\n",
        "\n",
        "# Sample Code\n",
        "\n",
        "# neighbors_list = [3,5,10,20,50,75]      # use either the one or the other neighbors_list \n",
        "# neighbors_list = list(range(5,500, 5))\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# accuracy_list = []\n",
        "# for test_number in neighbors_list:\n",
        "#   model = KNeighborsClassifier(n_neighbors=test_number)\n",
        "#   predictions = model.fit(X_train, y_train).predict(X_test)\n",
        "#   accuracy = accuracy_score(y_test, predictions)\n",
        "#   accuracy_list.append(accuracy)\n",
        "\n",
        "  # Because I will be working with k-fold cv, most likely I will need to take the average \n",
        "  # and define the standard deviation (which then can also be shown in the plot as bands)\n",
        "\n",
        "  # Important: Get score both training and testing \n",
        "\n",
        "# results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
        "# print(np.linspace(1,2,5))\n",
        "\n",
        "# Matplotlib plotting code -> Alternatively, use plotly\n",
        "\n",
        "# plt.plot(results_df['neighbors'],\n",
        "# results_df['accuracy'])\n",
        "# # Add the labels and title\n",
        "# plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',\n",
        "# title='Accuracy for different n_neighbors')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Dataframe \n",
        "\n",
        "# results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
        "# print(results_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZywZ_O294Rcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# II. Model setup\n",
        "# a) Linear\n",
        "\n",
        "# b) Nonlinear\n",
        "# K-NN\n",
        "# knn = KNeighborsClassifier(n_neighbors=6) # To Do: Check appropriatness of hyperparameter\n",
        "# knn.fit(df_3_adult_train_input, df_3_adult_train_label) \n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training\n",
        "\n",
        "# c) Others\n",
        "# Random Forest\n",
        "# random_forest = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs =-1)\n",
        "#  random_forest.fit(df_3_adult_train_input, df_3_adult_train_label)\n",
        "# To Do: Probably wrong because whole dataset and not only training set is used as input training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLHQERS8pV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute train/test split.\n",
        "\n",
        "# Check: Probably not necessary \n",
        "\n",
        "# Execute Train/test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_3_adult_train_input, \n",
        "#                                                     df_3_adult_train_label, \n",
        "#                                                     test_size=0.3, \n",
        "#                                                     random_state=21) # seed for random number generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMoVDkicTDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 2\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "    go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='group')\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Yvu-jJcado",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Alternative 1\n",
        "# fig = px.bar(df_3_adult_aggr, x=\"Race\", y=\"counting\", color='Sex', barmode='group')\n",
        "# fig.show()\n",
        "\n",
        "# Hint: Does not work properly.\n",
        "\n",
        "# Plot Alternative 2\n",
        "# import plotly.graph_objects as go\n",
        "\n",
        "# fig = go.Figure(data=[\n",
        "#     go.Bar(name='Male', x=df_3_adult_aggr[\"Race\"], y=df_3_adult_aggr[\"counting\"]),\n",
        "#     go.Bar(name='Female', x=df_3_adult_aggr[\"Sex\"], y=df_3_adult_aggr[\"counting\"])\n",
        "# ])\n",
        "# # Change the bar mode\n",
        "# fig.update_layout(barmode='group')\n",
        "# fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7546kms4na",
        "colab_type": "text"
      },
      "source": [
        "## Learning Curve Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBgKn1M62udK",
        "colab_type": "text"
      },
      "source": [
        "### Variant 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPJKpqPtfVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Learning Curve Function\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhGUR1hti1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curve(estimator = random_forest, title = \"Random Forest Learning Curve\", \n",
        "                    X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "                    axes=None, ylim=None, cv=10, train_sizes=sizes)\n",
        " \n",
        "# models = []\n",
        "\n",
        "# for model in models:\n",
        "#   plot_learning_curve(estimator=model, title=\"k-nn Learning Curve\", X = df_3_adult_train_input, y = df_3_adult_train_label, \n",
        "#                     axes=None, ylim=None, cv=10,\n",
        "#                     n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2UsugI2wJQ",
        "colab_type": "text"
      },
      "source": [
        "### Variant 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzmnhnP22Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yellowbrick"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Pq7eJW2xjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from yellowbrick.datasets import load_game\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "# Encode the categorical data\n",
        "X = df_3_adult_train_input\n",
        "y = df_3_adult_train_label\n",
        "\n",
        "# Create the learning curve visualizer\n",
        "cv = StratifiedKFold(n_splits=12)\n",
        "# sizes = np.linspace(0.1, 1.0, 10)\n",
        "sizes = [5, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300,\n",
        "             350, 400, 450, 500, 600, 700, 800, 900, 1000, 1250, 1500, 1750,\n",
        "             2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 4500,\n",
        "             4750, 5000, 6000, 7000, 8000, 9000, 10000, 15000]\n",
        "\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = LearningCurve(\n",
        "    model = random_forest, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
        ")\n",
        "\n",
        "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
        "visualizer.show()           # Finalize and render the figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZPq4Pio-ra3",
        "colab_type": "text"
      },
      "source": [
        "### Variant 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uAPV7p-tja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
        "\n",
        "# train_sizes = [1, 100, 500, 1000, 1500, 2000, 2500, 3000, 5000, 7500, 10000]\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# features = df_3_adult_train_input.columns\n",
        "# target = df_3_adult_train_label\n",
        "\n",
        "train_sizes, train_scores, validation_scores = learning_curve(estimator = random_forest, \n",
        "                                                              X = df_3_adult_train_input, \n",
        "                                                              y = df_3_adult_train_label, \n",
        "                                                              cv = 5)\n",
        "\n",
        "train_scores_mean = train_scores.mean(axis = 1)\n",
        "validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
        "print('\\n', '-' * 20) # separator\n",
        "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('Score', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.title('Learning curves for a knn model', fontsize = 18, y = 1.03)\n",
        "plt.legend()\n",
        "plt.ylim(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxOS8Pdw-z6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Bundling our previous work into a function ###\n",
        "\n",
        "def learning_curves(estimator, data, features, target, train_sizes, cv, scoring):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        estimator, data[features], data[target], train_sizes = train_sizes,\n",
        "        cv = cv, scoring = scoring)\n",
        "    \n",
        "    train_scores_mean = train_scores.mean(axis = 1)\n",
        "    validation_scores_mean = validation_scores.mean(axis = 1)\n",
        "\n",
        "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "\n",
        "    plt.ylabel('Score', fontsize = 14)\n",
        "    plt.xlabel('Training set size', fontsize = 14)\n",
        "    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n",
        "    plt.title(title, fontsize = 18, y = 1.03)\n",
        "    plt.legend()\n",
        "    plt.ylim(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgoEC9cb-27p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plotting the two learning curves ###\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "plt.figure(figsize = (16,5))\n",
        "\n",
        "for model, i in [(RandomForestClassifier(), 1)]:\n",
        "    plt.subplot(1,2,i)\n",
        "    learning_curves(estimator = random_forest, \n",
        "                    data = df_3_adult_dummies, \n",
        "                    features = df_3_adult_train_input.columns, \n",
        "                    target= \"Over-50K\", \n",
        "                    train_sizes = sizes,\n",
        "                    scoring = f1, \n",
        "                    cv= 10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}